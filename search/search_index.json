{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Data Develop &amp; Engineer","text":"<p>Disclaimer: This docs add-on my opinion from Data Engineer experience and experiment around ~5 years (Since 2019).</p> <p>Important</p> <p>I do not have much proper English grammar because I am in the middle level of trying to practice writing and reading. Please understand this problem and open your mind before continue this documents </p> <p>This project will deliver all Practice and Knowledge of Data Developer and Engineer area.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p> First, Data engineering is a critical part of the data lifecycle that enables organizations to manage and process large volumes of data efficiently and reliably<sup>3</sup>. By these concepts, Data Engineer can design and implement Data Pipelines that meet the requirements of their organizations and ensure that their data is managed Consistently and Reliably.</p> <p>What is DE do?</p> <p>Data Engineer is who able to Develop, Operate, and Maintain of Data Infrastructure, either On-Premises or Clouds, comprising databases, storages, compute engines, and pipelines to Extract, Transform, and Load Data.<sup>1</sup></p> Life Cycle of Data Engineering <p> When I started on this role, I got the idea about the future of my responsibilities. I know the Data Engineering tools will shift so fast because the last three year I start with the map-reduce processing on Hadoop HDFS but nowadays, it changes to in-memory processing like Impala or Spark.</p> <p></p> <p>You will see the right picture, the 2023 MAD (ML/AI/Data) Landscape , that show about how many possibility tools that able to use on your project. It has many area that you should to choose which one that match with the current architect or fit with your cost planing model.</p> <p> Finally, the below diagram shows how the focus areas of Data Engineering Shift as the analytics organization evolves.</p> Data Engineering Shift <p>Based upon this illustration, we can observe three distinct focus areas for the role:</p> <ul> <li> <p>Data Infrastructure: One example of a problem being solved in this instance might   be setting up a spark cluster for users to issue HQL queries against data on S3.</p> </li> <li> <p>Data Integration: An example task would be creating a dataset via SQL query,   joining tens of other datasets, and then scheduling the query to run daily using   the orchestration framework.</p> </li> <li> <p>Data Accessibility: An example could be enabling end-users to analyze significant   metrics movements in a self-serve manner.</p> </li> </ul> <p> Additional, the trend of Modern Data Stack will make the Data Engineering so easy and force you have the time to focus on Business Problem. In the another hand, Business Users able to use less of technical knowledge to interact the Data in the Data Platform that make less of SLA to require Data Engineer a lot! </p>"},{"location":"#roles","title":"Roles","text":"<p> In the future, if I do not in love with communication or management level skill that make me be  Lead Data Engineer, I will go to any specialize roles such as,</p> <ul> <li> <p> Data Platform Engineer</p> <p>Data Platform Engineer</p> <p>Read More about Data Architect</p> </li> <li> <p> DataOps Engineer</p> <p>DataOps Engineer</p> <p>Read More about DataOps</p> </li> <li> <p> MLOps Engineer</p> <p>MLOps Engineers Build and Maintain a platform to enable the development and deployment of machine learning models. They typically do that through standardization, automation, and monitoring.</p> <p>MLOps Engineers reiterate the platform and processes to make the machine learning model development and deployment quicker, more reliable, reproducible, and efficient.</p> <p>Read More about MLOps</p> </li> <li> <p> Analytic Engineer</p> <p>Analytic Engineer is who make sure that companies can understand their data and use it to Solve Problems, Answer Questions, or Make Decisions.</p> <p>Read More about Analytic Engineer</p> </li> </ul>"},{"location":"#communities","title":"Communities","text":"<p> This below is the list of Communities that you must join for keep update knowledge for Developer and Data Engineer trend.</p> <ul> <li> <p> Data Engineering</p> <p>The Medium Tag for Data Engineering knowledge and solutions </p> <p> Go Through</p> </li> <li> <p> Data Engineer Cafe</p> <p>An Area of Discussing Blog for Data Engineer like talk to your close friend at the Cafe </p> <p> Go Through</p> </li> <li> <p> ODDS Team</p> <p>The Medium Group that believes software development should be joyful and advocates deliberate practice </p> <p> Go Through</p> </li> <li> <p> TPA Roadmap</p> <p>Community Driven Roadmaps, Articles and Resources for developers in Thailand</p> <p> Go Through</p> </li> </ul>"},{"location":"#tl-dr","title":"TL; DR","text":"<p>Why Data Engineer?</p>"},{"location":"#methodologies","title":"Methodologies","text":"<ul> <li>Discovery Phase:</li> </ul> <p>At this early stage, flexibility is crucial because requirements in large enterprises often evolve. Identify the primary and secondary stakeholders. Meet them, and get their avatar into an orientation diagram or slide deck. These are ultimately your customers; engagement with them will make or break your project. Identify the product owner. This person, or people, will own the delivery and operations for this data product. They will be your tiebreakers and demo audience, and they are the ones who will sign off on your project's delivery.</p> <ul> <li>Requirements Gathering</li> </ul> <p>The process of gathering requirements from various stakeholders can be technically and inter-personally challenging. Not everyone will have the same understanding of the problem or its solutions. Techniques such as interviews, surveys, and workshops will help expose these differences so that we can work towards getting everyone on the same page. Mock-ups and wireframes can also help flush out requirements at this phase.</p> <ul> <li>Design and Planning</li> </ul> <p>Data engineers should plan the architecture, data pipelines, and infrastructure   based on the gathered requirements. This phase should stress the importance of   designing for scalability and maintainability. Deciding to buy or build happens   here. Will there be legacy code to deal with, existing system integrations, or greenfield? Consider which systems will consume the data when selecting a data store. Web apps will need low-latency data stores and will benefit from caching (think MemCacheD). Dashboards usually work best with relational database systems such as PostGres or MySQL, and caching may not be ideal for real-time or near-real-time data, but read replicas may help fan out the read load. Most data projects will benefit from an ETL (Extract, Transform, Load) pattern, where each step is a discrete, independent step that can be run and tested in isolation and end to end. The artifacts from this design process should include diagrams, documentation, and, hopefully, the beginnings of test cases. It would also be a good idea to start a source-to-target mapping that defines the data points in the consuming application (dashboard, web app, etc.) and shows their lineage all the way back to their original source.</p> <ul> <li>Story grooming and Backlog building</li> </ul> <p>Once we have a design and some test cases, we identify the Minimum Viable Product (MVP).   This could be a thin slice of functionality where a subset of data is exposed   end to end or a single data source end to end, the idea is to break up the work   into phases or chunks that are more manageable. Features are written to encapsulate   the required work for each part of the MVP, those features are filled in with   the user stories that make that feature work. At this point, you will want a   detailed design for the MVP, including data flows, expected inputs and outputs,   API contracts, etc. The components not in scope for MVP can have epics and even   features at this point, but since you will learn as you build out MVP, it might   make sense to leave the detail for non-MVP components until closer to the delivery   of the MVP.</p> <ul> <li>Implementation and Testing</li> </ul> <p>Once we have user stories, engineers can fill out the tasks that will satisfy the user story requirements. We can take these requirements and start writing tests that our production code will then satisfy. Some organizations will open a pull request with just the test cases to facilitate the conversation with the technical team and stakeholders. Initial development artifacts should include production code and unit tests. As complete functionality is deployed, integration tests can be built out where real data moves through the pipeline and is measured at critical points during its journey for accuracy and completeness. Once the entire pipeline is in place, we develop our end-to-end tests that cover the complete pipeline with checks to ensure we push the right data to the correct place. Using this test-driven development (TDD) approach, we can ensure the person writing the software feature is writing just the code needed to satisfy requirements and nothing more. This goes a long way to cut down the number of defects and missed requirements.</p> <ul> <li>Validation and Quality Assurance: Tracking row counts at every point in the pipeline and automated tests for known business rules will eliminate many validation issues</li> </ul> <p>Having Subject Matter Experts (SMEs) or Business analysts with domain knowledge get eyeballs on the data will identify problems before production. This means providing a stable, consistent, accessible place for validations. A good artifact from this process is a data quality report that shows the health or quality of data at each step in the pipeline. Things like anomaly detection on the values can automate the detection of bugs or drift.</p> <ul> <li>Feedback Loops</li> </ul> <p>Throughout each phase of this methodology, stakeholders should establish feedback loops to ensure alignment and address evolving requirements. Initially, initial diagrams, documentation, and story grooming can help establish feedback loops. Later, regular and frequent demos and design reviews will help shape conversations around expectations and functionality.</p> <ul> <li>Hand-off</li> </ul> <p>Once the functionality is delivered, an operations team must be trained to support and maintain the application. Clear diagrams and documentation will aid this process. Runbooks and troubleshooting guides are also invaluable at this point. Ideally, most of the invasive maintenance has been automated, and the operations team is left to handle support requests, such as requests for access, data issues, etc. Metrics such as the number of incidents and their root causes will help illuminate where future development work might need to happen.</p> <p>Conclusion:</p> <ul> <li>Know your customers: Strong stakeholder and product owner relationships are often at the crux of a successful data project.</li> <li>Drive consensus using diagrams, documentation, and test cases. The availability of such intellectual artifacts will likely shape the project delivery.</li> <li>Focus on repeatability and testing: time spent here early on will pay dividends later in the development lifecycle.</li> </ul> <ol> <li> <p>Information of this quote reference from  What is Data Engineering? \u21a9</p> </li> <li> <p> The 2023 MAD (ML/AI/DATA) Landscape \u21a9</p> </li> <li> <p>Unlocking the Power of Data: A Beginner\u2019s Guide to Data Engineering \u21a9</p> </li> <li> <p>Types of Data Professionals, credit to  Kevin Rosamont Prombo for creating the infographic \u21a9</p> </li> </ol>"},{"location":"daily-work/","title":"Daily Work &amp; Routine","text":""},{"location":"daily-work/#getting-started","title":"Getting Started","text":""},{"location":"daily-work/#morning-laying-the-groundwork","title":"Morning: Laying the Groundwork","text":"<p>Reviewing System Health and Performance: The day begins with a critical review of the data systems\u2019 health. Monitoring dashboards for alerts on data ingestion or processing issues ensures that any potential bottlenecks are identified and addressed early, safeguarding the data pipeline\u2019s integrity.</p> <p>Stand-up Meetings: Quick stand-up meetings with the data team, and occasionally with cross-functional teams, set the tone for the day. These discussions are vital for aligning on priorities, sharing updates on ongoing projects, and strategizing solutions for any emerging challenges.</p>"},{"location":"daily-work/#midday-deep-dive-into-core-responsibilities","title":"Midday: Deep Dive into Core Responsibilities","text":"<p>Tackling Data and Quality Issues: Data engineers meticulously address data discrepancies, ensuring accuracy and consistency. This includes cleansing data, resolving duplicate records, and rectifying format discrepancies, thereby upholding the data\u2019s integrity for reliable analysis.</p> <p>Aligning with Business Requirements: A significant part of the day involves translating business questions and requirements into technical specifications. This close collaboration with stakeholders ensures that the data infrastructure and processes align with business objectives, facilitating data-driven decision-making.</p> <p>Building and Optimizing ETL Processes: Designing and refining ETL processes to automate the flow of data through the pipeline is a core task. This involves coding, testing, and deploying processes that extract data from diverse sources, transform it into a structured format, and load it into a data warehouse or lake for analysis.</p>"},{"location":"daily-work/#afternoon-expansion-and-collaboration","title":"Afternoon: Expansion and Collaboration","text":"<p>Creating Dashboards and Reporting Tools: Collaborating with data analysts and business users, data engineers contribute to developing dashboards and visualizations. These tools are designed to make data insights accessible, supporting strategic business decisions.</p> <p>Collaborating with Domain Experts: Working with domain experts is crucial for refining data models and ensuring they accurately reflect real-world entities. This collaboration enhances the relevance and quality of data, facilitating more meaningful analysis.</p> <p>Embracing Software Engineering Practices: Developing custom tools and applications to automate routine tasks and improve efficiency is an integral part of the day. This might include creating scripts for data quality checks or developing integrations to streamline data flows.</p>"},{"location":"daily-work/#late-afternoon-to-evening-reflection-and-continuous-learning","title":"Late Afternoon to Evening: Reflection and Continuous Learning","text":"<p>Addressing Technical Debt and Bugs: Late afternoons often involve tackling technical debt and debugging issues, ensuring the data infrastructure is robust and efficient.</p> <p>Documentation: Comprehensive documentation of systems, data flows, and processes ensures knowledge sharing and consistency. This is crucial for maintaining a transparent and efficient data operation.</p> <p>Learning New Technologies: Staying abreast of emerging technologies and tools is essential. Dedicating time to learning enhances a data engineer\u2019s ability to implement innovative solutions, driving the organization\u2019s data capabilities forward.</p>"},{"location":"daily-work/#prioritizing-deep-work-data-issue-resolution-and-development-blocks","title":"Prioritizing Deep Work - Data Issue Resolution and Development Blocks","text":"<p>In the dynamic field of data engineering, amidst the array of daily tasks and responsibilities, it\u2019s crucial to carve out substantial, uninterrupted periods dedicated solely to either resolving data issues or focusing on development projects. This practice of setting aside larger blocks of time is not merely a scheduling preference but a strategic necessity to ensure the depth of focus required for these complex and time-consuming activities.</p>"},{"location":"daily-work/#data-issue-resolution-blocks","title":"Data Issue Resolution Blocks","text":"<p>Data issues, encompassing discrepancies, inconsistencies, or integrity concerns, demand meticulous attention to detail and a thorough investigative process. Given their potential to impact the reliability of data insights and the overall efficiency of data operations, data engineers allocate significant, focused periods to dive deep into these problems. This dedicated time allows for a comprehensive review of data pipelines, identification of root causes, and the implementation of robust solutions without the distractions of routine tasks.</p> <p>Example: On identifying a recurring discrepancy in sales data, a data engineer might block off a morning to systematically trace data flow, review transformation logic, and test potential fixes. This focused effort ensures that not only is the immediate issue resolved but also that systemic improvements are made to prevent future occurrences.</p>"},{"location":"daily-work/#development-time-blocks","title":"Development Time Blocks","text":"<p>Similarly, development activities, whether designing new data pipelines, optimizing existing processes, or developing custom tools, require uninterrupted attention. These blocks of time are earmarked for deep work, where data engineers can engage in complex coding tasks, architectural design, and testing. By isolating these development periods from the day\u2019s operational demands, engineers can innovate and build solutions that enhance the data infrastructure\u2019s scalability, efficiency, and robustness.</p> <p>Example: Planning a session to develop a new ETL process for integrating a recently acquired data source involves uninterrupted hours where the engineer can design, code, and iteratively test the pipeline, ensuring it meets the organization\u2019s data standards and performance criteria.</p>"},{"location":"daily-work/#strategic-scheduling-for-maximum-impact","title":"Strategic Scheduling for Maximum Impact","text":"<p>Incorporating these focused blocks into the daily schedule requires strategic planning. Data engineers, in collaboration with their teams and management, identify priorities and deadlines to determine the best times for these deep work sessions. This might involve scheduling data issue resolution early in the week when data loads are typically lower, or blocking development time post stand-up meetings when the day\u2019s objectives are clear.</p>"},{"location":"daily-work/#continuous-improvement-and-collaboration","title":"Continuous Improvement and Collaboration","text":"<p>Security and Compliance: Ensuring data security and compliance with regulations is an ongoing concern. Landing zones provide a controlled environment to enforce data governance standards, crucial for managing sensitive information.</p> <p>Standardized Data Communication: Establishing a standardized protocol for data exchange between the Enterprise Data Platform and various sources ensures a cohesive data strategy, vital for the integrity and accessibility of organizational data.</p>"},{"location":"daily-work/#conclusion","title":"Conclusion","text":"<p>The daily routine of a data engineer is characterized by a balance of technical challenges, strategic planning, and proactive collaboration. From ensuring the smooth operation of data pipelines to addressing data quality, aligning with business needs, and embracing continuous learning, each aspect of their day contributes to the overarching goal of leveraging data for strategic insights. As the data landscape continues to evolve, the role of the data engineer will remain integral to unlocking the potential of data to drive decision-making and innovation in the digital age.</p> <ol> <li> <p>Daily Work Routine as a Data Engineer \u21a9</p> </li> </ol>"},{"location":"data-storytelling/","title":"Data Storytelling","text":"<p>Quote</p> <p>In a world of information and patterns, Data Storytelling is a beautiful art form that uses ones and zeros.<sup>1</sup></p> <p>Data Storytelling entails communicating facts to a specific audience. It showcases the company\u2019s effectiveness and influence on its customers. Through data insights and beautiful representation, it conveys the entire story of the essential performance metrics, organizational strategies, and processes.</p>"},{"location":"data-storytelling/#what-are-the-components-of-data-storytelling","title":"What Are The Components Of Data Storytelling?","text":"<p>Data storytelling has four main parts: data, visual design, narration, and communication. You need to effectively communicate and share it with your audience.</p> <p>1) Data: The foundation of data storytelling is the data and its credibility. First and foremost, data analytics are very important. This helps a person find insights in the data and draw a conclusion. These key insights build the narration while proving its credibility and usage. When you have a thorough understanding of the data, it provides a stronger basis for supporting the numbers.</p> <p>2) Visual Design: Visually designing your data can enhance the process of data storytelling; examples include charts, graphs, KPIs, tables, etc. These visuals can turn complex concepts into easy narratives. By combining narrative and graphic representation, one can smoothly deliver the information and improve its recall for future use.</p> <p>3) Narrative: Narration is important in data storytelling since it requires expressing data verbally or in written form. It acts as an outlet for communicating to the target audience the story created through analyzing data. While showing the data, you can give recommendations or suggestions based on the data, pointing out vulnerable plots.</p> <p>4) Communication: To succeed in data storytelling, you need to know your audience, set clear goals, and have feedback mechanisms in place. To build interesting data narratives, one must understand the audience and their distinct viewpoints, inclinations, and expertise.</p> <p>Let\u2019s come to the main question \u2014 how is data storytelling beneficial for businesses? Well, businesses often become overwhelmed by a sea of data when navigating the maze. A directing light \u2014 data storytelling paves the way to clarity!</p>"},{"location":"data-storytelling/#why-is-data-storytelling-important-for-your-business","title":"Why Is Data Storytelling Important For Your Business?","text":"<p>Quote</p> <p>Numbers have an important story to tell. They rely on you to give them a clear and convincing voice. \u2014 Stephen Few.</p> <p>The extraction of value from the data that a company has gathered is an important step in strategizing its business plans.</p> <p>Easing complexity by sharing insights: It\u2019s often a complex procedure if the data comes in a huge bunch. Data storytelling plays a major role in simplifying complexity. It transforms data into easily understandable insights using visuals, narratives, and various charts like bar charts, columns, and pie charts. It builds a bridge between the tangled data and meaningful information.</p> <p>Its main job is to simplify complex concepts so that normal users can grasp the importance of those data points. Thus, apply them to their business growth.</p> <p>Make strategic decisions: Knowledge is power. By knowing the context or story behind every piece of information, companies can make strategic decisions. Businesses can successfully coordinate their growth strategies and set goals by utilizing the effectiveness of data storytelling. With this strategy, businesses can use the context and narrative capabilities of data to guide their decision-making and coordinate efforts to meet their objectives.</p> <p>Presenting data driven storytelling: Businesses can successfully communicate the narrative the data offers by using impact storytelling. By relating insights to real people and events, this strategy increases their applicability and makes them more accessible in the workplace. Companies may include stakeholders, decision-makers, and workers and ensure that the insights drawn from the data align with their objectives and experiences by narratively presenting the data. This strategy helps the business make better decisions by increasing the effect of data-driven insights.</p> <p>Also, it grabs people\u2019s attention and sparks emotions, which motivates them to take action. Thus, the organization can aim to create a data-driven culture. This involves identifying areas for improvement and understanding complex processes. Examples of these processes include customer service and supply chain management.</p> <p>Making a connection with data storytelling: One of the best practices of data storytelling is making it interactive. It lies in the message you convey through your data. The clearer the narrative of your data is, the better they can connect with or relate to it.</p> <p>For instance, questions like \u201cWill the presentation offer actionable insights to stakeholders? or Do these trends hold any significance?\u201d can go deeper into the root cause, and thus getting a solution will be easy.</p> <p>The trick to successful data storytelling is \u2014 when it blends together narratives, real-life events, and fascinating experiences. Communicating data and connecting with your clients with an element of authenticity makes your message more relatable.</p>"},{"location":"data-storytelling/#steps-to-a-persuasive-data-storytelling-presentation","title":"Steps to a Persuasive Data Storytelling Presentation","text":"<p>The power of data storytelling lies in the simplification of complicated information. It enhances effectiveness by utilizing visual representation, while traditional data storytelling relies on conventional elements like character, conflict, and resolution.</p> <p>Data storytelling is like an artist choosing colors and brushstrokes to create feelings. It uses visuals, narratives, and settings to engage and convince listeners. By carefully selecting and presenting data, you can transform complex information into a captivating narrative that connects and motivates action. Creating a compelling data story helps make informed decisions, facilitates effective interaction, and drives positive change in various industries.</p> <p>We all know that datasets are just numbers without the story that lies behind them, right? If the data is easy to understand, imagine the time and effort it can save. You can utilize these efforts to generate ideas for making better business decisions. We will examine the topic in more detail.</p> <p></p> <p>The characters that represent your ideal customer or group of clients are basically user personas. This helps in understanding your customers better by identifying things like their interests, personalities, or ages. These personas help in connecting with or communicating with your audience about the product marketing approach on a deeper level. This aids in understanding their choices and preferences.</p> <p>For instance \u2014 a dashboard with personalized hubs that help your audience use your product without any complications can make things easy. It will captivate a diverse audience from different departments. But before that \u2014 there are some crucial questions to focus on:</p> <ul> <li>What kind of data analysis and storytelling are they interested in? Use data to construct a picture that clarifies their course.</li> <li>A broad overview or a close examination of the data \u2014 what do they prefer? Adapt your story to their understanding and grab their attention.</li> <li>Are they beginners or professionals? Acknowledge their expertise and interact with them accordingly.</li> </ul> <p>The ability to effectively build an engaging narrative that keeps your audience interested in your story is a must. Ultimately, your ability to captivate them determines the impact of your data narrative presentation and the resulting outcome.</p>"},{"location":"data-storytelling/#colors-that-speak-of-the-facts-conveyed","title":"Colors That Speak of the Facts Conveyed:","text":"<p>Using color in data storytelling presentations is a bonus to a compelling narrative \u2014 it helps your audience understand the information more effectively. Data shown in green may indicate a favorable outcome, whereas data displayed in red may demonstrate a poor outcome.</p> <p>A personalized dashboard that helps you easily understand important information can greatly affect your clients.</p>"},{"location":"data-storytelling/#develop-engagement-among-users","title":"Develop Engagement Among Users","text":"<p>Prioritize the interests of your targeted audience. You could improve their engagement and make it more interactive and engaging by responding to their queries and comprehending their expectations.</p> <p>Responding to their questions and understanding their expectations is a successful strategy. This encourages a feeling of participation and connection while also enabling you to customize your material or products to suit their requirements better. A more pleasant and meaningful encounter may arise from taking the time to listen and understand their preferences, which will eventually boost audience participation and trust.</p>"},{"location":"data-storytelling/#visualizing-data-insights","title":"Visualizing Data Insights:","text":"<p>Data visualization storytelling plays the ultimate role. You can impress your audience with your data by using charts, filters, buttons, and visuals instead of dull tables and sheets. To learn more about the principles of effective data visualization, click here.</p> <p>You may compare data, evaluate performance, and draw attention to variations between scenarios by using data visualization and storytelling. This gives you the opportunity to provide useful feedback and pointers to your audience. This will help them progress toward their objectives.</p>"},{"location":"data-storytelling/#data-storytelling-in-action","title":"Data Storytelling in Action","text":"<p>Developing compelling and intuitive dashboard designs is essential for thriving in today\u2019s highly competitive business world. In response to this need, mokkup.ai created a design platform that gives businesses the ability to produce interactive dashboards.</p> <p>Key performance indicators (KPIs): KPI metrics are valuable in determining how well a business is doing in reaching specific targets. You can compare past and present information when you customize dashboards with Mokkup, which is one of its best features. As a result, users will find it easy to understand and will visualize the information being presented.</p> <p></p> <p>Themes: Is spring green the brand color for your organization, and are you seeking to align the dashboard\u2019s color scheme accordingly? Enhance the dashboard easily by using different styles and themes that match your preferences and clients\u2019 needs. You can effortlessly achieve the appearance you want owing to this flexibility.</p> <p></p> <p>Charts and Maps: Charts give you an almost limitless number of alternatives. You can enhance the quality of your dashboard design. Combo charts, pie charts, donut-shaped charts, funnel charts, and more are merely a few examples of the alternatives available. There are more alternatives available, including area and line charts. So use your imagination to create an incredible data storytelling screen for your dashboard.</p> <p></p> <p>Including maps can further enhance the visual appeal and context of data that is geographically focused. In such circumstances, you have the opportunity to explore the geomap, treemap, and heatmap. Isn\u2019t it better to use a map to display information about a specific location rather than bland tables?</p> <p>However, balancing the use of tables with other visual elements can avoid overwhelming the dashboard with excessive data. Instead of relying completely on bland tables, maps such as geomaps, treemaps, and heatmaps offer a more engaging and intuitive way to convey information.</p>"},{"location":"data-storytelling/#final-thoughts","title":"Final Thoughts","text":"<p>Presenting data storytelling in an accessible manner can have a greater impact on your audience. It transforms complex numbers into visually appealing content, making it more engaging and easy to understand. It will be simple for the average user to understand the story the numbers are trying to tell. There is no other format that can compare to the impact that words and images have on the audience\u2019s minds.</p> <p>Finally, knowing your audience and their needs and keeping your presentation concise will help you make effective data storytelling presentations.</p> <ol> <li> <p>Can Data Make You a Better Storyteller? \u21a9</p> </li> </ol>"},{"location":"practices/","title":"Practices","text":""},{"location":"practices/#beginner","title":"Beginner","text":"<ul> <li> TowardDS - Data Engineering: A Formula 1-inspired Guide for Beginners</li> <li> A Guide to Data Engineering Infrastructure</li> <li> Data Engineer Best Practice</li> <li> Data Engineering Best Practices</li> <li> Building the Foundation of Modern Data: A Deep Dive into Data Engineering</li> <li> <p> Data Engineering: Practice System Design Question and Solution: Streaming</p> </li> <li> <p> How we think about Data Pipelines is changing</p> </li> <li> <p> Cracking the Data Engineering Interview: The System Design Interview</p> </li> <li> The (Not So Subtle) Art of Not Giving A Fuck About Data</li> </ul>"},{"location":"practices/#develop","title":"Develop","text":"<ul> <li> Test Driven</li> </ul>"},{"location":"requirements-gathering/","title":"Requirements Gathering","text":"<p>One of the mistakes you\u2019ll make as a Data Engineer is not truly understanding the business requirements.</p> <p>Quote</p> <p>The business will come to you and ask for a real-time dashboard.<sup>1</sup></p> <p>But they mean they want the data updated 3-4x a day, or maybe they only look at the report once a week; at that moment, the data should be as up-to-date as possible.</p>"},{"location":"requirements-gathering/#getting-started","title":"Getting Started","text":""},{"location":"requirements-gathering/#identify-the-end-users","title":"Identify the End-Users","text":"<p>Begin by identifying the end-users, crucial stakeholders who utilize the project's output. Understanding the capabilities and preferences of the end-user is crucial for designing an appropriate solution.</p> <p>End-users (&amp; their preferences) for data projects are usually one of:</p> <ul> <li>Data analysts/Scientists: SQL, CSV files</li> <li>Business users: Dashboards, reports, Excel files</li> <li>Software engineers: SQL, APIs, CRMs</li> <li>External clients: Cloud storage, SFTP/FTP, APIs</li> </ul>"},{"location":"requirements-gathering/#help-end-users-define-requirements","title":"Help End-Users Define Requirements","text":"<p>Note</p> <p>Understand The Business - Not Just The Technical Requirements</p> <p>Assist end-users in defining requirements by engaging in conversations about their objectives and challenges. Understand their current operations to gain valuable insights. Use the following questions to refine requirements:</p> <ul> <li> <p>Business impact:</p> <p>How does having this data impact the business? What is the measurable improvement in the bottom line, business OKR, etc? Knowing the business impact helps in determining if this project is worth doing.</p> <p>Evaluate how the data impacts the business and quantify the improvements.</p> </li> <li> <p>Semantic understanding:</p> <p>What does the data represent? What business process generates this data? Knowing this will help you model the data and understand its relation to other tables in your warehouse.</p> <p>Grasp the data's representation and its relation to other warehouse tables.</p> </li> <li> <p>Data source: Where does the data originate? (an application DB, external vendor via SFTP/Cloud store dumps, API data pull, manual upload, etc).</p> </li> <li> <p>Frequency of data pipeline: How fresh does the data need to be? (n minutes, hourly, daily, weekly, etc). Is there a business case for not allowing a higher frequency? What is the highest frequency of data load acceptable by end-users?</p> </li> <li> <p>Historical data: Does historical data need to be stored? When loading data into a warehouse, the answer is usually yes.</p> </li> <li> <p>Data caveats: Does the data have any caveats? (e.g. seasonality affecting size, data skew, inability to join, or data unavailability). Are there any known issues with upstream data sources, such as late arriving data, or missing data?</p> </li> <li> <p>Access pattern: How will the end user access the data? Is access via SQL, dashboard tool, APIs, or cloud storage? In the case of SQL or dashboard access, What are the commonly used filter columns (e.g. date, some business entity)? What is the expected access latency?</p> </li> <li> <p>Business rules check (QA): What data quality metrics do the end-users care about? What are business logic-based data quality checks? Which numeric fields should be checked for divergence (e.g. can\u2019t differ by more than x%) across data pulls?</p> </li> <li> <p>Data output requirements: What is the data output schema? (column names, API field names, Cloud storage file name/size, etc)</p> </li> </ul> <p>Show appreciation to end-users for their time, keep them updated on progress, incorporate their feedback, suggest solutions for common issues, and acknowledge their expertise when presenting the project to a wider audience.</p> <p>Answering the above questions will give you a good starting point.</p> <p>Help the end-users feel invested in the project by following the steps below.</p> <p>Thank end-users for their time/expertise</p> <ul> <li>Update them on progress</li> <li>Ask &amp; incorporate their feedback</li> <li>Recommend solutions(or different ways to do things) for their common issues</li> <li>Acknowledge their help &amp; expertise when presenting the project to a wider audience</li> </ul> <p>End-users who feel invested will root for the project, and help evangelize it. Having end-users who root for the project helps a lot with resource allocation.</p> <p>Clearly define the requirements, record them (e.g. JIRA, etc), and get sign-off from the stakeholders.</p>"},{"location":"requirements-gathering/#end-user-validation","title":"End-User Validation","text":"<p>Provide end-users with sample data for analysis, allowing them to validate its accuracy and usability. Record any new requirements or changes, getting sign-off from stakeholders before proceeding.</p> <p>Record any new requirements or changes (e.g. JIRA, etc), and get sign-off from the stakeholders. Do not start work on the transformation logic until you get a sign-off from the stakeholders.</p>"},{"location":"requirements-gathering/#deliver-iteratively","title":"Deliver Iteratively","text":"<p>Break down large projects into smaller, manageable parts and work with stakeholders to set timelines and priorities. This approach facilitates a short feedback cycle, making it easier to adapt to changing requirements. Track progress with clear acceptance criteria.</p> <p>E.g. If you are building an ELT pipeline (REST API =&gt; dashboard), you can split it into modeling the data, pulling data from a REST API, loading it into a raw warehouse table, &amp; building a dashboard for the modeled data.</p> <p>Delivering in small chunks enables a short feedback cycle from the end-user making changing requirements easy to handle. Track your work (tickets, etc) with clear acceptance criteria.</p>"},{"location":"requirements-gathering/#handling-changing-requirementsnew-features","title":"Handling Changing Requirements/New Features","text":"<p>Establish a process for handling change or feature requests, ensuring end-users can request modifications. Prioritize requests with stakeholder input, communicate delivery timelines, and educate end-users on the request process to prevent scope creep and maintain timely delivery.</p> <p>Do not accept Adhoc change/feature requests!(unless it\u2019s an emergency). Create a process to</p> <ul> <li>Allow end-users to request changes/features</li> <li>Prioritize the change/feature requests with help from stakeholders</li> <li>Decide and communicate delivery timelines to end-users</li> </ul> <p>Educate the end-user on the process of requesting a new feature/change. Following a process will prevent scope creep and allow you to deliver on time.</p>"},{"location":"requirements-gathering/#conclusion","title":"Conclusion","text":"<p>Effectively managing ever-changing requirements is a challenging aspect of a data engineer's role. By following the steps outlined in this article, you can navigate these challenges, ensuring timely project delivery, making a significant impact, enjoying your work on data projects, and fostering supportive end-users.</p> <p>The next time you start a data project, follow the steps shown above to</p> <ul> <li>Deliver on time</li> <li>Make a huge impact</li> <li>Make working on the data projects a joy, and</li> <li>Build supportive end-users</li> </ul> <ol> <li> <p>Becoming a Better Data Engineer Tips \u21a9</p> </li> <li> <p>https://www.startdataengineering.com/post/n-questions-data-pipeline-req/\u00a0\u21a9</p> </li> </ol>"},{"location":"abstract/data_architecture/","title":"Data Architecture","text":"<ul> <li>Data Mesh Implementation Strategies</li> <li>Event-Driven Data Architectures</li> <li>Polyglot Persistence in Microservices</li> <li>Serverless Data Architectures</li> <li>Cloud-Native Data Architectures</li> <li>Data Lake and Lakehouse Architectures</li> </ul>"},{"location":"abstract/data_architecture/#why","title":"Why?","text":"<ul> <li>\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e2d\u0e2d\u0e01\u0e41\u0e1a\u0e1a Data Platform Architecture \u0e43\u0e2b\u0e49\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e2a\u0e21\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 data \u0e43\u0e19\u0e1a\u0e23\u0e34\u0e29\u0e31\u0e17\u0e44\u0e14\u0e49</li> <li>\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e41\u0e25\u0e30\u0e14\u0e39\u0e41\u0e25 Data Platform \u0e43\u0e2b\u0e49 scalability \u0e41\u0e25\u0e30 reliability \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 resource/cost \u0e43\u0e2b\u0e49\u0e04\u0e38\u0e49\u0e21\u0e04\u0e48\u0e32\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14</li> </ul> <ul> <li>Building a Data Platform in 2024</li> <li>https://life.wongnai.com/get-to-know-data-platform-engineer-role-at-lmwn-4b43443eaca8</li> <li>Medium: Cloud Agnostic Data Platform</li> <li>Modern Architecture for Emerging Data Infrastructures</li> <li>Modern Architecture for Emerging Data Infrastructures</li> </ul>"},{"location":"abstract/data_architecture/#what-is-data-architecture","title":"What is Data Architecture?","text":"<p>Data Architecture is the blueprint of a data system that serves the business requirements of a product and describes how data is collected, stored, transformed and distributed. It consists of data models, governance policies, rules and standards that need to be implemented and followed to build a robust and secure data system.</p> <p></p>"},{"location":"abstract/data_architecture/#data-platform-architect","title":"Data Platform Architect","text":"<ul> <li>Data Platforms : Good Architect \u2014 Bad Architect</li> </ul>"},{"location":"abstract/data_architecture/#knowledge","title":"Knowledge","text":"<ul> <li>My key takeaways after building a data engineering platform</li> <li>Microservices vs. Monolithic Approaches in Data</li> </ul>"},{"location":"abstract/data_architecture/#use-cases","title":"Use Cases","text":"<ul> <li>https://medium.com/cj-express-tech-tildi/journey-of-tildi-data-engineer-in-2023-%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%9E%E0%B8%B1%E0%B8%92%E0%B8%99%E0%B8%B2-data-platform-%E0%B8%AD%E0%B8%A2%E0%B9%88%E0%B8%B2%E0%B8%87%E0%B8%95%E0%B9%88%E0%B8%AD%E0%B9%80%E0%B8%99%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%87-fd8a795f3942</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/","title":"Data Warehouse: Data Modeling Technique","text":"<p>Data Modeling is a process of creating a conceptual representation of the data and its relationships within an organization or system. Dimensional modeling is an advanced technique that attempts to present data in a way that is intuitive and understandable for any user. It also allows for high-performance access, flexibility, and scalability to accommodate changes in business needs.</p> <ul> <li>Guide With Problems</li> <li>https://medium.com/@dnyanesh.bandbe88/the-3-brothers-of-data-modelling-kimball-inmon-data-vault-5788863e98c8</li> <li>https://medium.com/@khedekarpratik123/data-vault-2-0-modern-data-modelling-methodology-part-1-ed91ed408d48</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/#type-of-data-modeling","title":"Type of Data Modeling","text":""},{"location":"abstract/data_architecture/data_modeling/#one-big-tabel","title":"One Big-Tabel","text":"<ul> <li>What is One Big Table?</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/#data-vault","title":"Data Vault","text":"<p>https://patrickcuba.medium.com/data-vault-is-not-a-monolith-3ea2014ffedc</p>"},{"location":"abstract/data_architecture/data_modeling/#references","title":"References","text":"<ul> <li>Medium: Data Modeling Techniques for Data Warehouse</li> <li>https://towardsdatascience.com/data-modelling-for-data-engineers-93d058efa302</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-anchor-approach/","title":"Anchor Approach","text":"<p>The Anchor Model further normalizes the data vault model. The initial intention of Lars. R\u00f6nnb\u00e4ck was to design a highly scalable model. His core concept is that all expansion involves adding rather than modifying. Therefore, he normalized the model to 6NF, and it becomes a K-V structural model.</p> <p>The Anchor Model consists of the following:</p> <ul> <li>Anchors: Anchors are similar to Hubs in the Data Vault Model. They stand for business entities and have only primary keys.</li> <li>Attributes: Attributes are similar to satellites in the Data Vault Model but are more normalized. They are in the K-V structure. Each table describes attributes of only one anchor.</li> <li>Ties: Ties indicate the relationship between Anchors and get described using a table. Ties are similar to links in the Data Vault Model and can improve the general model expansion capability.</li> <li>Knots: Knots stand for the attributes that may be shared by multiple anchors, for example, enumerated and public attributes such as gender and state.</li> </ul> <p>We can further subdivide these four basic objects into historical and non-historical objects, where historical objects record changes in the data using timestamps and keeping multiple records.</p> <p>This division allowed the author of the Anchor Model to achieve high scalability. However, this model also increases the number of join query operations. The creator believes that analysis and query in a data warehouse are performed only based on a small section of fields. This is similar to the array storage structure, which can significantly reduce data scanning and reduce the impact on query performance. Some databases with the table elimination feature, for example, MariaDB, can greatly reduce the number of join operations. However, this is still open to discussion.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-anchor-approach/#references","title":"References","text":"<ul> <li>Alibaba Cloud: Comparison of Data Modeling Methods for BigData</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/","title":"Data Vault Approach","text":"<p>In contrary to Inmon\u2019s view, Linstedt assumes that all available data from the entire time period should be loaded into the warehouse. This is known as the \"single version of the facts\" approach. As with Kimball\u2019s star schema, with the Data Vault Linstedt introduces some additional objects to organize the data warehouse structure. These objects are referred to as the hub, satellite and link.</p> <p>The Data Vault methodology is a hybrid approach that combines aspects of both the Kimball and Inmon methodologies.</p> <p>It uses a hub-and-spoke architecture to model the data and focuses on creating separate entities for business processes, data sources, and data types.</p> <p>The Data Vault methodology is known for its scalability, flexibility, and ability to handle complex data relationships.</p> <p>Quote</p> <p>Single version of the facts</p> <p>Data Vault modeling is a hybrid approach based on third normal form and dimensional modeling aimed at the logical Enterprise Data Warehouse (EDW). The data vault model is built as a ground-up, incremental, and modular models that can be applied to big data, structured, and unstructured data sets.</p> <p>Data Vault 2.0 (DV2) modeling is focused on providing flexible, scalable patterns that work together to integrate raw data by business key for the enterprise data warehouse. DV2 modeling includes minor changes to ensure the modeling paradigms can work within the constructs of big data, unstructured data, multi-structured data, and NoSQL.</p> <p>Data Vault focuses on Agile Data Warehouse Development where scalability, data integration/ETL and development speed are important. Most customers have a landing zone, Vault zone and a data mart zone which correspond to the Databricks organizational paradigms of Bronze, Silver and Gold layers. The Data Vault modeling style of hub, link and satellite tables typically fits well in the Silver layer of the Databricks Lakehouse.</p> <p>A data vault is a data modeling design pattern used to build a data warehouse for enterprise-scale analytics. The data vault has three types of entities: hubs, links, and satellites.</p> <p>Data Vault 1.0 Loading Pattern:</p> <pre><code>---\nData Vault 1.0 Loading Pattern\n---\n\nflowchart LR\n    A[Source] --&gt;|Loading| B(Staging)\n    B -- Hub load --&gt; C[Hub]\n    C --&gt; D[Satellite]\n    C --&gt; E[Link]\n    D -.-&gt; |ID Lookup| C\n    E -.-&gt; |ID Lookup| C</code></pre> <p>Data Vault 2.0 Loading Pattern:</p> <pre><code>flowchart LR\n    A[Source] --&gt;|Loading| B(Staging)\n    B -- Hub load --&gt; C[Hub]\n    B -- Link load --&gt; E[Link]\n    B -- Satellite load --&gt; D[Satellite]</code></pre> <p>Data Vault Modeling 2.0 changes the sequence numbers to hash keys. The hash keys provide stability, parallel loading methods, and decoupled computation of parent key values for records.</p> <p>Regarding the hashes, they have a great benefit for the Big Data workloads. The loading pattern for Data Vault 1.0 required to load the hubs first, and later, when the links and satellites were loaded, make the lookups on the hubs table to retrieve the surrogate keys. It changes in the Data Vault 2 because the hash key is deterministic, i.e. it can be resolved from already existing data, making the load parallel for all components</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#hash","title":"Hash","text":"<p>There are many hash functions to choose from: <code>MD5</code>, <code>MD6</code>, <code>SHA-1</code>, <code>SHA-2</code>, and some more. We recommend to use the MD5 Algorithm with a length of 128 bit in most cases, because it is most ubiquitously available across most platforms, and has a decently low percentage chance of duplicate collision with an acceptable storage requirement.</p> <p>Hash Keys are not optional in the Data Vault, they are a must. The advantages of</p> <ul> <li>Massively Parallel Processing (MPP) Architecture</li> <li>data load performance</li> <li>consistency</li> <li>auditability</li> </ul> <p>Note: \\ Hash values are the fundamental component of Data Vault 2.0 modeling. They are generated using system functions as data is loaded into the data vault. Hashes reduce dependencies, allow for quicker joins between tables (HashKey), and allow for fast comparisons to detect changes in data (HashDiff).</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#basic-structures-of-data-vault","title":"Basic Structures of Data Vault","text":""},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#hubs","title":"Hubs","text":"<p>Hub (Immutable Business Key): Contains a unique list of business objects that represents a domain or concept within the enterprise, everything else connected to the hub gives us more context.</p> <p>Note: \\ Business Keys that are supplied by users to identify, track, and locate information, such as a customer number, invoice number, or product number. Should be,</p> <ul> <li>Unique</li> <li>At the same level of granularity</li> </ul> Column Alias Description Constraints Inclusion HashKey <code>HUB_{name}_HK</code> HashKey generated from Business Key PK Required BusinessKey <code>{BS-name}_BK</code> Business defined business key UQ Required LoadDatetime <code>LDTS</code> Load Datetime from Stage to Data Vault Required RecordSource <code>RSCR</code> Specifics the source system from which the key originated Required LastSeenDate Date a record was last included on a data load Optional <p>Loading Pattern:</p> <pre><code>flowchart LR\n    A[(Source)] --&gt; B[\"Select Distinct&lt;br&gt;List of Business&lt;br&gt;Keys\"]\n    B --&gt; C{Key Exists in&lt;br&gt;Target Hub?}\n    C -- No --&gt; D[Generate Hash Key]\n    D --&gt; F[Insert Row&lt;br&gt;Into Target&lt;br&gt;Hub]\n    F --&gt; G[(\"Data Vault&lt;br&gt;Hub\")]\n    C -- Yes --&gt; E[Drop Row&lt;br&gt;From Data Flow]</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#links","title":"Links","text":"<p>Link (Unit of Work): Represents the relationship between two or more business objects, representing a part of one or many business processes or even value streams.</p> <p>Tables that show the relationships between Hubs. Their level of granularity is determined by the number of hubs they connect, and they are non-temporal. When thinking of a traditional star schema, links are often associated with fact tables.</p> <p>Keynotes on Links:</p> <ul> <li>Do not show effectivity</li> <li>Only support inserts</li> </ul> <p>Different types of links can be used to make models more flexible:</p> <ul> <li> <p>Same as Link: \\   helps to solve the data quality issues problem where a given concept can be misspelled   and has to be linked to the correct definition. An example of this fuzzy matching   modeling can be zip codes written in different formats (e.g., 00000 or 00-000)</p> </li> <li> <p>Hierarchical Link: \\   Represents hierarchies inside the same hub. An example of that hierarchy can be   a hierarchy of product categories.</p> </li> <li> <p>Standard Link: \\   Be a standard link created from business rules, so it can be easily discarded   without affecting the auditability. The official specification calls it also a   Deep Learning link because it can give confidence for the prediction and strength   for the correlation of 2 datasets, but if we stay with these 2 columns, we could   also try to use it to model a result of the ML recommendation system between a   user and an item. The link would then specify how possible it is that the user   will like the product. Here, the link is mutable since the prediction can change   or even be discarded if the algorithm detects other user preferences changes.</p> </li> <li> <p>Non-Historized Link (aka transactional): \\   That is perfectly adapted to the immutable facts, so something that happened in   the past can neither be undone nor modified. And since the changes will never happen,   they don't need to be stored in the satellites.</p> </li> </ul> Column Alias DESC Constraints Inclusion HashKey sha HashKey generated from Business Keys of Linked Hubs PK, UQ Required BusinessKey _key Concatenation of Business Keys from linked Hubs UQ Optional LoadDatetime <code>LDTS</code> Load Date &amp; Time from Stage to DV Required RecordSource <code>RSCR</code> Specifics the source system from which the key(s) originated Required HubHashKey1 HushKey from Hub Relationship 1 FK Required HubHashKey2 HushKey from Hub Relationship 2 FK Required ... HushKey from Hub Relationship ... FK Required <p>Loading Pattern:</p> <pre><code>flowchart LR\n\n    A[(Source)] --&gt; B1[\"Select Distinct&lt;br&gt;List of Business&lt;br&gt;Keys\"]\n    B1 --&gt; B[Generate Each&lt;br&gt;Contributing&lt;br&gt;Hub's Hash Key]\n    B --&gt; C{Key Exists in&lt;br&gt;Target Link?}\n    C --&gt; |No| D[\"Generate Hash Key&lt;br&gt;(if required)\"]\n    D --&gt; F[Insert Row&lt;br&gt;Into Target&lt;br&gt;Link]\n    F --&gt; G[(\"Data Vault&lt;br&gt;Link\")]\n    C --&gt; |Yes| E[Drop Row&lt;br&gt;From Data Flow]\n</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#satellites","title":"Satellites","text":"<p>Satellite (Descriptive Context): Descriptive change-tracking content eiter describing the business object (hab-satellite) or the unit of work (link-satellite).</p> <p>Tables that provide context to the business objects and relationships described in Hubs and Links. Each satellite is connected to only one Hub or Link, but a Hub or Link can have multiple satellites.</p> <p>Keynotes on Satellites:</p> <ul> <li>One per source system</li> <li>Stores all context</li> <li>Stores all history</li> <li>Delta driven, similar to SCD</li> <li>EndDate is only attribute that is updated</li> <li>Most flexible construct</li> </ul> <p>Different types of satellite can be used to make models more flexible:</p> <ul> <li> <p>Multi-active Satellite: \\   In this satellite given hub or link entity has multiple active values. For example,   it can be the case of a phone number that can be professional pr personnel, and both   may be active at a given moment.</p> </li> <li> <p>https://www.scalefree.com/scalefree-newsletter/using-multi-active-satellites-the-correct-way-1-2/</p> </li> <li> <p>https://www.scalefree.com/scalefree-newsletter/using-multi-active-satellites-the-correct-way-2-2/</p> </li> <li> <p>Effectivity Satellite: \\   A descriptive record that is valid (effective) only for some specific period of time.   You will there the start and end dates and an example of it can be an external   contractor working in a company in different periods.</p> </li> <li> <p>System-Driven Satellite: \\   Mutable, created from hard business rules (data-type rules, like enforcing integer type;   not involving pure business rules like classifying a revenue as poor or rich category of people)   for performance purposes. The examples of these satellites are the point-in-time table (PIT)   and bridge table, presented in the next paragraph.</p> </li> </ul> Column Alias DESC Constraints Inclusion HashKey sha{type} HashKey from parent Hub or Link PK, FK Required LoadDatetime LDTS Batch Load Date &amp; Time from Stage to DV PK Required EndDatetime EDTS Load Date &amp; Time the record became inactive Required RecordSource RSCR Specifics the source system from which the key(s) originated Required HashDiff HASH_DIFF Hushed value of all attributes data Optional ExtractDate Date data was extracted from source system Optional Status Insert (I)/Update (U)/Delete (D) Attributes1 Attributes column 1. Number and type will vary Optional Attributes2 Attributes column 2. Number and type will vary Optional ... Attributes column ... Number and type will vary Optional <p>Loading Pattern:</p> <pre><code>flowchart LR\n\n    A[(Source)] --&gt; B1[\"Select Distinct&lt;br&gt;List of Satellite&lt;br&gt;Records\"]\n    B1 --&gt; B2[\"Generate Single&lt;br&gt;Hub's or Hubs'&lt;br&gt;and Link's Hash&lt;br&gt;Key(s)\"]\n    B2 --&gt; B[\"Find Latest&lt;br&gt;Satellite&lt;br&gt;Record\"]\n    B --&gt; C{Find Latest&lt;br&gt;Record?}\n    C -- No --&gt; D[\"Insert Row&lt;br&gt;Into Target&lt;br&gt;Satellite\"]\n    D --&gt; F[(Data Vault Satellite)]\n    C -- Yes --&gt; E[Compare All&lt;br&gt;Fields/Columns]\n    E --&gt; G{All&lt;br&gt;Fields/Columns&lt;br&gt;Match?}\n    G -- No --&gt; I[\"Insert Row Into Target&lt;br&gt;Satellite &amp; Perform End-Date&lt;br&gt;Processing\"]\n    I --&gt; F\n    G -- Yes --&gt; H[\"Drop Row&lt;br&gt;From Data Flow\"]\n</code></pre> <p>The hash difference column applies to the satellites. The approach is the same as with the business keys, only that here all the descriptive data is hashed. That reduces the effort during an upload process because just one column has to be looked up. The satellite upload process first examines if the hash key is already in the satellite, and secondly if there are differences between the hash difference values.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#point-in-time","title":"Point-In-Time","text":"<p>PITs (Simplify with Equijoin): disposable table designed to simplify querying of business object or relationship state attributes for information marts.</p> <p>Note: \\ (Business Vault)</p> <p>Represents multiple satellite tables in one. But the idea is not to copy any of the context values but only their load dates. For example, if a hub has 3 different satellites, satA, satB and satC, a point-in-time table will store the most recent load date for every satellite and so for every business key. So the stored data will be built like (business key [BK] from the hub, MAX(loadDate from satA for BK), MAX(loadDate from satB for the BK), MAX(loadDate from satC for BK)</p> <p>The PIT serves two purposes:</p> <ul> <li> <p>Simplify the combination of multiple deltas at different \u201cpoint in time\u201d \\   A PIT table creates snapshots of data for dates specified by the data consumers   upstream. For example, it is often usual to report the current state of data each day.   To accommodate these requirements, the PIT table includes the date and time of   the snapshot, in combination with the business key, as a unique key of the entity   (a hashed key including these two attributes, named as CustomerKey).   For each of these combinations, the PIT table contains the load dates and the   corresponding hash keys from each satellite that corresponds best with the   snapshot date.</p> </li> <li> <p>Reduce the complexity of joins for performance reasons \\   The PIT table is like an index used by the query and provides information about   the active satellite entries per snapshot date. The goal is to materialize as   much of the join logic as possible and end up with an inner join with equi-join   conditions only. This join type is the most performant version of joining on   most (if not all) relational database servers. In order to maximize the   performance of the PIT table while maintaining low storage requirements,   only one ghost record is required in each satellite used by the PIT table.   This ghost record is used when no record is active in the referenced satellite   and serves as the unknown or NULL case. By using the ghost record, it is   possible to avoid NULL checks in general, because the join condition will   always point to an active record in the satellite table: either an actual   record which is active at the given snapshot date or the ghost record.</p> </li> </ul> <p>For example,</p> <p>We identify range of <code>PITForm</code> and <code>PITTo</code> and map the data in satellites to this range (greater or equal than).</p> <p>Note: \\ PIT tables are incrementally updated when new data becomes available in the Data Vault tables that support them.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#bridge","title":"Bridge","text":"<p>Bridges (Shorten the Distance): Disposable table designed to shorten the distance between business objects for information marts.</p> <p>Note: \\ (Business Vault)</p> <p>Similar to the PIT tables, bridge tables are also the tables designed with the performance in mind. But unlike a PIT table, a bridge table will only store the business keys from the hubs and links. If you are familiar with the star schema, you will see some similarities between bridge and fact tables. The difference is that the bridge stores only the keys, whereas the fact will also store the measures. A single accepted value in the bridge tables are aggregations computed at runtime. Together with PIT table, the bride is considered as a query-assist structure.</p> <p>There are some important notes to clarify on the structure of the Bridge table:</p> <ul> <li>Each Bridge table contains a <code>zero record</code>, which provides an 'unknown' record   that replaces to any NULL value.</li> <li>Each Bridge record receives a surrogate key that uniquely identifies a row in   the Bridge table. This is purely for identification purposes.</li> <li>The load date / time stamp used is derived from the involved Hub and Link tables.   When loading the Bridge table, it will reuse the lowest (earliest) value from   the tables that are in scope. This value contains the earliest moment in time   a relationship between Core Business Concepts could be established.</li> </ul> <p>Note: \\ Bridge tables are incrementally updated when new data becomes available in the Data Vault tables that support them.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#reference","title":"Reference","text":"<p>Note: \\ (Raw Vault)</p> <p>They are present to avoid data storage redundancy for the values used a lot. For example, a satellite can store a product category code instead of de-normalizing the whole relationship, so bring the category description, full name to the satellite itself, and use the reference table to retrieve this information when needed. The reference tables are built from the satellite, hub or links values entities but are often used only by the satellites since they're the most \"descriptive\" part of the data vault modeling.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#staging","title":"Staging","text":"<p>Exactly like a staging area, staging tables are temporary structures used to improve the load experience. They can also provide the Data Vault features if the raw data's loading process to the data warehouse storage doesn't support them. More exactly, you will find there a staging table and a second-level staging table that will load the raw data from the staging table and add all required attributes like a hash key, load times or hash difference key)</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#types-of-data-vault","title":"Types of Data Vault","text":""},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#raw-vault","title":"Raw Vault","text":"<p>Raw Vault provides the modelled outcome of business processes from source systems, business vault extends those business processes to how the business sees them. Business rules supplied to Raw Vault are idempotent, business vault\u2019s rules must be the same.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#business-vault","title":"Business Vault","text":"<p>Data that has been modified by the business rules and is modeled in DV style tables; sparsely created.</p> <p>So, the raw vault is the raw, unfiltered data from the source systems that has been loaded into hubs, links and satellites based on business keys. The business vault is an extension of a raw vault that applies selected business rules, de-normalizations, calculations and other query-assistance functions. It does this in order to facilitate user access and reporting.</p> <p>A Business Vault is modeled in DV style tables, such as hubs, links and satellites, but it is not a complete copy of all objects in the Raw Vault. Within it, there will only be structures that hold some significant business value.</p> <p>The data will be transformed in a way to apply rules or functions that most of the business users find useful as opposed to doing these repeatedly into multiple marts. This includes things like data cleansing, data quality, accounting rules or repeatable calculations.</p> <p>In Data Vault 2.0, the Business Vault also includes some special entities that help build more efficient queries against the Raw Vault. These are Point-in-Time and Bridge Tables.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#metric-vault","title":"Metric Vault","text":"<p>The Metrics Vault is an optional tier used to hold operational metrics data for the Data Vault ingestion processes. This information can be invaluable when diagnosing potential problems with ingestion. It can also act as an audit trail for all the processes that are interacting with the Data Vault.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#error-marts","title":"Error Marts","text":"<p>Error Marts are an optional layer in the Data Vault that can be useful for surfacing data issues to the business users. Remember that all data, correct or not, should remain as historical data in the Data Vault for audit and traceability.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#metrics-marts","title":"Metrics Marts","text":"<p>The Metrics Mart is an optional tier used to surface operational metrics for analytical o r reporting purposes.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#data-vault-pipeline","title":"Data Vault Pipeline","text":"<p>Let\u2019s run through each stage of the pipeline</p> <ol> <li> <p>Data is landed either as an INSERT OVERWRITE or INSERT ONLY. Without dropping    the target\u2019s contents then this is the first place we can use Streams to process    new records only downstream.</p> </li> <li> <p>Landed content is staged with data vault metadata tags, some of these are:</p> </li> <li> <p>Surrogate hash keys \u2013 for joining related data vault tables</p> </li> <li>Load date \u2013 the timestamp of when the data enters the data warehouse</li> <li>Applied date \u2013 the timestamp of the landed data</li> <li>Record source \u2013 where the data came from</li> <li> <p>Record hashes \u2013 a single column representing a collection of descriptive attributes</p> </li> <li> <p>Autonomous loads through</p> </li> <li> <p>Hub loaders \u2013 a template reused to load one or many hub tables</p> </li> <li>Link loaders \u2013 a template reused to load zero or many link tables</li> <li> <p>Sat loaders \u2013 a template reused to load zero or many satellite tables</p> </li> <li> <p>Test Automation measuring the integrity of all loaded (and related) data vault    artefacts from the staged content</p> </li> <li> <p>Snapshot is taken of the current load dates and surrogate hash keys from the    parent entity (hub or link) and adjacent satellite tables.</p> </li> <li> <p>Use the AS_OF date table to control the PIT manifold to periodically populate    target PIT tables at the configured frequency.</p> </li> <li> <p>PIT</p> <p></p> <p>A Point-in-Time (PIT) table is a physical collection of applicable surrogate  keys and load dates for a snapshot period. It must be a table otherwise the  potential benefits of EQUIJOIN are not realised.</p> <p>It improves join performance and forms the base for information mart views  and easily allows you to define marts to be built for specific logarithmic  time windows by a snapshot date. The traditional approach to building a PIT  table makes use of an adjacent date table to define the logarithmic period  and the PIT windows itself.</p> </li> <li> <p>AS_OF</p> <p></p> <p>AS_OF table controls the PIT snapshot in a combination of two ways</p> <ul> <li>By Window: defining the start and end date of the AS_OF period, the window    of snapshots to take. You can define a much larger than needed table but    subset the window in execution.</li> <li>By Frequency: defining at what frequency snapshot keys are to be sent to    a PIT target. This is tailored by the business and is a part of the report    frequency and depth. Ideally this would not have any involvement by engineering    teams, only to set this up. From there the business controls the 1 and 0 switches.</li> </ul> </li> <li> <p>Information Marts that are defined once as views over a designated PIT table.</p> </li> </ol>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#data-vault-architecture","title":"Data Vault Architecture","text":"<ul> <li> <p>Raw Source (Copy from your OLTP data sources)</p> </li> <li> <p>Staging (nowadays as Persistent Staging Area in a Datalake, because it is cheaper than a Relational DB)</p> </li> <li> <p>Raw Vault (applying so-called Hard Rules, like data type changes)</p> </li> <li> <p>Business Vault (applying so-called Soft Rules, all your Business Logic, Aggregations, Concatenation, ...)</p> </li> <li> <p>Information Mart (Data Mart sometimes virtualized, but not always ... usually Star/Snowflake Schema)</p> </li> <li> <p>Cube/Tabular Model</p> </li> <li> <p>BI Tool</p> </li> </ul> <p>Hard Rules: \\ These should be applied before data is stored in the DataVault. Any rules applied here do not alter the contents or the granularity of the data, and maintains auditability.</p> <ul> <li>Data typing</li> <li>Normalization / De-normalization</li> <li>Adding system fields (tags)</li> <li>De-duplication</li> <li>Splitting by record structure</li> <li>Trimming spaces from character strings</li> </ul> <p>Soft Rules: \\ Rules that change, or interpret the data, for example adds business logic. This changes the granularity of the data.</p> <ul> <li>Concatenating name fields</li> <li>Standardizing addresses</li> <li>Computing monthly sales</li> <li>Coalescing</li> <li>Consolidation</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#data-vault-modeling-approaches","title":"Data Vault Modeling Approaches","text":"<ul> <li> <p>Bottom-up approach (Source-Driven)   This is a source centric design approach, where you rely on understanding the   source systems and design your DV model based on that.</p> </li> <li> <p>Top-down approach (Business-Driven)   This is a business centric design approach, where you start by understanding the   business needs, use cases, and business keys in order to design the DV Model.</p> </li> <li> <p>Combined approach   This is a hybrid approach, where you use both Bottom-up and Top-down approaches.</p> </li> </ul> <p>Examples:</p> <ul> <li>Hubs and Links follow Top-down approach</li> <li>Satellite follows Bottom-up approach</li> </ul> <p></p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#unit-of-work","title":"Unit of Work","text":"<ul> <li>https://hanshultgren.wordpress.com/2011/05/04/unit-of-work-demystified/</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#advantages-of-data-vault","title":"Advantages of Data Vault","text":"<ul> <li>Scalable \\   One of the biggest advantages of Data Vault model is the ability to scale up or down   quickly - a huge asset for companies going through growth or transition periods.</li> </ul> <p>Because satellites are source system specific, adding sources is an easy as adding   satellites. No updates to existing satellite tables are required.</p> <p>Note: \\ Update may be necessary for views in the information mart</p> <ul> <li>Repeatable \\   Three main entities - Hubs, Links, and Satellites - all follow the same pattern.   Scripts to build tables or run ETL processes can be automated based on these patterns   and metadata.</li> </ul> <p>A number of services and programs exist to quickly automate these processes</p> <ul> <li>Auditable \\   Build on core principle that data is never deleted, and all data is loaded in   its original format. Record source column in entities allows for tracking back   to source system.</li> </ul> <p>Tracking of business keys and separation of business keys (Hubs) from context (Satellites)   allows for easier compliance with GDPR &amp; similar data protection regulations.</p> <ul> <li> <p>Adaptable \\   Separation of hard and soft rules allows for quicker updated.</p> </li> <li> <p>Hard: Any rule that does not change content of individual fields or gain.</p> </li> <li>Soft: Any rule that changes or interprets data, or changes the gain (turning data into information)</li> </ul> <p>Changes in business logic requires no change to ETL processes - only updates   to virtualized information mart layer.</p> <p>Fit within an Agile Framework</p> <ul> <li> <p>Optimized Loading</p> </li> <li> <p>Decreased process complexity</p> </li> <li>Decreased amount of data being processed</li> <li> <p>Increased opportunities for parallelization</p> </li> <li> <p>Platform Agnostic   A data vault architecture and model can be built on many platforms - both on premise   and on cloud. Initial design for Data Vaults were to handle batch processing,   but patterns also now exist for handling streaming data.</p> </li> </ul> <p>Good fit for:</p> <ul> <li>Enterprise teams where the ability to audit data is a primary concern</li> <li>Teams that need flexibility and who want to make large structural changes to their data without causing delays in reporting</li> <li>More technical data teams that can manage and govern the network-like growth of data vault models</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#disadvantages","title":"Disadvantages","text":"<ul> <li>Training &amp; Complexity   Because the Data Vault is not a well known modeling technique, hiring or training   staff may be an issue or expense.</li> </ul> <p>Data Vault models have a tendency to become very large and complex, which may be   a daunting process for those new to the technique.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#conclusion","title":"Conclusion","text":"<ul> <li>Fix the SCD2 problem, because of SAT table can only insert strategy not update, or delete.</li> <li>Any record have unique key.</li> <li>Can do Change Data Capture (CDC) because all record have end_datetime tracking.</li> <li>\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e47\u0e19 Agile Data Warehouse \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e16\u0e49\u0e32 business process \u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19 \u0e01\u0e47\u0e44\u0e21\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19 data model \u0e43\u0e2b\u0e21\u0e48\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14 \u0e41\u0e04\u0e48\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e2b\u0e23\u0e37\u0e2d\u0e41\u0e01\u0e49 Sat \u0e44\u0e21\u0e48\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e01\u0e31\u0e1a Dimension model \u0e17\u0e35\u0e48 fact table \u0e43\u0e2b\u0e0d\u0e48\u0e21\u0e32\u0e01\u0e41\u0e25\u0e30\u0e40\u0e01\u0e47\u0e1a relation \u0e15\u0e48\u0e32\u0e07\u0e46\u0e44\u0e27\u0e49 \u0e08\u0e36\u0e07\u0e41\u0e01\u0e49\u0e44\u0e02\u0e44\u0e14\u0e49\u0e22\u0e32\u0e01\u0e40\u0e1e\u0e23\u0e32\u0e30\u0e08\u0e30\u0e01\u0e23\u0e30\u0e17\u0e1a Relation \u0e17\u0e35\u0e48\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19 fact \u0e19\u0e31\u0e49\u0e19\u0e44\u0e1b\u0e2b\u0e21\u0e14 \u0e07\u0e48\u0e32\u0e22\u0e17\u0e35\u0e48\u0e04\u0e37\u0e2d\u0e44\u0e1b\u0e2a\u0e23\u0e49\u0e32\u0e07 Fact \u0e43\u0e2b\u0e21\u0e48\u0e17\u0e35\u0e48\u0e21\u0e35\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e0b\u0e49\u0e33\u0e0b\u0e49\u0e2d\u0e19\u0e01\u0e31\u0e1a Fact \u0e40\u0e01\u0e48\u0e32\u0e1a\u0e32\u0e07\u0e2a\u0e48\u0e27\u0e19 \u0e01\u0e47\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e1b\u0e25\u0e37\u0e2d\u0e07 Data Storage \u0e2d\u0e35\u0e01</li> <li>\u0e16\u0e49\u0e32\u0e21\u0e35 Entity Relation \u0e43\u0e2b\u0e21\u0e48 \u0e40\u0e02\u0e49\u0e32\u0e21\u0e32 \u0e08\u0e30\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e02\u0e49\u0e32\u0e17\u0e35\u0e48 Link table \u0e44\u0e14\u0e49\u0e17\u0e31\u0e19\u0e17\u0e35</li> <li>Data Sources \u0e21\u0e35\u0e2b\u0e25\u0e32\u0e01\u0e2b\u0e25\u0e32\u0e22 \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e09\u0e30\u0e19\u0e31\u0e49\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30 Record \u0e02\u0e2d\u0e07\u0e17\u0e38\u0e01 table \u0e08\u0e30\u0e15\u0e49\u0e2d\u0e07\u0e23\u0e30\u0e1a\u0e38 Record Source \u0e27\u0e48\u0e32\u0e21\u0e31\u0e19\u0e21\u0e32\u0e08\u0e32\u0e01 data source \u0e44\u0e2b\u0e19 \u0e23\u0e30\u0e1a\u0e1a\u0e44\u0e2b\u0e19</li> <li>\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e40\u0e01\u0e47\u0e1a data \u0e40\u0e1b\u0e47\u0e19 centralize data warehouse/hub \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e44\u0e27\u0e49\u0e43\u0e0a\u0e49\u0e40\u0e1b\u0e47\u0e19 source \u0e02\u0e2d\u0e07 data mart, Adhoc analytics \u0e2b\u0e23\u0e37\u0e2d ETL/ELT application \u0e15\u0e48\u0e2d\u0e44\u0e1b</li> <li>\u0e44\u0e21\u0e48\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e17\u0e33 Reporting \u0e17\u0e35\u0e48 Data Vault \u0e40\u0e1e\u0e23\u0e32\u0e30 data \u0e17\u0e35\u0e48\u0e19\u0e35\u0e48\u0e22\u0e31\u0e07\u0e44\u0e21\u0e48\u0e21\u0e35\u0e01\u0e32\u0e23 Transform, Cleansing</li> <li>\u0e04\u0e27\u0e23\u0e17\u0e33 Data Marts, ETL / ELT \u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e17\u0e33 Analytics, Reporting</li> <li>\u0e04\u0e19\u0e17\u0e35\u0e48 Implement&amp;Maintain Data Vault \u0e15\u0e49\u0e2d\u0e07\u0e21\u0e35\u0e2a\u0e01\u0e34\u0e25\u0e23\u0e30\u0e14\u0e31\u0e1a\u0e19\u0e36\u0e07\u0e40\u0e1e\u0e23\u0e32\u0e30 Surrogate Key \u0e08\u0e30\u0e43\u0e0a\u0e49\u0e01\u0e32\u0e23 Hashing \u0e17\u0e33 \u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48\u0e40\u0e1e\u0e35\u0e22\u0e07\u0e01\u0e32\u0e23\u0e17\u0e33\u0e40\u0e1b\u0e47\u0e19 run_id \u0e40\u0e09\u0e22\u0e46</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-approach/#references","title":"References","text":"<ul> <li>https://www.databricks.com/glossary/data-vault</li> <li>https://www.databricks.com/blog/2022/06/24/prescriptive-guidance-for-implementing-a-data-vault-model-on-the-databricks-lakehouse-platform.html</li> <li>https://atitawat.medium.com/data-vault-%E0%B8%84%E0%B8%B7%E0%B8%AD%E0%B8%AD%E0%B8%B0%E0%B9%84%E0%B8%A3-part1-2f4cf602ed6c</li> <li>https://medium.com/hashmapinc/getting-started-with-data-vault-2-0-c19945874fe3</li> <li>https://medium.com/hashmapinc/3nf-and-data-vault-2-0-a-simple-comparison-4b0694c9a1d1</li> <li>https://aginic.com/blog/modelling-with-data-vaults/</li> <li>https://www.waitingforcode.com/general-big-data/data-vault-2-big-data/read</li> <li>https://www.scalefree.com/scalefree-newsletter/point-in-time-tables-insurance/</li> <li>https://digitalcommons.georgiasouthern.edu/cgi/viewcontent.cgi?article=2402&amp;context=etd ***</li> <li>https://www.linkedin.com/pulse/data-vault-pit-flow-manifold-patrick-cuba/</li> <li>https://medium.com/snowflake/data-vault-naming-standards-76c93413d3c7</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-bussiness-vault-act-schema/","title":"Data Vault: Business Vault &amp; Activity Schema","text":"<p>https://patrickcuba.medium.com/business-vault-activity-schema-03f9b30c11d9</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-implement/","title":"Data Vault Implementation","text":""},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-implement/#hubs","title":"Hubs","text":"<pre><code>CREATE PROCEDURE [RV].[LOAD_PRODUCT_HUB]\n    @LOAD_PRCS      BIGINT\nAS\n    WITH NewProductBussKey AS\n    (\n        SELECT DISTINCT product_number\n        FROM [STG].[PRODUCT_TABLE_2022]     AS SRC\n        LEFT OUTER JOIN [RV].[HUB_PRODUCT]  AS TGT\n            ON SRC.product_number = TGT.product_key\n        WHERE TGT.product_key IS NULL\n    )\n    INSERT INTO [RV].[HUB_PRODUCT] WITH (TABLOCK) -- bulk load\n    SELECT\n        HASH(product_number)                AS product_hash_key,\n        product_number                      AS product_key,\n        GETDATE()                           AS LDTS,\n        @LOAD_PRCS                          AS LDID, -- unique ID for load\n        'PRODUCT_TABLE_2022'                AS Record_Source\n    FROM [STG].[PRODUCT_TABLE_2022]\nRETURN 0\n</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-implement/#links","title":"Links","text":"<pre><code>CREATE PROCEDURE [RV].[LOAD_SALES_PRODUCT_LINK]\n    @LOAD_PRCS      BIGINT\nAS\n    WITH NewDistinctBusinessKey AS\n    (\n        SELECT DISTINCT\n            H_SALES.sales_key,\n            H_PRODUCT.product_key\n        FROM [STG].[SALES_ORDER_DETAIL_2022]        AS SRC\n        LEFT OUTER JOIN [RV].[HUB_SALES]            AS H_SALES -- lookup sales key\n            ON SRC.sales_order_id = H_SALES.sales_key\n        LEFT OUTER JOIN [RV].[HUB_PRODUCT]          AS H_PRODUCT -- lookup product key\n            ON SRC.product_number = H_PRODUCT.product_key\n        LEFT OUTER JOIN [RV].[LINK_SALES_PRODUCT]     AS TGT\n            ON TGT.product_key  = H_PRODUCT.product_key\n            AND TGT.sales_key   = H_SALES.sales_key\n        WHERE TGT.sales_key IS NULL\n    )\n    INSERT INTO [RV].[LINK_SALES_PRODUCT] WITH (TABLOCK) -- bulk load\n    SELECT\n        HASH(sales_order_id, product_number)        AS link_sales_product_hash_key,\n        sales_order_id                              AS sales_key,\n        product_number                              AS product_key,\n        GETDATE()                                   AS LDTS,\n        @LOAD_PRCS                                  AS LDID, -- unique ID for load\n        'SALES_ORDER_DETAIL_2022'                   AS Record_Source\n    FROM [STG].[SALES_ORDER_DETAIL_2022]\nRETURN 0\n</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-implement/#satellites","title":"Satellites","text":"<pre><code>CREATE PROCEDURE [RV].[Load_SAT_ProductDetails]\n    @LOAD_PRCS BIGINT\nAs\n    DECLARE @RecordSource        nvarchar(100)\n    DECLARE @DefaultValidFrom    datetime2(0)     \u2013use datetime2(0) to remove milliseconds\n    Declare @DefaultValidTo      datetime2(0)\n    DECLARE @LoadDateTime        datetime2(0)\n    SET @RecordSource            = N\u2019AdventureWorks2012.Product.Product\u2018\n    SET @DefaultValidFrom        = \u201a1900-01-01\u2018\n    SET @DefaultValidTo          = \u201a9999-12-31\u2018\n    SET @LoadDateTime            = GETDATE()\nBEGIN TRY\nBegin Transaction\n    INSERT INTO [RV].[SAT_ProductDetails_DV20]\n    (\n         Product_Hsk,\n         LoadTimestamp,\n         Name,\n         ListPrice,\n         LOAD_PRCS,\n         RecordSource,\n         ValidFrom,\n         ValidTo,\n         IsCurrent\n    )\n    SELECT\n         Product_Hsk,                            \u2013Hash Key\n         @LoadDateTime,                          \u2013LoadDatetimeStamp\n         Name,\n         ListPrice,\n         @LOAD_PRCS as LOAD_PRCS,\n         @RecordSource as RecordSource,\n         @LoadDateTime,                          \u2013Actual DateTimeStamp\n         @DefaultValidTo,                        \u2013Default Expiry DateTimestamp\n         1                                       \u2013IsCurrent Flag\n    FROM\n    (\n        MERGE [RV].[SAT_ProductDetails_DV20]       AS Target     \u2013Target: Satellite\n        USING\n        (\n           \u2014 Query distinct set of attributes from source (stage)\n           SELECT DISTINCT\n               stage.Product_Hsk,\n               stage.Name,\n           stage.ListPrice\n           FROM stage.Product_Product_AdventureWorks2012_DV20 as stage\n        ) AS Source\n            ON Target.Product_Hsk = Source.Product_Hsk         \u2013Identify Columns by Hub/Link Hash Key\n            AND Target.IsCurrent = 1                           \u2013and only merge against current records in the target\n        \u2013when record already exists in satellite and an attribute value changed\n        WHEN MATCHED AND\n            (\n                 Target.Name &lt;&gt; Source.Name\n                 OR Target.ListPrice &lt;&gt; Source.ListPrice\n            )\n        \u2014 then outdate the existing record\n        THEN UPDATE SET\n            IsCurrent  = 0,\n            ValidTo    = @LoadDateTime\n        \u2014 when record not exists in satellite, insert the new record\n        WHEN NOT MATCHED BY TARGET\n        THEN INSERT\n        (\n            Product_Hsk,\n            LoadTimestamp,\n            Name,\n            ListPrice,\n            LOAD_PRCS,\n            RecordSource,\n            ValidFrom,\n            ValidTo,\n            IsCurrent\n        )\n        VALUES\n        (\n            Source.Product_Hsk,\n            @LoadDateTime,\n            Source.Name,\n            Source.ListPrice,\n            @LOAD_PRCS,\n            @RecordSource,\n            @DefaultValidFrom,     \u2013Default Effective DateTimeStamp\n            @DefaultValidTo,       \u2013Default Expiry DateTimeStamp\n            1                      \u2013IsCurrent Flag\n        )\n        \u2014 Output changed records\n        OUTPUT\n            $action AS Action\n            ,Source.*\n    ) AS MergeOutput\n    WHERE MergeOutput.Action = 'UPDATE' AND Product_Hsk IS NOT NULL;\n\n    COMMIT\n\n    SELECT\n        'Success' as ExecutionResult\n    RETURN;\nEND TRY\nBEGIN CATCH\n     IF @@TRANCOUNT &gt; 0\n     ROLLBACK\n     SELECT\n          'Failure' as ExecutionResult,\n          ERROR_MESSAGE() AS ErrorMessage;\n     RETURN;\nEND CATCH\nGO\n</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-implement/#dimension","title":"Dimension","text":"<pre><code>CREATE OR REPLACE VIEW v_curr_customer AS\nSELECT hub.h_customer_key\n     , hub.customer_no\n     , s1.preferred_contact\n     , s1.e_mail_address\n     , s1.phone_number\n     , s1.private_person\n     , s1.reseller\n     , s1.delivery_type\n     , s2.last_name cust_last_name\n     , s2.first_name cust_first_name\n     , s2.street cust_street\n     , s2.street_no cust_street_no\n     , s2.zip_code cust_zip_code\n     , s2.city cust_city\n     , s3.last_name bill_last_name\n     , s3.first_name bill_first_name\n     , s3.street bill_street\n     , s3.street_no bill_street_no\n     , s3.zip_code bill_zip_code\n     , s3.city bill_city\n  FROM h_customer hub\n  JOIN pit_customer pit\n    ON (hub.h_customer_key = pit.h_customer_key)\n  LEFT JOIN s_customer_info s1\n    ON (s1.h_customer_key = pit.h_customer_key AND s1.load_date = pit.s1_load_date)\n  LEFT JOIN s_customer_address s2\n    ON (s2.h_customer_key = pit.h_customer_key AND s2.load_date = pit.s2_load_date)\n  LEFT JOIN s_billing_address s3\n    ON (s3.h_customer_key = pit.h_customer_key AND s3.load_date = pit.s3_load_date)\n WHERE pit.load_end_date IS NULL;\n</code></pre> <pre><code>EXPLAIN PLAN FOR\nSELECT h_customer_key\n     , customer_no     \u2014 H_CUSTOMER\n     , e_mail_address  \u2014 S_CUSTOMER_INFO\n     , cust_first_name \u2014 S_CUSTOMER_ADDRESS\n     , bill_first_name \u2014 S_BILLING_ADDRESS\n  FROM v_curr_customer;\n\nSELECT * FROM dbms_xplan.display();\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\n| Id  | Operation                     | Name               | Rows  |\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\n|   0 | SELECT STATEMENT              |                    | 20000 |\n|*  1 |  HASH JOIN RIGHT OUTER        |                    | 20000 |\n|   2 |   TABLE ACCESS STORAGE FULL   | S_CUSTOMER_INFO    | 27000 |\n|*  3 |   HASH JOIN RIGHT OUTER       |                    | 20000 |\n|   4 |    TABLE ACCESS STORAGE FULL  | S_BILLING_ADDRESS  |  4964 |\n|*  5 |    HASH JOIN OUTER            |                    | 20000 |\n|*  6 |     HASH JOIN                 |                    | 20000 |\n|   7 |      TABLE ACCESS STORAGE FULL| H_CUSTOMER         | 20000 |\n|*  8 |      TABLE ACCESS STORAGE FULL| PIT_CUSTOMER       | 20000 |\n|   9 |     TABLE ACCESS STORAGE FULL | S_CUSTOMER_ADDRESS | 33406 |\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\n</code></pre> <pre><code>EXPLAIN PLAN FOR\nSELECT h_customer_key\n   \u2013, customer_no     \u2014 H_CUSTOMER\n     , e_mail_address  \u2014 S_CUSTOMER_INFO\n     , cust_first_name \u2014 S_CUSTOMER_ADDRESS\n   \u2013, bill_first_name \u2014 S_BILLING_ADDRESS\n  FROM v_curr_customer;\n\nSELECT * FROM dbms_xplan.display();\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n| Id  | Operation                   | Name               | Rows  |\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n|   0 | SELECT STATEMENT            |                    | 20000 |\n|*  1 |  HASH JOIN RIGHT OUTER      |                    | 20000 |\n|   2 |   TABLE ACCESS STORAGE FULL | S_CUSTOMER_INFO    | 27000 |\n|*  3 |   HASH JOIN OUTER           |                    | 20000 |\n|*  4 |    TABLE ACCESS STORAGE FULL| PIT_CUSTOMER       | 20000 |\n|   5 |    TABLE ACCESS STORAGE FULL| S_CUSTOMER_ADDRESS | 33406 |\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-implement/#loading-dimensions-from-data-vault-model","title":"Loading Dimensions From Data Vault Model","text":"<p>https://danischnider.wordpress.com/2021/12/20/multi-version-load-in-data-vault/ https://danischnider.wordpress.com/2015/11/12/loading-dimensions-from-a-data-vault-model/</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-implement/#insert-only","title":"Insert-only","text":"<p>https://www.scalefree.com/scalefree-newsletter/insert-only-in-data-vault/</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-implement/#tips-to-get-the-best-performance-out-of-a-data-vault-model-in-databricks-lakehouse","title":"Tips to get the best performance out of a Data Vault Model in Databricks Lakehouse","text":"<ul> <li> <p>Use Delta Formatted tables for Raw Vault, Business Vault and Gold layer tables.</p> </li> <li> <p>Make sure to use OPTIMIZE and Z-order indexes on all join keys of Hubs, Links   and Satellites.</p> </li> <li> <p>Do not over partition the tables -especially the smaller satellites tables.   Use Bloom filter indexing on Date columns, current flag columns and predicate   columns that are typically filtered on to ensure the best performance - especially   if you need to create additional indices apart from Z-order.</p> </li> <li> <p>Delta Live Tables (Materialized Views) makes creating and managing PIT tables very easy.</p> </li> <li> <p>Reduce the <code>optimize.maxFileSize</code> to a lower number, such as 32-64MB vs. the   default of 1 GB. By creating smaller files, you can benefit from file pruning   and minimize the I/O retrieving the data you need to join.</p> </li> <li> <p>Data Vault model has comparatively more joins, so use the latest version of DBR   which ensures that the Adaptive Query Execution is ON by default so that the best   Join strategy is automatically used. Use Join hints only if necessary.   (for advanced performance tuning).</p> </li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-implement/#implements","title":"Implements","text":"<ul> <li>Data Vault on Snowflake</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-data-vault-implement/#references","title":"References","text":"<ul> <li>https://dbtvault.readthedocs.io/en/latest/</li> <li>https://www.oraylis.de/blog/2016/data-vault-satellite-loads-explained</li> <li>https://www.databricks.com/blog/2022/06/24/prescriptive-guidance-for-implementing-a-data-vault-model-on-the-databricks-lakehouse-platform.html</li> <li>https://datavaultalliance.com/news/building-a-real-time-data-vault-in-snowflake/ ***</li> <li>https://jerryrun.wordpress.com/2018/09/12/chapter-4-data-vault-20-modeling/</li> <li>https://github.com/dbsys21/databricks-lakehouse/blob/main/lakehouse-buildout/data-vault/TPC-DLT-Data-Vault-2.0.sql</li> <li>https://aws.amazon.com/blogs/big-data/design-and-build-a-data-vault-model-in-amazon-redshift-from-a-transactional-database/</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-implement/","title":"Dimension Model Implementation","text":""},{"location":"abstract/data_architecture/data_modeling/dwh-dim-implement/#example-01","title":"Example: 01","text":"<p>I will discuss Star Schema and how to create one using credit-debit transaction dataset in Snowflake data warehouse. This post is more suitable to those who are new to Star Schema or Dimensional Modeling. But if you are a seasoned data modeler, you are more than welcome to read this post and please leave your feedback in the comment section.</p> <p>So how do we build a Star Schema? As proposed by Kimball, there are 4 steps in designing of a dimensional model.</p> <ol> <li> <p>Select the business process. The first step is to identify the business    process that you want to model. Model the processes that are most significant    or relevant to the business first.</p> </li> <li> <p>Declare the grain. Grain refers to the level of detail of the information    that you will store in the fact table. The grain should be at the most atomic    or lowest level possible. For example, A line item on a grocery receipt. The    grocery owner might want to ask questions such as \u201cwhat are the items that    sold the best during the day in our grocery store?\u201d, and to answer this question    we need to dig into line-item level instead of the order-level.</p> </li> <li> <p>Identify the dimensions. You can identify the dimensions by looking at the    descriptive information or attributes that exist in your business process and    provide context to your measurable events. For example: payment method, customers,    locations, etc.</p> </li> <li> <p>Identify the facts. Facts are the quantitative measures in your business    process that are always in numeric. For example: price, minutes, speed, etc.    You should identify/select the measures that are true to your selected grain.</p> </li> </ol> <p>The dataset has 23 columns but for simplicity, I will exclude 4 irrelevant columns. Here are the descriptions of the columns.</p> Columns Descriptions TRANSACTION_REFERENCE The transaction identifier for each transaction made by consumer USER_REFERENCE The user identifier of the consumer AGE_BAND The consumer age range SALARY_BAND The consumer salary range. POSTCODE The postcode of where the consumer lives. LSOA Geographical hierarchy: small areas that has similar population size (average of approximately 1,500 residents or 650 households). MSOA Geographical hierarchy: medium areas where the minimum population size is 5000 (average of 7200). DERIVED_GENDER The consumer gender identity. TRANSACTION_DATE The transaction date made by the consumer. ACCOUNT_REFERENCE The consumer bank account identifier. PROVIDER_GROUP_NAME The consumer's bank for executing his/her transactions. ACCOUNT_TYPE The account type: current, savings, etc. CREDIT_DEBIT Type of transaction made by consumer: debit or credit. AMOUNT The amount of transaction. AUTO_PURPOSE_TAG_NAME The transaction purpose. MERCHANT_NAME The merchant's name. MERCHANT_BUSINESS_LINE The merchant's business category. ACCOUNT_CREATED_DATE The date of when the account first created. ACCOUNT_LAST_REFRESHED The date of when the account last updated."},{"location":"abstract/data_architecture/data_modeling/dwh-dim-implement/#dim_users","title":"DIM_USERS","text":"<p>DIM_USERS will store users\u2019 demographic information such as user id, age, salary, gender and address.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.dim_users AS (\n    SELECT\n        DISTINCT user_reference AS user_id,\n        age_band,\n        salary_band,\n        postcode,\n        LSOA,\n        MSOA,\n        derived_gender AS gender\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-implement/#dim_accounts","title":"DIM_ACCOUNTS","text":"<p>DIM_ACCOUNTS stores account level attributes such as account id, bank name and account type.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.dim_accounts AS (\n    SELECT\n        DISTINCT account_reference AS account_id,\n        provider_group_name AS bank_name,\n        account_type,\n        account_created_date,\n        account_last_refreshed\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-implement/#dim_merchants","title":"DIM_MERCHANTS","text":"<p>All information about the merchants such as merchant\u2019s name and business category will be stored in DIM_MERCHANTS. The dataset does not provide merchant identifier, so in this case I have decided to create a surrogate key for merchant\u2019s key identifier.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.dim_merchants AS (\n    SELECT\n        DISTINCT HASH(merchant_name, merchant_business_line)::VARCHAR AS merchant_id,\n        merchant_name,\n        merchant_business_line\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-implement/#dim_transactions","title":"DIM_TRANSACTIONS","text":"<p>DIM_TRANSACTIONS stores information on transaction attributes such as transaction type and purpose.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.dim_transactions AS (\n    SELECT\n        transaction_reference AS transaction_id,\n        credit_debit AS transaction_type,\n        auto_purpose_tag_name AS transaction_purpose\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-implement/#dim_dates","title":"DIM_DATES","text":"<p>I decided to create a data dimension that stores all date related parsed values such as day of the month, day name, month of the year, month name, etc. This will be handy when we need to generate time based reports.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.dim_dates AS (\n    SELECT\n        DISTINCT transaction_date,\n        DAY(transaction_date)::VARCHAR as day_of_month,\n        DAYNAME(transaction_date) as day_name,\n        MONTH(transaction_date)::VARCHAR as month_of_year,\n        MONTHNAME(transaction_date) as month_name,\n        YEAR(transaction_date)::VARCHAR as year\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-implement/#fct_transactions","title":"FCT_TRANSACTIONS","text":"<p>FCT_TRANSACTIONS will store the numeric information (transaction amount) and foreign keys that connect it to the dimension tables. To note, I also add transaction date column as a way to connect the FCT_TRANSACTIONS to DIM_DATES table. A better approach is to use surrogate key to generate date identifier but that is outside the scope of this post.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.fct_transactions AS (\n    SELECT\n        transaction_date AS transaction_date,\n        transaction_reference AS transaction_id,\n        user_reference AS user_id,\n        account_reference AS account_id,\n        HASH(merchant_name, merchant_business_line)::VARCHAR AS merchant_id,\n        amount::NUMBER as amount\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre> <p></p>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-implement/#references","title":"References","text":"<ul> <li>https://danischnider.wordpress.com/2022/11/10/star-schema-design-in-oracle-fundamentals/</li> <li>https://blog.devgenius.io/implementing-star-schema-in-snowflake-data-warehouse-1f890cdda952</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-rapidly-changing-dimension/","title":"Rapidly Changing Dimension","text":"<p>A dimension is a fast changing or rapidly changing dimension if one or more of its attributes in the table changes very fast and in many rows. Handling rapidly changing dimension in data warehouse is very difficult because of many performance implications.</p> <p>As you know slowly changing dimension type 2 is used to preserve the history for the changes. But the problem with type 2 is, with each and every change in the dimension attribute, it adds new row to the table. If in case there are dimensions that are changing a lot, table become larger and may cause serious performance issues. Hence, use of the type 2 may not be the wise decision to implement the rapidly changing dimensions.</p> <p>Separate Rapidly Changing Attribute by Implementing Junk Dimension</p> <p>Consider the fact table, in which not all the attributes of the table changes rapidly. There may be some attribute that may be changing rapidly and other not. The idea here is to separate the rapidly changing attribute from the slowly changing ones and move those attribute to another table called junk dimension and maintain the slowly changing attribute in same table. In this way, we can handle situation of increasing table size.</p> <p>For example:</p> <p>Consider patient dimension where there are 1000 rows in it. On average basis, each patient changes the 10 of attributes in a year. If you use the type 2 to manage this scenario, there will be 1000*10 = 10000 rows. Imagine if the table has 1 million rows, it\u2019ll become very hard to handle the situation with type 2. Hence, we use rapidly changing dimension approach.</p> <pre><code>erDiagram\n    PATIENT {\n        integer Patient_id PK\n        string Name\n        string Gender\n        string Marital_status\n        numeric Weight\n        numeric BMI\n    }</code></pre> <p>The attribute like patient_id, Name, Gender, Marital_status will not change or changes very rarely. And attribute like weight and BMI (body mass index) changes every month based on the patient visit to hospital. So, we need to separate the weight column out of the patient table otherwise we end up filling the table if we use SCD type 2 on PATIENT dimension. We can put the weight column which is rapidly changing into junk dimension table.</p> <p>Below is the structure of Junk dimension table:</p> <pre><code>erDiagram\n    PATIENT_JNK_DIM {\n        integer Pat_SK PK\n        numeric Weight\n        numeric BMI\n    }</code></pre> <p>Note: Pat_SK is the surrogate key and acts as a primary key for junk dimension table.</p> <p>Link Junk Dimension and PATIENT Table</p> <p>In this step, we must link the junk dimension and patient table. Keep in mind; we cannot simply refer the junk dimension table by adding its primary key to patient table as foreign key. Because any changes made to junk dimension will have to reflect in the patient table, this obviously increases the data in patient dimension. Instead, we create one more table called mini dimension that acts as a bridge between Patient and Junk dimension. We can also add the columns such as start and end date to track the change history. Below is the structure of the mini dimension:</p> <pre><code>erDiagram\n    PATIENT_MINI_DIM {\n        integer Pat_SK PK\n        integer Pat_id PK\n        datetime Start_Date\n        datetime End_Date\n    }</code></pre> <p>This table is just bridge between two tables and does not require any surrogate key in it. Below is the diagrammatic representation of the Rapidly Changing Dimension implementation.</p> <pre><code>erDiagram\n    PATIENT {\n        integer Patient_id PK\n        string Name\n        string Gender\n        string Marital_status\n    }\n    PATIENT_MINI_DIM {\n        integer Pat_SK PK\n        integer Pat_id PK\n        datetime Start_Date\n        datetime End_Date\n    }\n    PATIENT_JNK_DIM {\n        integer Pat_SK PK\n        numeric Weight\n        numeric BMI\n    }\n    PATIENT ||--o{ PATIENT_MINI_DIM : is\n    PATIENT_JNK_DIM ||--o{ PATIENT_MINI_DIM : is</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-rapidly-changing-dimension/#references","title":"References","text":"<ul> <li>https://dwgeek.com/rapidly-changing-dimension-data-warehouse.html/</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-slowly-changing-dimension/","title":"Slowly Change Dimension","text":"<p>It depends on the business requirement, where any particular feature history of changes in the data warehouse is preserved. It is called a slowly changing feature, and a quality dimension is called a slowly changing dimension.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-techniques/","title":"Dimensional Modeling Techniques","text":"<p>Table of Contents:</p> <ul> <li>Dimension Hierarchy Techniques</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-techniques/#dimension-hierarchy-techniques","title":"Dimension Hierarchy Techniques","text":""},{"location":"abstract/data_architecture/data_modeling/dwh-dim-techniques/#fixed-depth-positional-hierarchies","title":"Fixed Depth Positional Hierarchies","text":"<p>A fixed depth hierarchy is a series of many-to-one relationships, such as product to brand to category to department. When a fixed depth hierarchy is defined and the hierarchy levels have agreed upon names, the hierarchy levels should appear as separate positional attributes in a dimension table. A fixed depth hierarchy is by far the easiest to understand and navigate as long as the above criteria are met. It also delivers predictable and fast query performance. When the hierarchy is not a series of many-to-one relationships or the number of levels varies such that the levels do not have agreed upon names, a ragged hierarchy technique must be used.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-techniques/#slightly-raggedvariable-depth-hierarchies","title":"Slightly Ragged/Variable Depth Hierarchies","text":"<p>Slightly ragged hierarchies don\u2019t have a fixed number of levels, but the range in depth is small. Geographic hierarchies often range in depth from perhaps three levels to six levels. Rather than using the complex machinery for unpredictably variable hierarchies, you can force-fit slightly ragged hierarchies into a fixed depth positional design with separate dimension attributes for the maximum number of levels, and then populate the attribute value based on rules from the business.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-dim-techniques/#references","title":"References","text":"<ul> <li>https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-inmon-approach/","title":"Bill Inmon Approach","text":"<p>The Bill Inmon design approach uses the normalized form (3NF) for building entity structure, avoiding data redundancy as much as possible. This results in clearly identifying business requirements and preventing any data update irregularities.</p> <p>The Inmon approach, also known as normalized modeling, is known as the top-down or data-driven strategy, in which we start with the data warehouse and break it down into data marts. These data marts are then specialised to satisfy the demands of other departments inside the firm, such as finance, accounting, and human resources.</p> <ul> <li>Begin with the corporation's data model. Identify all of the data sources that   the company has access to.</li> <li>Identify the essential entities (customer, product, order, etc.) and their respective   linkages based on the data and understanding of business needs.</li> <li>Create a thorough, logical model using the entity structure. The logical model   includes all the properties of each entity, as well as their respective interactions   and co-dependencies, in great detail. According to data modelling terminology,   the logical model creates logical schemas for entity relationships.</li> <li>Build the physical model from the logical one. Extract data from various sources,   alter it and integrate it into a normalised data model using ETL operations.   Each normalised data model stores data in the third normal form to avoid redundancy.   The data warehouse's core is the normalised data model.</li> <li>Create data marts for different departments. For all reporting needs, data marts   are used to access data, and the data warehouse serves as a single source of truth.</li> </ul> <p>Quote</p> <p>Single version of the truth</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-inmon-approach/#advantages-of-the-inmon-approach","title":"Advantages of the Inmon Approach","text":"<p>The following are the main advantages of the Inmon method:</p> <ul> <li> <p>Because it is the only source for data marts and all data in the data warehouse is integrated, the data warehouse genuinely acts as the enterprise's single source of truth.</p> </li> <li> <p>Due to the limited redundancy, data update abnormalities are avoided. This simplifies the ETL(Extraction, transformation, and loading) procedure and reduces the risk of failure.</p> </li> <li> <p>Because the logical model represents the distinct business entities, We may easily understand the business processes.</p> </li> <li> <p>Very versatile \u2014 As business requirements change or source data changes, updating the data warehouse is simple because everything is in one location.</p> </li> <li> <p>Can handle a variety of reporting requirements within the organisation.</p> </li> <li> <p>Flexibility: Inmon's approach adapts faster to changing business needs and data source alterations. Inmon's architecture is more versatile due to the ETL process design that results in normalised data. The architects alter only a few normalised tables, communicating the modification downstream.</p> </li> <li>Single source of truth: Because of the normalised data model, the data warehouse serves as a single source of truth for the entire organisation.</li> <li>Less prone to errors: Because normalisation minimises data redundancy, both engineering and analytical procedures are less susceptible to errors.</li> <li>Completeness: Inmon's approach incorporates all Enterprise data, ensuring that all reporting requirements are met.</li> </ul> <p>Good fit for:</p> <ul> <li>Low complexity data that connects neatly together</li> <li>Simple, business-focused downstream use cases for the data</li> <li>Central data teams that have deep knowledge of the facets of their data</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-inmon-approach/#disadvantages-of-the-inmon-approach","title":"Disadvantages of the Inmon Approach","text":"<p>The following are some drawbacks of the Inmon method:</p> <ul> <li> <p>As more tables and joins are added, the model and implementation can grow increasingly complicated.</p> </li> <li> <p>We'll need people knowledgeable in data modelling and the business in general. These resources might be difficult to come by and can be rather costly.</p> </li> <li> <p>Management should be aware that the initial setup and delivery will take longer.</p> </li> <li> <p>More ETL work is required as the data marts are developed from the data warehouse.</p> </li> <li> <p>A vast team of professionals is required to manage the data environment efficiently.</p> </li> <li> <p>Cost of initial setup and regular maintenance: The time and cost required to set up and maintain Inmon's architecture are far greater than the time and investment needed for Kimball's architecture. Normalised schemas are more challenging to build and maintain than their denormalised counterparts.</p> </li> <li>Skill requirement: Highly skilled engineers are required for Inmon's method, which is harder to come by and more expensive to maintain on the payroll.</li> <li>Extra ETL is required: Separating data marts from the data warehouse necessitates the employment of more ETL processes to generate the data marts, resulting in increased engineering overhead.</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-inmon-approach/#reference","title":"Reference","text":"<ul> <li>Inmon Approach in DWH Designing</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/","title":"Ralph Kimball Approach","text":"<p>https://towardsdatascience.com/data-modeling-for-mere-mortals-part-2-dimensional-modeling-fundamentals-ae2f53622321</p> <p>The Kimball methodology, also known as dimensional modeling, is a bottom-up approach that focuses on designing the data warehouse around the business process or subject area.</p> <p>It uses a star schema or snowflake schema to model the data and focuses on creating dimension and fact tables to support analysis. The Kimball methodology is known for its simplicity, flexibility, and ease of use.</p> <p>In contrast, dimensional models or Kimball dimensional data models, data models based on the technique developed by Ralph Kimball, are denormalized structures designed to retrieve data from a data warehouse. They are optimized to perform the Select operation and are used in the basic design framework to build highly optimized and functional data warehouses.</p> <ul> <li> <p>Kimball\u2019s model follows a bottom-up approach. The Data Warehouse (DW) is provisioned   from Datamarts (DM) as and when they are available or required.</p> </li> <li> <p>The Datamarts are sourced from OLTP systems are usually relational databases in   Third normal form (3NF).</p> </li> <li> <p>The Data Warehouse which is central to the model is a de-normalized star schema.   The OLAP cubes are built on this DWH.</p> </li> </ul> <p>I will say that this is the latest model that serve to Data Science or Data Analytic for using to cube analysis process.</p> <p>In contrast, dimensional models or Kimball dimensional data models, data models based on the technique developed by Ralph Kimball, are denormalized structures designed to retrieve data from a data warehouse. They are optimized to perform the Select operation and are used in the basic design framework to build highly optimized and functional data warehouses.</p> <p>The Kimball approach is called bottom-up because we start with user-specific data marts, which are the core building blocks of our conceptual data warehouse. It's critical to know which model best meets your needs from the start; so that it can be incorporated into the data warehouse structure.</p> <ul> <li>Begin by identifying and documenting the most significant business operations,   demands, and queries that are being asked.</li> <li>All data sources available across the organisation should be documented.</li> <li>Create ETL pipelines that gather, modify, and load data into a de-normalised   data model from data sources. The dimensional model is constructed in the form   of either a star schema or a snowflake schema.</li> <li>The dimensional model has frequently constructed around and within dimensional   data marts for specific departments.</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#what-is-data-dimensional-modelling","title":"What is Data Dimensional Modelling?","text":"<p>Data Dimension Model (DDM) : is a technique that uses Dimensions and Facts to store the data in a Data Warehouse efficiently. The purpose is to optimize the database for faster retrieval of the data. Dimensional Models have a specific structure and organise the data to generate reports that improve performance.</p> <p>A dimensional model in data warehouse is designed to read, summarize, analyze numeric information like values, balances, counts, weights, etc. in a data warehouse. In contrast, relation models are optimized for addition, updating and deletion of data in a real-time Online Transaction System.</p> <p>The concept of Dimensional Modelling was developed by Ralph Kimball and consists of \"fact\" and \"dimension\" tables.</p> <p>This article will introduce the concepts and features of Dimensional Data Modelling, the components that make up a Dimensional Data Model, the types &amp; steps of Dimensional Data Modelling and also the benefits and limitations of Dimensional Data Modelling.</p> <p>Key Features of Dimensional Data Modelling:</p> <p>Data Dimensional Modelling has gained popularity because of its unique way of analysing data present in different Data Warehouses. The 3 main features of DDM are as follows:</p> <ul> <li> <p>Easy to Understand: \\   DDM helps developers create and design databases and Schemas easily interpreted   by business users. The relationship between Dimensions and Facts are pretty simple   to read and understand.</p> </li> <li> <p>Promote Data Quality: \\   DDM schemas enforce data quality before loading into Facts and Dimensions.   Dimension and Fact are tied up by foreign keys that act as a constraint for   referential integrity check to prevent fraudulent data from being loaded onto Schemas.</p> </li> <li> <p>Optimise Performance: \\   DDM breaks the data into Dimensions and Facts and links them with foreign keys,   thereby reducing the data redundancy. The data is stored in the optimised form   and hence occupies less storage and can be retrieved faster.</p> </li> </ul> <p>Hence, Dimensional models are used in data warehouse systems and not a good fit for relational systems.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#components-of-dimensional-data-modelling","title":"Components of Dimensional Data Modelling","text":"<p>There are 5 main components of any DDM.</p> <ol> <li> <p>Dimension \\    Dimensions are the assortment of information that contain data around one or    more business measurements. It may be topographical information, item data,    contacts, and so on. In simple terms, they give who, what, where, and the context    to the fact creation. \\    In other words, a dimension is a window to view information in the facts.</p> </li> <li> <p>Facts/Measures \\    Facts are the collection of measurements, metrics, transactions, etc.,    from different business processes and are almost always numeric.    It typically contains business transactions and measure values.    A single fact table row has a one-to-one relationship to a    measurement event as described by the fact table\u2019s grain. Thus, a fact table    corresponds to a physical observable event, and not to the demands of a particular report.</p> </li> </ol> <p>Within a fact table, only facts consistent with the declared grain are allowed.    For example, in a retail sales transaction, the quantity of a product sold and    its extended price are good facts, whereas the store manager\u2019s salary is disallowed.</p> <ol> <li> <p>Attributes \\    Attributes are the elements of the Dimension Table. For example, in an account Dimension,    the attributes can be:</p> </li> <li> <p>First Name</p> </li> <li>Last Name</li> <li>Phone, etc.</li> </ol> <p>Attributes are used to search, filter, or classify facts.    Dimension Tables contain Attributes</p> <ol> <li>Fact Tables \\    Fact tables are utilized to store measures or transactions in the business.    The Fact Tables are related to Dimension Tables with the keys known as foreign keys. \\    For example, in Internet business, a Fact can store the requested amount of items.    Fact Tables, as a rule, have huge rows and fewer columns.</li> </ol> <p>Note: \\ All fact tables have foreign keys that refers to the dimension tables primary keys to easily connect them to produce specific measure.</p> <ol> <li>Dimension Tables \\    Dimension Tables store the Dimensions from the business and establish the context    for the Facts (They are joined to fact table via a foreign key).    They contain descriptive data that is linked to the Fact Table.    Dimension Tables are usually optimized tables and hence contain large columns    and fewer rows.</li> </ol> <p>For example:</p> <ul> <li>Contact information can be viewed by name, address and phone dimension.</li> <li>Product information can be viewed by product-code, brand, color, etc.</li> <li>City, state, etc. can view store information.</li> </ul> <p>Note: \\ No set limit set for given for number of dimensions \\ The dimension can also contain one or more hierarchical relationships</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#types-of-dimensions-in-dimensional-data-modelling","title":"Types of Dimensions in Dimensional Data Modelling","text":"<p>There are 9 types of Dimensions/metrics when dealing with Dimensional Data Modelling.</p> <ol> <li> <p>Conformed Dimension \\    A Conformed Dimension is a type of Dimension that has the same meaning to all    the Facts it relates to. This type of Dimension allows both Dimensions and    Facts to be categorised across the Data Warehouse.</p> </li> <li> <p>Outrigger Dimension \\    An Outrigger Dimension is a type of Dimension that represents a connection between    different Dimension Tables.</p> </li> <li> <p>Shrunken Dimension \\    A Shrunken Dimension is a perfect subset of a more general data entity.    In this Dimension, the attributes that are common to both the subset and the    general set are represented in the same manner.</p> </li> <li> <p>Role-Playing Dimension \\    A Role-Playing Dimension is a type of table that has multiple valid relationships    between itself and various other tables. Common examples of Role-Playing Dimensions    are time and customers. They can be utilised in areas where certain Facts do not    share the same concepts.</p> </li> <li> <p>Dimension to Dimension Table \\    This type of table is a table in the Star Schema of a Data Warehouse. In a Star Schema,    one Fact Table is surrounded by multiple Dimension Tables. Each Dimension corresponds    to a single Dimension Table.</p> </li> <li> <p>Junk Dimension \\    A Junk Dimension is a type of Dimension that is used to combine 2 or more related    low cardinality Facts into one Dimension. They are also used to reduce    the Dimensions of Dimension Tables and the columns from Fact Tables.</p> </li> <li> <p>Degenerate Dimension \\    A Degenerate Dimension is also known as a Fact Dimension. They are standard    Dimensions that are built from the attribute columns of Fact Tables.    Sometimes data are stored in Fact Tables to avoid duplication.</p> </li> <li> <p>Swappable Dimension \\    A Swappable Dimension is a type of Dimension that has multiple similar versions    of itself which can get swapped at query time. The structure of this Dimension    is also different, and it has fewer data when compared to the original Dimension.    The input and output are also different for this Dimension.</p> </li> <li> <p>Step Dimension \\    This is a type of Dimension that explains where a particular step fits into the process.    Each step is assigned a step number and how many steps are required by that    step to complete the process.</p> </li> </ol> <p>To explore about the types of Dimensions in detail, click this link.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#types-of-dimension-tables","title":"Types of Dimension Tables","text":""},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#steps-to-carry-out-dimensional-data-modelling","title":"Steps to Carry Out Dimensional Data Modelling","text":"<p>Dimensional Data Modelling requires certain analysis on the data to understand data behaviour and domain. The main goal a Dimensional Data Model tries to address is that it tries to describe the Why, How, When, What, Who, Where of the business process in detail.</p> <pre><code>Select the business process (WHY)\n|\n|---&gt; Declare the Grain (HOW MUCH)\n      |\n      |---&gt; Identify the Dimension (3WS)\n            |\n            |---&gt; Identity (WHAT)\n                  |\n                  |---&gt; Build the Schema\n</code></pre> <p>The major steps to start Dimensional Data Modelling are:</p> <ol> <li>Identify the Business Process \\    A Business Process is a very important aspect when dealing with Dimensional Data Modelling.    The business process helps to identify what sort of Dimension and Facts are needed    and maintain the quality of data.</li> </ol> <p>To describe business processes, you can use    Business Process Modelling Notation (BPMN) or    Unified Modelling Language (UML).</p> <ol> <li>Identify Grain \\    Identification of Grain is the process of identifying how much normalisation    (the lowest level of information) can be achieved within the data for any    table in your data warehouse.</li> </ol> <p>It is the stage to decide the incoming frequency of data (i.e.daily, weekly, monthly, yearly),    how much data we want to store in the database (one day, one month, one year, ten years),    and how much the storage will cost.</p> <p>During this stage, you answer questions like</p> <ul> <li> <p>Do we need to store all the available products or just a few types of products? \\      This decision is based on the business processes selected for Data Warehouse</p> </li> <li> <p>Do we store the product sale information on a yearly, monthly, weekly, daily or hourly basis? \\      This decision depends on the nature of reports requested by executives</p> </li> <li> <p>How do the above two choices affect the database size?</p> </li> </ul> <p>Note: \\ Grain is the level of detail or the depth of the information that\u2019s stored in the data warehouse and answer this type of questions:</p> <ul> <li>Do we store all products or specific categories?</li> <li>We will use data from week or month or year?</li> <li>We will hold sales per day or per product or per store?</li> </ul> <p>Only facts consistent with the declared grain are allowed.</p> <ol> <li> <p>Identify the Dimensions \\    Dimensions are the key components in the Dimensional Data Modelling process.    It contains detailed information about the objects like date, store, name,    address, contacts, etc. For example, in an E-Commerce use case,    a Dimension can be:</p> </li> <li> <p>Product</p> </li> <li>Order Details</li> <li>Order Items</li> <li>Departments</li> <li>Customers (etc).</li> </ol> <p>The Dimensional Model for a customer conducting an E-Commerce transaction is shown below:</p> <pre><code>Customer Dimension  -----&gt; table name\n---\nCustomer ID         -----&gt; primary key\nCustomer Name\nGender\nAge\nAddress\nCity                --|\nState                 |--&gt; hierarchy level for location\nCountry             --|\nAnnual income\nPhone number\nPurchased day       --|\nPurchased month       |--&gt; hierarchy level for date\nPurchased quarter     |\nPurchased year      --|\n</code></pre> <ol> <li>Identify the Facts \\    Once the Dimensions are created, the measures/transactions are supposed to be linked    with the associated Dimensions. The Fact Tables hold measures and are linked to Dimensions    via foreign keys. Usually, Facts contain fewer columns and huge rows.</li> </ol> <p>For example, in an E-Commerce use case, one of the Fact Tables can be of orders,    which holds the products\u2019 daily ordered quantity. Facts may contain more than    one foreign key to build relationships with different Dimensions.</p> <p>This step is co-associated with the business users of the system because this    is where they get access to data stored in the data warehouse.    Most of the fact table rows are numerical values like price or cost per unit, etc.</p> <ol> <li>Build the Schema \\    The next step is to tie Dimensions and Facts into the Schema. Schemas are the table structure,    and they align the tables within the database.</li> </ol> <p>There are 2 popular types of Schemas:</p> <ul> <li> <p>Star Schema: \\       The Star Schema is the Schema with the simplest structure and easy to design.      In a Star Schema, the Fact Table surrounds a series of Dimensions Tables.      Each Dimension represents one Dimension Table. These Dimension Tables are not      fully normalised (The fact tables in a star schema which is third normal form      whereas dimensional tables are de-normalized).</p> <p>In this Schema, the Dimension Tables will contain a set of  attributes that describes the Dimension. They also contain foreign keys that  are joined with the Fact Table to obtain results.</p> <pre><code>erDiagram\n   DIM_DATE ||--o{ FCT_SALES : is\n   DIM_DATE {\n      string timestamp_id PK\n      string date\n      string month\n      string year\n   }\n   DIM_CUSTOMER ||--o{ FCT_SALES : is\n   DIM_CUSTOMER {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }\n   FCT_SALES {\n      string sales_key PK\n      string date_key FK\n      string customer_key FK\n      string store_key FK\n      numeric sales_amount\n   }\n   DIM_STORE ||--o{ FCT_SALES : is\n   DIM_STORE {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }\n   DIM_MOVIE ||--o{ FCT_SALES : is\n   DIM_MOVIE {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }</code></pre> </li> <li> <p>Snowflake Schema: \\       A Snowflake Schema is the extension of a Star Schema, and includes more Dimensions.      Unlike a Star Schema, the Dimensions are fully normalised and are split down      into further tables.      This Schema uses less disk space because they are already normalised.      It is easy to add Dimensions to this Schema and the data redundancy      is also less because of the intricate Schema design.</p> <pre><code>erDiagram\n   DIM_DATE ||--o{ FCT_SALES : is\n   DIM_DATE {\n      string timestamp_id PK\n      string date\n      string month\n      string year\n   }\n   DIM_CUSTOMER ||--o{ FCT_SALES : is\n   DIM_CUSTOMER {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }\n   FCT_SALES {\n      string sales_key PK\n      string date_key FK\n      string customer_key FK\n      string store_key FK\n      numeric sales_amount\n   }\n   DIM_STORE ||--o{ FCT_SALES : is\n   DIM_STORE {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }\n   DIM_MOVIE ||--o{ FCT_SALES : is\n   DIM_MOVIE {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }\n   DIM_COUNTRY ||--o{ DIM_STORE : in</code></pre> </li> </ul> Star Schema Snowflake Schema Hierarchies for the dimensions are stored in the dimensional table. Hierarchies are divided into separate tables. It contains a fact table surrounded by dimension tables. One fact table surrounded by dimension table which are in turn surrounded by dimension table In a star schema, only single join creates the relationship between the fact table and any dimension tables. A snowflake schema requires many joins to fetch the data. Simple DB Design. Very Complex DB Design. Denormalized Data structure and query also run faster. Normalized Data Structure. High level of Data redundancy Very low-level data redundancy Single Dimension table contains aggregated data. Data Split into different Dimension Tables. Cube processing is faster. Cube processing might be slow because of the complex join. Offers higher performing queries using Star Join Query Optimization. Tables may be connected with multiple dimensions. The Snowflake schema is represented by centralized fact table which unlikely connected with multiple dimensions. <p>More Detail: Star and Snowflake Schema in Data Warehouse with Model Examples</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#types-of-fact-tables","title":"Types of Fact Tables","text":"<p>There are three main types of fact tables:</p> <ol> <li>Transaction \\    The transaction fact table is the most basic and the simplest type, In the Transaction    fact table, every row corresponding to a measurement event at a point in space    and time which means a grain set at a single transaction. The row is added only    if there\u2019s a transaction is happened by a customer or for a product.</li> </ol> <p>Note: Row in Transaction fact table represents transaction, and it dimensions.</p> <p>For example, \\    If we have a retail store, on Sunday we sold 40 items, and on Monday we sold 15 items,    So on Sunday there are 40 transaction rows had been added to our fact table and on Monday    there are 15 transaction rows had been added to our table, There are no aggregate values,    we are storing the transaction's data.</p> <p>Because of low granularity, we can monitor detailed business processes.    But because we have rows for each transaction that happened, It causes performance    issues due to the large size of data.</p> <ol> <li>Periodic Snapshot \\    In the periodic table, We have lower granularity which means that the row in    the periodic table is a summarization of data over a period of time (day, week, month, etc).    Our grain here is periodic data summarization, not single transactions.    It helps to review the aggregate performance of the business process at intervals    of time.</li> </ol> <p>Then those unavailable measurements can be kept empty (Null) or can be filled    up with the last available measurements.</p> <p>For example, \\    If we want to know the quantity that been sold from specific product through    the last week. Our grain is a week.</p> <p>Because of summarized data, our performance now is better than the transaction    fact table. But now we have higher grain, So we lost the detailed business    processes that we had in the transaction fact table.</p> <p>Note: Rows in Periodic table represent performance of an activity at the end of specific period.</p> <ol> <li>Accumulated Snapshot \\    In Accumulating snapshot fact table, Row represents an Entire process,    which means that our row corresponding to measurements that occurred at defined    steps between the beginning and end of the process. we use it when users need    to perform pipeline and workflow analysis like Order fulfillment.</li> </ol> <p>Order fulfillment covers the complete process from when a sale process takes    place all the way through delivery to the customer. So here we have multiple    activities in the process, First when we receive an order from a customer,    then send the order to the inventory to organize it, then move the order to    the shipment activity, and finally, the customer receives his order.</p> <p>Note:</p> <ul> <li>Row in Accumulating snapshot represents an Entire process</li> <li>There is a date foreign key in the fact table for each critical activity in the process.</li> </ul> <p>Accumulating Snapshot fact table helps us in complex analysis, workflow, and    pipeline process, on another hand It needs high ETL process complexity.</p> Transaction Periodic Accumulating Row Transaction and It dimensions. summarized data over a period of time. Entire process activities. Granularity Lowest granularity 1 row / Transaction Higher than transaction.1 row / period Highest granularity 1 Row / Entire process Table Size Largest Smaller than Transaction Smallest Example Sales amount of products on a daily basis. Total sales amount of product through May. Order fulfillment <p>Fact-less Fact Table: \\ Fact-less facts are fact tables that haven\u2019t any measures, It only has a foreign key for each dimension. We can say that Fact-less fact is only an intersection of Dimensions Or A bridge between dimension keys.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#rules-for-dimensional-modelling","title":"Rules for Dimensional Modelling","text":"<p>Following are the rules and principles of Dimensional Modeling:</p> <ul> <li> <p>Load atomic data into dimensional structures.</p> </li> <li> <p>Build dimensional models around business processes.</p> </li> <li> <p>Need to ensure that every fact table has an associated date dimension table.</p> </li> <li> <p>Ensure that all facts in a single fact table are at the same grain or level of detail.</p> </li> <li> <p>It\u2019s essential to store report labels and filter domain values in dimension tables</p> </li> <li> <p>Need to ensure that dimension tables use a surrogate key</p> </li> <li> <p>Continuously balance requirements and realities to deliver business solution to   support their decision-making</p> </li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#benefits-of-dimensional-data-modelling","title":"Benefits of Dimensional Data Modelling","text":"<p>As now, you understand the process of Dimensional Data Modelling, you can imagine why it is so important and how many benefits DDM provides for the company. Some of those benefits are given below:</p> <ul> <li> <p>The Dimension Table stores the history information and a standard Dimension Table   holds good quality data and allows easy access across the business.</p> </li> <li> <p>You can introduce new Dimensions without affecting other Dimensions and Facts in the Schema.</p> </li> <li> <p>Dimension and Fact Tables are easier to read and understand as compared to a normal table.</p> </li> <li> <p>Dimensional Models are built based on business terms, and hence it is quite   understandable by the business.</p> </li> <li> <p>Dimensional Data Modelling in a Data Warehouse creates a Schema which is optimised   for high performance. It means fewer joins between tables, and it also helps with   minimised data redundancy.</p> </li> <li> <p>The Dimensional Data Model also helps to boost query performance. It is more denormalized;   therefore, it is optimized for querying.</p> </li> <li> <p>Dimensional Data Models can comfortably accommodate the change. Dimension Tables   can have more columns added to them without affecting existing Business Intelligence   applications using these tables.</p> </li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#limitations-of-dimensional-data-modelling","title":"Limitations of Dimensional Data Modelling","text":"<p>Although Dimensional Data Modelling is very crucial to any organisation, it has a few limitations that companies need to take care of when incorporating the concept into their applications. Some of those limitations are given below:</p> <ul> <li> <p>Designing and creating Schemas require domain knowledge about the data.</p> </li> <li> <p>To maintain the integrity of Facts and Dimensions, loading the Data Warehouses   with a record from various operational systems is complicated.</p> </li> <li> <p>It is severe to modify the Data Warehouse operations if the organisation adopts   the Dimensional technique and changes the method in which they do business.</p> </li> </ul> <p>Despite these limitations, the DDM technique has proved to be one of the simplest and most efficient techniques to handle data in Data Warehouses to date.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#advantages","title":"Advantages","text":"<p>Kimball's architecture has several advantages.</p> <ul> <li>Simplicity and speed: Kimball's architecture is significantly easier and faster to construct and establish.</li> <li>Understandable: Non-technical and technical staff both may understand the dimensional data model.</li> <li>Relevancy: Kimball's bottom-up methodology, unlike Inmon's, makes all data linkages relevant to the business needs.</li> <li>Engineering team needs: In comparison to Inmon's technique, Kimball requires fewer engineers with less specific technical abilities to set up and operate the data warehouse.</li> </ul> <p>Good fit for:</p> <ul> <li>Medium-to-large number of data sources</li> <li>Centralized data teams</li> <li>End-use case for data is primarily around business intelligence and providing insights</li> <li>Teams that want to create an easily navigable and predictable data warehouse design</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#disadvantages","title":"Disadvantages","text":"<p>Kimball's architecture has some drawbacks.</p> <ul> <li>Data redundancy: There is more data redundancy and hence a higher likelihood of errors since data is fed into a dimensional model.</li> <li>No single source of truth: Data marts are used to design and organise data in the data warehouse. When combined with data redundancy, Kimball's architecture prevents the company from having a single source of truth.</li> <li>Less adaptable: Kimball's architecture is less flexible and adaptable to modifications when data demands change, business requirements vary, and incoming data sources alter their payloads.</li> <li>Incomplete: The strategy taken begins (and concludes) with important business processes. As a result, it does not provide a complete 360-degree picture of business data. Instead, it helps report on particular subject areas in the corporate world.</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#what-is-multi-dimensional-data-model-in-data-warehouse","title":"What is Multi-Dimensional Data Model in Data Warehouse?","text":"<p>Multidimensional data model in data warehouse is a model which represents data in the form of data cubes. It allows to model and view the data in multiple dimensions, and it is defined by dimensions and facts.</p> <p>Multidimensional data model is generally categorized around a central theme and represented by a fact table.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#conclusion","title":"Conclusion","text":"<p>This article gave an in-depth knowledge about Dimensional Data Modelling, its types, features, components and also the steps required for any company to set up a DDM approach. It also gave a brief understanding of the benefits and limitations of the DDM approach. Overall, adopting any new approach can be a tedious task for any company, but, by having systematic techniques put in place the company can monitor those parameters carefully and also optimise the performance.</p> <ul> <li> <p>A dimensional model is a data structure technique optimized for   Data warehousing tools.</p> </li> <li> <p>A fact table is a primary table in a dimensional model. \\    There are 4 types of facts/measures:</p> </li> <li> <p>Additive: \\      Business measures that can be aggregated across all dimensions</p> </li> <li> <p>Non-additive: \\      Business measures that can be aggregated across some dimensions and not across      others (usually date and time dimensions).\\      For example, <code>Items Inventory</code> (can be summed through product, But it can\u2019t be summed through date)</p> </li> <li> <p>Semi-additive: \\      Business measures that cannot be aggregated across any dimension.\\      For example, <code>Sales Tax</code>, or <code>Unit Price</code></p> </li> <li> <p>Fact-less</p> </li> </ul> <p>The most common form of dimensional modeling is the star schema. A star schema is a multidimensional data model used to organize data so that it is easy to understand and analyze, and very easy and intuitive to run reports on. Kimball-style star schemas or dimensional models are pretty much the gold standard for the presentation layer in data warehouses and data marts, and even semantic and reporting layers. The star schema design is optimized for querying large data sets.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-kimball-approach/#reference","title":"Reference","text":"<ul> <li>https://hevodata.com/learn/dimensional-data-modelling</li> <li>https://www.guru99.com/dimensional-model-data-warehouse.html</li> <li>https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/</li> <li>https://datavalley.technology/dimensional-modeling-part-1-introduction-and-fact-types/</li> <li>https://dwgeek.com/types-of-dimension-tables-data-warehouse.html/</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/","title":"One Big Table (OBT) Approach","text":"<p>In the dynamic landscape of data warehousing, the methodologies employed for structuring data play a pivotal role in unlocking timely insights for informed decision-making. Since the foundational theories of the 1990s and 2000s by Inmon (Inmon W.H., Building the Data Warehouse, 1990), Kimball (Kimball R, The Data Warehouse Toolkit, 1996), and later Linstedt (Linstedt D, Data Vault Series 1 \u2014 Data Vault Overview, 2002), various data modeling techniques for traditional data warehousing have evolved and debated. This blog post delves into two data modeling techniques: Dimensional Modeling and One Big Table (OBT).</p> <p>Through this blogpost, we will explore the strengths and challenges of these two techniques and discuss the best practices for implementing them on Databricks.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/#dimensional-modeling-essential-then-optional-now-understanding-the-shift","title":"Dimensional Modeling: essential then, optional now \u2014 understanding the shift","text":"<p>Designing a dimensional data model requires a bottom-up understanding of the business requirements and high-level profiling of data sources. The most common implementation of Dimensional Modeling is Star Schema, which has become widely adopted as the presentation layer in most data warehouses in the past decades. This method denormalizes the data into measurable business process events called Facts and the contextual details surrounding the business process events called Dimensions.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/#the-rise-of-dimensional-modeling-why-it-became-the-gold-standard","title":"The rise of Dimensional Modeling: why it became the gold standard","text":"<p>Dimensional Modeling was introduced to optimize the data model for analytics in the logical layer of the Relational Databases Management Systems (RDBMS) without redesigning the physical layer. The logical and physical layers of the RDBMS were purposefully designed for Online Transaction Processing (OLTP), facilitating efficient row-oriented data entry, and used primarily for transactional purposes, maintaining ACID (Atomicity, Consistency, Isolation, Durability) properties in normalized data. On the other hand, Dimensional Modeling optimizes the logical layer for Online analytical processing (OLAP) to process and aggregate historical data with better performance. Dimensional Modeling in the logical layer offered various benefits, such as:</p> <ul> <li>Simplified Data Usability: Dimensional Modeling simplifies data understanding by structuring it into fact and dimensional tables, making it intuitive for end-users to map real-world processes to the data model. Dimensional Modeling supports efficient data aggregation, which can serve the Business Intelligence (BI) tools as the semantic layer.</li> <li>Query Performance: The primary advantage of Dimensional Modeling lies in its ability to optimize for query performance without compromising the depth of historical analysis. The Star Schema, a common implementation of Dimensional Modeling, optimizes analytics by denormalizing data into business-grained facts and dimensions, improving query performance and data aggregation.</li> <li>Scalability and Consistency: Dimensional Models excel at scalability, accommodating growing data volumes and changing business requirements. The Star Schema can adapt changes in the dimensions and facts, facilitate Slowly Changing Dimensions and integration of incremental data. There are further approaches to reducing the redundancy in the data, such as implementing a Snowflake Schema or using Surrogate Keys in the dimension tables.</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/#dimensional-modeling-under-scrutiny-navigating-new-technological-landscapes","title":"Dimensional Modeling under scrutiny: navigating new technological landscapes","text":"<p>Dimensional Modeling achieved a level of optimization on the logical layer effectively balancing redundancy and query performance, which was crucial when storage and computing resources were costly and limited row-oriented databases were insufficient for analytical processing. The advancement in Data technologies has brought to light certain considerations associated with traditional Dimensional Modeling.</p> <ul> <li>Operational and Design Limitations: Dimensional Modeling requires a substantial initial investment in schema design, as well as ongoing maintenance of data pipelines, typically managed through ETL tools. Beyond operational overhead, Dimensional Modeling faces inherent design limitations, such as the complexity of managing Slowly Changing Dimensions and Fact-to-Fact joins. In the past, these challenges were considered worthwhile trade-offs for the purpose of improved query performance. However, with the advancements in modern data technologies, these issues can often be circumvented, reducing the necessity of complex schema designs and operational overhead.</li> <li>Technological Evolution in Data Storage and Processing: Modern data warehousing allows for flexibility and scalability by decoupling storage and processing. These technologies leverage Massive Parallel Processing (MPP) and physical columnar storage, which inherently optimize historical data aggregation and analytics by default. Also, the drawbacks of denormalization are mitigated since the storage cost has significantly reduced over time. Advanced optimization technologies, e.g., data compression on the storage layer and clustering, outweigh the complexity of keeping the normalized data.</li> </ul>"},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/#dimensional-modeling-best-practices-on-databricks","title":"Dimensional Modeling best practices on Databricks","text":"<p>Considering the advanced Data Warehousing capabilities of Databricks, adhering strictly to Dimensional Modeling e.g. Star Schema is not a necessity anymore; however possible and very well supported. We\u2019ll review various Databricks technologies here that make it possible to implement and optimize the Dimensional Modeling technique.</p> <ul> <li>ACID properties: Databricks Delta Lake supports ACID transactions on Delta Tables, simplifying the maintenance and quality of the Dimensional Models.</li> <li>Data layers: The Star Schema can be deployed into a Gold Layer of the medallion architecture in Databricks to power analytics and fast decision-making.</li> <li>ELT pipelines: The pipeline to transform the transactional data into dimensional data models is supported by Databricks SQL and Delta Live Tables (DLT).</li> <li>Relational constraints: Unlike usual data lakes, Databricks supports schema enhancement such as relational constraints e.g. Primary Keys, Foreign Keys, and Identity Columns in Databricks SQL as Surrogate Keys and enforced CHECK constraints for data quality. The physical and virtual constraints can exist as meta-objects in the Unity Catalog.</li> <li>Unified governance: All data models, Dimension tables, Fact tables, and their relations are registered centrally into the Unity Catalog. With Unity Catalog as the unified governance layer, Dimensions and Facts can be discovered and shared across organizations without the need to duplicate it.</li> <li>Optimization: Databricks supports Liquid Clustering, which incrementally optimizes the data layout without rewriting the data, which can be applied to fact tables as well as dimension tables.</li> </ul> <p>In this blogpost, you can follow a simple 5-step guideline to implement Dimensional Modeling on Databricks.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/#embracing-simplicity-the-one-big-table-obt-approach-as-a-modern-alternative","title":"Embracing simplicity: The One Big Table (OBT) approach as a modern alternative","text":"<p>One Big Table emerges as another alternative concept to Dimensional Modeling. This method, characterized by its simplicity, involves storing data in a single, expansive table, offering a much-simplified data model by significantly reducing the number of tables that need to be governed and updated.</p> <p>Reconsidering OBT: unveiling the benefits</p> <ul> <li>Simplicity: In contrast to designing a Dimensional Model, setting up One Big Table is quick and easy. It facilitates flexibility for swift integration and modification of data while simplifying workflows with data at high velocities. As complex joins are usually not required, finding, querying, and exploring a single wide big table is also straightforward, increasing the discoverability and readability of data assets.</li> <li>Data Consistency &amp; Governance: Since all data is stored and updated in One Big Table, the risks of data duplication and data inconsistencies are reduced and can be easily mitigated and monitored with quality constraints applied to a single table. By reducing the amount of tables to govern, One Big Table can also significantly decrease administrative workloads.</li> <li>Performance: Although the OBT approach originally required scanning and filtering of an entire large table, it avoids complex joins and transformations, reducing the amount of data shuffling, and often delivering faster query results compared to Dimensional Modeling. This benefit is additionally capitalized by a much simpler reduction of data skews.</li> </ul> <p>Addressing the challenges of OBT</p> <ul> <li>Performance: Despite previously highlighted performance benefits of the One Big Table approach by redundancy of joins and shuffles, as tables grow in size, they can introduce performance bottlenecks for queries where the entire table needs to be scanned. This has historically been the most prominent challenge of the One Big Table concept, which can be particularly challenging for serving the BI tools. With columnar storage in Delta, OBT can lead to high data compression for the table, making storage of repeated values less of a concern compared to classical row-based databases. However, since it is indeed One Big Table, if you need to prune files by more than one to two dimensions for ALL attributes you need in the table, OBT can quickly become inefficient. If you have five relational tables, each with one or two clustered columns, you can efficiently prune files using up to ten dimensions. If you have an OBT model, you can only use two to three dimensions before clustering becomes useless.</li> <li>Data exploration: One Big Table lacks the structure inherent to Dimensional Modeling by mapping specific business requirements to data architecture. Although this can also pose a benefit, as tables grow, particularly in width, navigating and exploring data in One Big Table can become complicated, particularly using BI tools. Moreover, this approach can lead to duplicated analysis and results in writing numerous window functions instead of joins.</li> <li>Data privacy: Storing all data in One Big Table concentrates a lot of data, including sensitive information, in one place. As the number of users querying data from a single big table is larger than in Dimensional Models, the concept requires fine-grained access controls, data masking, and anonymization, which can become complex to manage for very wide tables.</li> <li>Data Quality: The level of data quality and observability can seriously influence the decision to utilize OBT. If you need to implement data quality checks beyond your filtering/clustered columns, and if your data quality rules / checks need to be measured across multiple rows, this can get expensive and complex fast. Overall for OBT, keep data quality simple: check quality statelessly (data quality checks across rows at a point in time is stateful) or keep the data quality rules to a window that utilizes the clustering columns. Try to measure / enforce data quality within the write-side of the pipeline, especially since OBT models tend to be much larger.</li> </ul> <p>Overall, OneBigTable can be incredibly useful for certain use cases, but should be used with caution. The OBT model works well for use cases where you only need to filter the table on 1\u20133 dimensions, and the rest of your analytics / apps are built on those filters. Real world examples include IoT data, logging systems, or single use case / data view applications. In IoT data, you usually only ever need to filter the data by an event timestamp, sensor type, and then the rest of the downstream analytics are built on that. This is a perfect example of a string use case for OBT because you can have 100s/1000s of attributes that a single sensor row carries, but you only ever need to filter by the initial timestamp/sensor type columns. In the Sensor IoT example, modeling the data for 100s+ number of data attributes is not worth the lift, and the OBT model is built perfectly for this.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/#obt-best-practices-on-databricks","title":"OBT best practices on Databricks","text":"<p>Let\u2019s create OBT from a dimensional Databricks sample dataset using a Databricks SQL Serverless (XS) warehouse and explore the benefits and challenges of OBT and how we can address them in Databricks.</p> <pre><code>CREATE TABLE tpch_obt AS\nSELECT\n  *\nFROM\n  samples.tpch.customer\n  JOIN saples.tpch.orders ON c_custkey == o_custkey\n</code></pre> <p>The resulting Delta table comprises 730 MB stored across 7 files.</p> <p>Let\u2019s analyze the average order price for the AUTOMOBILE market segment:</p> <pre><code>SELECT\n MEAN(o_totalprice)\nFROM\n tpch_obt\nWHERE\n c_mktsegment == \"AUTOMOBILE\"\n</code></pre> <p>The wall-clock duration time on a Databricks SQL Serverless XS Warehouse was 3.5 seconds.</p> <p>Looking at the query profile in the Query History provides further insights:</p> <p></p> <p>Note that 7 files were scanned and the total task time was 9.7 seconds.</p> <p>Querying the Dimensional Model on the same Databricks SQL Serverless XS warehouse with a slightly faster wall-clock duration of 2.6 seconds looks like the following (if you want to reproduce the performance test, make sure to restart your SQL Serverless Warehouse after the OBT creation to avoid CACHING):</p> <pre><code>SELECT\n MEAN(a.o_totalprice)\nFROM\n samples.tpch.orders a JOIN samples.tpch.customer b ON a.o_custkey == b.c_custkey\nWHERE\n b.c_mktsegment == \"AUTOMOBILE\"\n</code></pre> <p>Looking at the query history reveals further details:</p> <p></p> <p>Despite the latter query requiring a join, it\u2019s faster as fewer files have to be scanned.</p> <p>When executing both queries again, one can capitalize on Databricks SQL automatic caching capabilities (see this blog post), effectively reducing wall-clock duration to &lt;500ms for both queries.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/#optimize-query-performance-using-liquid-clustering","title":"Optimize query performance using Liquid Clustering","text":"<p>To optimize the query performance for the OBT beyond Caching, Databricks Liquid Clustering offers an easy and automated approach to simplify data layout, resulting in largely optimized query performance.</p> <pre><code>ALTER TABLE tpch_obt CLUSTER BY (c_mktsegment);\nOPTIMIZE tpch_obt;\n</code></pre> <p>Subsequently querying the table (same statement as before) results in &gt;20x task speed up and &gt;3x wall-clock duration, bringing the latter down to 1.13 seconds.</p> <p>The optimized data layout by liquid clustering effectively reduced the number of files to read from 7 to 2.</p> <p></p> <p>As datasets increase in size, this effect becomes even more prominent in the absolute speedup time.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/#data-privacy-with-row-filters-and-column-masks","title":"Data Privacy with Row Filters and Column Masks","text":""},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/#row-filters","title":"Row filters","text":"<p>Row filters allow you to apply a filter to a table so that subsequent queries only return rows for which the filter predicate evaluates to true. This facilitates overcoming the privacy challenges of OBT. A row filter is implemented as an SQL user-defined function (UDF).</p> <p>Create the row filter function:</p> <pre><code>CREATE FUNCTION mktsegment(c_mktsegment STRING)\nRETURN IF(IS_ACCOUNT_GROUP_MEMBER('pl_obt'), true, c_mktsegment = 'HOUSEHOLD');\n</code></pre> <p>Apply the row filter to a table:</p> <pre><code>ALTER TABLE tpch_obt SET ROW FILTER mktsegment ON (c_mktsegment);\n</code></pre> <p>Subsequent queries of users who are members of the group pl_obt will return only rows where the <code>c_mktsegment</code> column equals <code>'HOUSEHOLD'</code>.</p>"},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/#column-masks","title":"Column masks","text":"<p>Column masks let you apply a masking function to a table column. The masking function gets evaluated at query runtime, substituting each reference of the target column with the results of the masking function. In the following example, we create a user-defined function that masks the total_price column so that only users who are members of the pl_obt group can view values in that column.</p> <p>Create the column masking function:</p> <pre><code>CREATE FUNCTION price_mask(o_totalprice DECIMAL)\n    RETURN CASE WHEN is_member('pl_obt')\n    THEN o_totalprice ELSE '***-**-****' END;\n</code></pre> <p>Apply the column masking function to a table:</p> <pre><code>ALTER TABLE tpch_obt ALTER COLUMN o_totalprice SET MASK price_mask;\n</code></pre> <p>Subsequent queries of users who are not members of the <code>pl_obt</code> group will result in masked values:</p> <pre><code>SELECT o_totalprice FROM tpch_obt LIMIT 10;\n</code></pre>"},{"location":"abstract/data_architecture/data_modeling/dwh-obt-approach/#references","title":"References","text":"<ul> <li>https://medium.com/@hubert.dulay/one-big-table-obt-vs-star-schema-a9f72530d4a3</li> <li>https://medium.com/dbsql-sme-engineering/one-big-table-vs-dimensional-modeling-on-databricks-sql-755fc3ef5dfd</li> </ul>"},{"location":"abstract/data_governance/","title":"Data Governance","text":"<p>Data Governance is a comprehensive approach that comprises the principles, practices and tools to manage an organization\u2019s data assets throughout their lifecycle. By aligning data-related requirements with business strategy, data governance provides superior data management, quality, visibility, security and compliance capabilities across the organization. Implementing an effective data governance strategy allows companies to make data easily available for data-driven decision-making while safeguarding their data from unauthorized access, and ensuring compliance with regulatory requirements.</p>"},{"location":"abstract/data_governance/#key-elements-of-data-governance","title":"Key Elements of Data Governance","text":"<ul> <li>Data cataloging</li> </ul> <p>Effective data governance requires knowledge of the data that exists within an   organization. This is where a data catalog comes in, as it provides a centralized   metadata repository for an organization\u2019s data assets. A data catalog allows   stakeholders to quickly discover, understand and access the data they need,   improving data-related activities such as discovery, governance and analytics.   It acts as a searchable index of all the data available, including information   about its format, structure, location and usage, providing semantic value to an   otherwise unidentifiable sea of information. Incorporating a data catalog into   a governance program can help organizations improve their data management, enhance   collaboration, reduce redundancy and ensure proper access controls and audit   information retrieval.</p>"},{"location":"abstract/data_governance/#why-is-data-governance-so-important","title":"Why is Data Governance so important?","text":"<ul> <li> <p>It can be a colossal waste of time when not done correctly\u2014 Think about organizing   a library with no books.</p> </li> <li> <p>It is foundational to data.</p> </li> <li> <p>It can save significant time and money when done correctly.</p> </li> <li> <p>It can help with the rapid adoption of new technologies or strategies.</p> </li> <li> <p>Mismanagement of data is a liability for an organization \u2014 Data that gets into   the wrong hands represents real business risk.</p> </li> </ul>"},{"location":"abstract/data_governance/#references","title":"References","text":"<ul> <li>Databricks: Data Governance</li> <li>Digging into Data Governance and Data Modeling</li> </ul>"},{"location":"abstract/data_governance/dgn-data-quality-framework-for-scale/","title":"Data Quality Framework for Scale","text":""},{"location":"abstract/data_governance/dgn-data-quality-framework-for-scale/#references","title":"References","text":"<ul> <li>GetDBT: Data Quality Framework</li> </ul>"},{"location":"abstract/data_governance/dgn-implement-process/","title":"How to implement a data governance process effectively","text":""},{"location":"abstract/data_governance/dgn-implement-process/#references","title":"References","text":"<ul> <li>https://www.thoughtspot.com/data-trends/data-governance/how-to-implement-data-governance</li> </ul>"},{"location":"abstract/data_governance/dgn-with-modern-data-team/","title":"Data Governance: Rethinking with Modern Data Teams","text":"<p>https://prukalpa.medium.com/special-edition-rethinking-data-governance-with-modern-data-team-43f2f05fe5f9</p>"},{"location":"abstract/data_lakehouse/","title":"Data Lakehouse","text":"<ul> <li>https://behindthescenes.nocnoc.com/%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%AB%E0%B8%A5%E0%B8%B1%E0%B8%87%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-data-lakehouse-%E0%B8%9A%E0%B8%99-aws-%E0%B8%97%E0%B8%B5%E0%B9%88-nocnoc-ceb2c7334443</li> <li> <p>https://medium.com/@midogax272/spark-submit-with-pyspark-and-aws-emr-serverless-6-9-0-aa451c3961e5</p> </li> <li> <p>https://medium.com/@karim.faiz/open-data-lakehouse-revolutionizing-data-management-2cecf94f95e3</p> </li> <li>Data Lake 101: Architecture</li> </ul>"},{"location":"abstract/data_lakehouse/data-lakehouse-policy-based-access-control/","title":"Data Lakehouse: Policy-Based Access Control (PBAC)","text":"<p>https://python.plainenglish.io/policy-based-access-control-pbac-what-it-is-and-why-you-need-it-in-your-modern-data-lakehouse-672e869c2082</p>"},{"location":"abstract/data_management/","title":"Data Management","text":"<p>Data Management refers to the practices, architectural techniques, and tools that manage, store, and analyze data throughout its lifecycle. Effective data management ensures that data is accurate, available, and accessible when needed while maintaining security and compliance with relevant regulations.</p> <p>For trend of Data Management, We should follow the sharing knowledge from the  Gartner Hype Cycle for Data Management. Data management strategy can change everytime because it depend on technology.</p> Gartner Hype Cycle for Data Management 2023"},{"location":"abstract/data_management/#getting-started","title":"Getting Started","text":""},{"location":"abstract/data_management/#data-governance","title":"Data Governance","text":"<p>Includes policies, procedures, and standards that ensure the appropriate use, management, and protection of data throughout its lifecycle. It also involves establishing roles and responsibilities for data management, as well as ensuring compliance with legal and regulatory requirements.</p> <ul> <li>Policies and Standards: Establishing clear policies for data usage, privacy, and security.</li> <li>Data Stewardship: Assigning responsibilities to ensure data quality and compliance.</li> </ul> <p>Read More about Data Governance</p>"},{"location":"abstract/data_management/#data-quality-and-consistency","title":"Data Quality and Consistency","text":"<p>Ensuring that data is accurate, complete, and consistent. It includes defining data quality metrics, establishing data quality rules, and implementing data profiling and cleansing tools and techniques.</p> <ul> <li>Data Cleansing: Regularly cleaning data to remove inaccuracies and inconsistencies.</li> <li>Data Validation: Implementing processes to ensure data accuracy and reliability.</li> </ul>"},{"location":"abstract/data_management/#data-integration-transformation","title":"Data Integration &amp; Transformation","text":"<p>Consolidating data from multiple sources into a single, unified view of the data. It includes selecting appropriate data integration tools, defining data mapping and transformation rules, and establishing data synchronization and replication protocols.</p> <ul> <li>ETL/ELT Processes</li> <li>Batch and Stream Processing</li> <li>Real-Time Data Integration</li> <li>Data Transformation Techniques</li> </ul>"},{"location":"abstract/data_management/#data-lifecycle-management","title":"Data Lifecycle Management","text":"<ul> <li>Archiving and Retention: Implementing policies for data archiving and retention   based on data usage and legal requirements.</li> <li>Disposal: Securely disposing of data that is no longer needed.</li> </ul>"},{"location":"abstract/data_management/#data-security-and-privacy","title":"Data Security and Privacy","text":"<p>This involves ensuring the confidentiality, integrity, and availability of data. It includes establishing data security policies and procedures, implementing access controls and encryption, and complying with legal and regulatory requirements.</p> <ul> <li>Advanced Encryption: Using cutting-edge encryption techniques to protect data.</li> <li>Regulatory Compliance: Ensuring adherence to global data protection regulations   like GDPR, CCPA, and others.</li> <li> <p>Zero Trust Architecture: Implementing security models that verify every access   request as if it originated from an open network.</p> </li> <li> <p>Data Encryption and Masking</p> </li> <li>Access Control and Authentication</li> <li>Compliance and Regulatory Requirements</li> <li>Privacy-Preserving Data Processing</li> </ul>"},{"location":"abstract/data_management/#data-democratization","title":"Data Democratization","text":"<ul> <li>Self-service Analytics: Providing tools and platforms that enable non-technical   users to access and analyze data.</li> <li>Data Literacy Programs: Promoting data literacy across the organization to empower   employees to make data-driven decisions.</li> </ul>"},{"location":"abstract/data_management/#conclusion","title":"Conclusion","text":"<p>Overall, a data management strategy is a comprehensive approach to managing data that includes a range of components designed to ensure the effective use, management, and protection of data throughout its lifecycle.</p>"},{"location":"abstract/data_management/data-integration/","title":"Data Integration &amp; Transformation","text":"<p>Data transformation is the process of taking raw data and making meaning out of it; it forms the foundation of all analytics work and represents how data practitioners create tangible value from their companies.</p> <p>ETL: Data is extracted from different sources, transformed to meet analysis needs, and then loaded into a data warehouse. This can be time-consuming and less adaptable to changes.</p> <p>ELT: Data is extracted and directly loaded into the data warehouse in its raw form. Transformation occurs afterward, leveraging the power of the modern cloud-based data warehouse\u2019s processing capabilities. This approach is more agile and scalable, accommodating the growing volume and variety of data.</p> <p>https://medium.com/towards-data-engineering/why-etl-becomes-elt-or-even-let-1ea7b21e2f28 Medium: Part2 - Tool Selection Strategy</p> <p>Unveiling Essential Framework Components</p>"},{"location":"abstract/data_management/data-integration/#references","title":"References","text":"<ul> <li>Data Ingestion: Architectural Patterns</li> </ul>"},{"location":"abstract/data_management/data-modeling/","title":"Data Model","text":"<p>Data Model is essentially a blueprint for a building designed by an architect. It is the technique of documenting complex software system designs in the form of an easily understandable graphic. To describe how the data will flow, the diagram will be made using text and symbols.</p> Overall Data Modeling <p>Note</p> <p>Data Modeling is the process of creating a visual representation of either a whole information system or parts of it to communicate connections between data points and structures.<sup>1</sup></p>"},{"location":"abstract/data_management/data-modeling/#types-of-data-models","title":"Types of Data Models","text":"Types of Data Models"},{"location":"abstract/data_management/data-modeling/#conceptual-data-models","title":"Conceptual Data Models","text":"<p> They are also referred to as domain models and offer a big-picture view of what the system will contain, how it will be organized, and which business rules are involved. Conceptual models are usually created as part of the process of gathering initial project requirements.</p> <p>Typically, they include entity classes (defining the types of things that are important for the business to represent in the data model), their characteristics and constraints, the relationships between them and relevant security and data integrity requirements. Any notation is typically simple.</p> <p></p>"},{"location":"abstract/data_management/data-modeling/#logical-data-models","title":"Logical Data Models","text":"<p> They are less abstract and provide greater detail about the concepts and relationships in the domain under consideration. One of several formal data modeling notation systems is followed. These indicate data attributes, such as data types and their corresponding lengths, and show the relationships among entities. Logical data models don't specify any technical system requirements.</p> <p>This stage is frequently omitted in agile or DevOps practices. Logical data models can be useful in highly procedural implementation environments, or for projects that are data-oriented by nature, such as data warehouse design or reporting system development.</p> <p></p>"},{"location":"abstract/data_management/data-modeling/#physical-data-models","title":"Physical Data Models","text":"<p> They provide a schema for how the data will be physically stored within a database. As such, they're the least abstract of all. They offer a finalized design that can be implemented as a relational database, including associative tables that illustrate the relationships among entities as well as the primary keys and foreign keys that will be used to maintain those relationships.</p> <p>Physical data models can include database management system (DBMS)-specific properties, including performance tuning.</p> <p></p>"},{"location":"abstract/data_management/data-modeling/#how-can-define-a-good-data-model","title":"How Can Define a Good Data Model?","text":"<p>In other words, how can we compare various Data Modeling options? What Factors should be taken into account?</p> <ol> <li> <p>Performance    This is a vast topic, and we are not discussing database vendors, data indexing,    or technical modifications to boost read and write speeds. I believe we can    ascribe performance advantages solely based on how we model the data.</p> </li> <li> <p>Productivity    On the developer side, we want a model that is simple to work with and reason about,    so we can \"create a lot of good code\" without wasting time (the concept of productivity).</p> </li> <li> <p>Clearness    The Data Model\u2019s ability to be comprehended by those who look at it. As you may    have heard, most developers read code rather than write it, therefore we must    clearly grasp what we are doing with our data.</p> </li> <li> <p>Flexibility    The Model\u2019s capacity to evolve without having a significant influence on our code.    Because the startup you work for is evolving, the systems and Data Models that    power it will need to evolve as well.</p> </li> <li> <p>Traceability    Finally, we want to have data that is useful to the system as well as data that    is valuable to our users. Knowing what happened in the past, what values the    entities had at some point in time, being able to travel back and forth in time,    and so on.</p> </li> </ol> <p>Example</p> <p>Standard Data Modeling (a.k.a. Domain Models)</p> <p>It\u2019s as simple as this: The domain of the problem you\u2019re solving dictates how you define your entities and their properties. As a result, each entity will be comparable to other entities in the same domain while being significantly different from entities in other domains. As an example, we may have a distinct sort of box for each type of item that we want to store.</p> <p>This type of modeling is obvious since it is specified in the same way that we think about the problem.</p> <p>So, let\u2019s run the first test on our five dimensions of analysis (points 1\u201310):</p> Performance: <code>6 points</code> <p>This type of model does not perform well, as we will demonstrate later.</p> Productivity: <code>3 points</code> <p>Each collection (or table) will require its own function to update the values in each field, correct? This is not helpful for developer productivity unless you create a middleware to communicate with the database in a \"parametric fashion,\" which is equally unnatural. We shall offer a better method of accomplishing this.</p> Clearness: <code>10 points</code> <p>Yes, the model is crystal clear, precisely as humans think.</p> Flexibility: <code>3 points</code> <p>However, things aren\u2019t going so well here. Each new field that is added necessitates a change to the model.</p> Traceability: <code>2 points</code> <p>This type of modeling updates fields in place, so if your address changes, you\u2019ll lose the previous one, right? The remedy is to have a distinct table (a log table) that records all changes, but it will be independent of the rest of the model and thus \"noisy.\"</p>"},{"location":"abstract/data_management/data-modeling/#read-mores","title":"Read Mores","text":"<ul> <li>Types of Data Model</li> </ul> <ol> <li> <p> What is data modeling? \u21a9</p> </li> </ol>"},{"location":"abstract/data_management/data-quality/","title":"Data Quality","text":"<p>Data quality measures how well a dataset complies with the criteria of accuracy, completeness, validity, consistency, uniqueness, timeliness, and fitness for purpose, and is fundamental for all data governance initiatives within an organization. Data quality standards ensure that companies make decisions based on data to achieve their business objectives.</p> <p>https://medium.com/@romina.elena.mendez/data-quality-f720fc56912a</p> <ul> <li>Layers of Data Quality</li> <li>Why Data Quality Is Harder than Code Quality</li> </ul>"},{"location":"abstract/data_management/data-quality/#references","title":"References","text":"<ul> <li>https://snowplow.io/blog/the-road-to-better-data-quality-your-questions-answered/</li> <li>https://www.talend.com/resources/data-quality-best-practices/</li> </ul>"},{"location":"abstract/data_management/data-warehouse/","title":"Data Warehouse","text":"<p>Data Warehouse (DW or DWH) is a centralized repository or digital storage system that integrates and stores data from various sources within an organization. It is designed to support Business Intelligence (BI) activities such as reporting, data analysis, and decision-making.</p> <p></p>"},{"location":"abstract/data_management/data-warehouse/#types-of-data-warehouse","title":"Types of Data Warehouse","text":"<p>There are three main types of data warehouses:</p> <ul> <li>Enterprise Data Warehouse (EDW)</li> <li>Operational Data Store (ODS)</li> <li>Data Mart (DM)</li> </ul> <p>Overall, the type of data warehouse an organization chooses depends on its specific needs and requirements. Some organizations may require a centralized, enterprise-wide data warehouse, while others may benefit from a more focused data mart or an operational data store that supports real-time decision-making.</p>"},{"location":"abstract/data_management/data-warehouse/#enterprise-data-warehouse","title":"Enterprise Data Warehouse","text":"<p>Enterprise Data Warehouse (EDW) is a centralized warehouse that provides decision support service across the enterprise. It offers a unified approach for organizing and representing data. It also provides the ability to classify data according to the subject and give access according to those divisions.</p> <p>EDWs are usually a collection of databases that offer a unified approach for organizing data and classifying data according to subject. These data sources could be the Databases of various Enterprise Resource Planning (ERP) systems, Customer Relationship Management (CRM) systems, and other forms of Online Transactional Processing (OLTP) systems.</p> <p>Note</p> <p>This type of data warehouse is the most common and serves as a centralized repository for all of an organization's data. An EDW typically integrates data from various sources across the entire enterprise and is designed to support the reporting and analytics needs of multiple departments.</p>"},{"location":"abstract/data_management/data-warehouse/#enterprise-data-warehouse-architecture","title":"Enterprise Data Warehouse Architecture","text":"<p>While there are many architectural approaches that extend warehouse capabilities in one way or another, we will focus on the most essential ones. Without diving into too much technical detail, the whole data pipeline can be divided into three layers:</p> <ul> <li>Raw data layer (data sources)</li> <li>Warehouse and its ecosystem</li> <li>User interface (analytical tools)</li> </ul> <p>The tooling that concerns data Extraction, Transformation, and Loading into a warehouse is a separate category of tools known as ETL. Also, under the ETL umbrella, data integration tools perform manipulations with data before it\u2019s placed in a warehouse. These tools operate between a raw data layer and a warehouse.</p>"},{"location":"abstract/data_management/data-warehouse/#one-tier-architecture","title":"One-tier Architecture","text":"<p>This is considered to be one of the most primitive forms of EDW architectures. In this architecture, the Reporting Tools are connected directly to the Data Warehouse. Although this architecture is easy to set up and implement, it causes various issues for large datasets. Most organizations today have hundreds of Gigabytes of data. This means that to perform any query, the Reporting Tool would have to go through all that data which is a time taking process. Going through the large dataset for each query would result in low performance.</p> <p>Hence, the One-tier EDW Architecture is only suitable for organizations with small datasets.</p> <p></p> <p>One-tier architecture for EDW means that you have a database directly connected with the analytical interfaces where the end user can make queries. Setting the direct connection between an EDW and analytical tools brings several challenges:</p> <ul> <li>Traditionally, you can consider your storage a warehouse starting from 100GB of data. Working with it directly may result in messy query results, as well as low processing speed.</li> <li>Querying data right from the DW may require precise input so that the system will be able to filter out non-required data. Which makes dealing with presentation tools a little difficult.</li> <li>Limited flexibility/analytical capabilities exist.</li> </ul> <p>Additionally, the one-tier architecture sets some limits to reporting complexity. Such an approach is rarely used for large-scale data platforms, because of its slowness and unpredictability. To perform advanced data queries, a warehouse can be extended with low-level instances that make access to data easier.</p>"},{"location":"abstract/data_management/data-warehouse/#two-tier-architecture","title":"Two-tier Architecture","text":"<p>The Two-tier Architecture implements a Data Mart layer between the Reporting layer and the EDW. Data Marts can be seen as smaller Databases that contain domain-specific information, which is only a part of the data stored in the Data Warehouse. All information stored in the Data Warehouse is split into various Data Marts based on the domain of information.</p> <p>The Reporting Tools are then connected to this Data Mart Layer. Because a single Data Mart consists of only a small part of the data in the Data Warehouse (low-level repository that contains domain-specific information), performing queries on it would require much less time than it would on a Data Warehouse. A Two-tier EDW is considered to be more suitable for real-life scenarios.</p> <p></p> <p>Creating a data mart layer will require additional resources to establish hardware and integrate those databases with the rest of the data platform. But, such an approach solves the problem with querying: Each department will access required data more easily because a given mart will contain only domain-specific information. In addition, data marts will limit the access to data for end users, making EDW more secure.</p>"},{"location":"abstract/data_management/data-warehouse/#three-tier-architecture","title":"Three-tier Architecture","text":"<p>A Three-tier Architecture further implements an Online Analytical Processing (OLAP) Layer between the Data Mart Layer and the Reporting Layer. The OLAP Layer consists of OLAP Cubes is a specific type of database which are used to store data in a multidimensional form allowing faster analysis to be performed on the data.</p> <p></p> <p>It\u2019s pretty difficult to explain in words, so let\u2019s look at this handy example of what a cube can look like.</p> <p></p> <p>So, as you can see, a cube adds dimensions to the data. You may think of it as multiple Excel tables combined with each other. The front of the cube is the usual two-dimensional table, where the region (Africa, Asia, etc.) is specified vertically, while sales numbers and dates are written horizontally. The magic begins when we look at the upper facet of the cube, where sales are segmented by routes and the bottom specifies time-period. That\u2019s known as multidimensional data.</p> <p>OLAP Cubes allow various operations to be performed on it, which results in quality analysis. These operations are as follows:</p> Roll-upDrill-downSliceDice <p>This can be defined as the process of reducing the attributes being measured by either performing aggregations or moving up the hierarchy (performing grouping based on a specific order).</p> Example <p>A sample Roll-up operation is as follows:</p> <p></p> <p>This can be defined as the process of increasing the number of attributes being measured to perform a more in-depth analysis by moving down the hierarchy. Drill-down is considered to be the opposite of the Roll-up operation.</p> Example <p>A sample Drill-down operation is as follows:</p> <p></p> <p>This can be defined as the process of removing a dimension by specifying a filter on the dimension to be removed.</p> Example <p>A sample Slice operation is as follows:</p> <p></p> <p>This can be defined as the process of specifying filters for two or more dimensions resulting in the formation of a Sub-Cube.</p> Example <p>A sample Dice operation is as follows:</p> <p></p>"},{"location":"abstract/data_management/data-warehouse/#operational-data-store","title":"Operational Data Store","text":"<p> Operational Data Store (ODS) are nothing but data store required when neither Data warehouse nor OLTP systems support organizations reporting needs. In ODS, Data warehouse is refreshed in real-time. Hence, it is widely preferred for routine activities like storing records of the Employees.</p> <p>Note</p> <p>Unlike traditional data warehouses typically used for long-term storage and historical data analysis, an ODS focuses on providing a current, integrated, and consistent view of operational data from multiple sources. It acts as an intermediary layer between the operational systems (such as transactional databases, CRM systems, or ERP systems) and the data warehouse or data mart.</p> <p>This is a type of data warehouse that stores operational data from various sources and provides near real-time reporting and analysis. It is designed to handle frequent updates and queries from operational systems. It also serves as a source of data for the EDW or data marts.</p> <p> An ODS is a type of data warehouse that stores real-time or near-real-time data from transactional systems. It is designed to support operational reporting and analysis, and it typically uses a bottom-up approach to design, which means that the data model is based on specific business requirements.</p>"},{"location":"abstract/data_management/data-warehouse/#data-mart","title":"Data Mart","text":"<p>Data Mart is a subset of the data warehouse. It specially designed for a particular line of business, such as sales, finance, sales or finance. In an independent data mart, data can collect directly from sources.</p> <p>Data Mart is also a storage component used to store data of a specific function or part related to a company by an individual authority, so data marts are flexible and small.</p> <p>A data mart is a subset of an EDW that is designed to serve a specific business unit or department. It is optimized for querying and reporting on a specific subject area, such as sales or marketing, and it is typically easier and faster to implement than an EDW.</p> <p>Quote</p> <p>In truth, the Kimball model was for data marts, not a data warehouse. A data mart and a data warehouse are fundamentally different things. \u2014 Bill Inmon</p> <p>Quote</p> <p>A data mart is a curated subset of data often generated for analytics and business intelligence users. Data marts are often created as a repository of pertinent information for a subgroup of workers or a particular use case. \u2014 Snowflake</p>"},{"location":"abstract/data_management/data-warehouse/#comparison","title":"Comparison","text":"EDW ODS DM Purpose Serves the entire organization Supports operational reporting Serves a specific business unit/department Data Integration Integrates data from multiple sources Integrates real-time data from transactional systems Integrates data from a specific subject area Data Model Top-down approach to design Bottom-up approach to design Designed based on specific business requirements Complexity More complex and time-consuming to design and implement Less complex and quicker to implement Less complex and quicker to implement Query and Analysis Supports complex queries and analytics Supports operational reporting and simple analysis Optimized for querying and reporting on a specific subject area Data Volume Large volume of historical data Real-time or near-real-time data Smaller volume of data Users Business analysts, executives, data scientists Operational staff, business analysts Business analysts, departmental staff Cost Higher cost due to complexity and scale Lower cost due to simpler design and implementation Lower cost due to simpler design and implementation Criteria EDW ODS DM Scope Enterprise-wide Operational Departmental or functional Data sources Multiple internal and external sources Multiple operational sources EDW, ODS, or other sources Data integration High degree of integration and standardization Moderate degree of integration and standardization Low degree of integration and standardization Data granularity Mixed levels of granularity Low level of granularity (detailed) High level of granularity (aggregated or summarized) Data currency Historical and current data Near real-time or real-time data Historical and current data Data quality High quality (cleansed and validated) Moderate quality (some cleansing and validation) High quality (cleansed and validated) Data structure Relational or dimensional models Relational models Dimensional models Data volume Very large (terabytes or petabytes) Large (gigabytes or terabytes) Small or medium (megabytes or gigabytes) Query performance Moderate to high (depends on indexing and partitioning) Low to moderate (depends on updates and concurrency) High (optimized for analysis) Query complexity High (supports complex and ad-hoc queries) Low to moderate (supports simple and predefined queries) Moderate to high (supports complex and ad-hoc queries) Query frequency Low to moderate (periodic or on-demand) High (continuous or near-continuous) Moderate to high (periodic or on-demand) User types Analysts, managers, executives, data scientists, etc. Operational staff, managers, etc. Analysts, managers, etc."},{"location":"abstract/data_management/data-warehouse/#architecture","title":"Architecture","text":""},{"location":"abstract/data_management/data-warehouse/#bottom-tier-data-layer","title":"Bottom tier (Data Layer)","text":"<p>The bottom tier consists of the Data Repository, usually a relational database system, which collects, cleanses, and transforms data from various data sources through a process known as Extract, Transform, and Load (<code>ETL</code>) or a process known as Extract, Load, and Transform (<code>ELT</code>).</p> <p>As a preliminary process, before the data is loaded into the repository, all the data relevant and required are identified from several sources of the system. These data are then cleaned up, to avoid repeating or junk data from its current storage units. The next step is to transform all these data into a single format of storage. The final step of ETL is to Load the data on the repository.</p> <p>ETL Layer, This layer is responsible for storing the data in the data warehouse. The data is typically stored in a relational database management system (RDBMS), which is optimized for querying and reporting on large datasets. In some cases, data may also be stored in columnar or in-memory databases for improved performance.</p> <p>Few commonly used ETL tools are:</p> <ul> <li>Informatica</li> <li>Microsoft SSIS</li> <li>Snaplogic</li> <li>Confluent</li> <li>Apache Kafka</li> <li>Alooma</li> <li>Ab Initio</li> <li>IBM Infosphere</li> </ul>"},{"location":"abstract/data_management/data-warehouse/#middle-tier-semantics-layer","title":"Middle tier (Semantics Layer)","text":"<p>The middle tier consists of an <code>OLAP</code> (Online Analytical Processing) servers which enables fast query speeds. The Data Warehouse can have more than one OLAP server, and it can have more than one type of OLAP server model as well, which depends on the volume of the data to be processed and the type of data held in the bottom tier.</p> <p>ETL Layer. This layer is responsible for extracting, transforming, and loading the data from various sources into the data warehouse. This is typically done using ETL (Extract, Transform, Load) tools, which automate the process of moving and converting data.</p> <p>Three types of OLAP models can be used in this tier, which are known as</p> ROLAPMOLAPHOLAP <p>Relational online analytical processing is a model of online analytical processing which carries out an active multidimensional breakdown of data stored in a relational database, instead of redesigning a relational database into a multidimensional database.</p> <p>This is applied when the repository consists of only the relational database system in it.</p> <p>Multidimensional online analytical processing is another model of online analytical processing that catalogs and comprises directories directly on its multidimensional database system.</p> <p>This is applied when the repository consists of only the multidimensional database system in it.</p> <p>Hybrid online analytical processing is a hybrid of both relational and multidimensional online analytical processing models.</p> <p>When the repository contains both the relational database management system and the multidimensional database management system, HOLAP is the best solution for a smooth functional flow between the database systems. HOLAP allows storing data in both the relational and the multidimensional formats.</p> <p>The type of OLAP model used is dependent on the type of database system that exists.</p>"},{"location":"abstract/data_management/data-warehouse/#top-tier-analytics-layer","title":"Top tier (Analytics Layer)","text":"<p>The top tier is represented by some kind of front-end user interface or reporting tool, which enables end users to conduct ad-hoc data analysis on their business data. It holds various tools like query tools, analysis tools, reporting tools, and data mining tools.</p> <p>Reporting Layer. This layer is responsible for presenting the data to end-users in a format that is easy to understand and analyze. This layer includes tools for querying, reporting, and visualization, which allow users to create custom reports and dashboards based on the data in the data warehouse.</p> <p>Below are the few commonly used Top Tier tools.</p> <ul> <li>IBM Cognos</li> <li>Microsoft BI Platform</li> <li>SAP Business Objects Web</li> <li>Pentaho</li> <li>Crystal Reports</li> <li>SAP BW</li> <li>SAS Business Intelligence</li> </ul> <p>In addition to the three-tier architecture, some data warehouse architectures also include a metadata layer, which provides information about the data in the data warehouse, such as its origin, format, and meaning. The metadata layer can be used to help users understand and navigate the data.</p>"},{"location":"abstract/data_management/data-warehouse/#design","title":"Design","text":"<p>data warehouse design that can be applied within the framework of the design methods discussed earlier. Here is a brief overview of how each of these approaches relates to the design methods.</p> <p>Overall, the choice of design approach will depend on the specific needs and circumstances of the organization. A bottom-up approach may be more appropriate for organizations with complex and varied data sources, while a top-down approach may be more appropriate for organizations with well-defined business requirements. Hybrid design may be a good choice for organizations that need a flexible and adaptable data warehouse that can accommodate changing business requirements over time.</p>"},{"location":"abstract/data_management/data-warehouse/#bottom-up","title":"Bottom-up","text":"<p>Bottom-up design is an approach to data warehouse design that focuses on building small, specialized data marts first and then integrating them into a larger data warehouse. This approach is often used when there are different data sources with varying levels of complexity, and it allows for a more incremental and flexible approach to data warehouse development. Bottom-up design is often associated with dimensional modeling and may use hybrid modeling techniques to integrate the different data marts.</p> <p>Example</p> <p>Dimensional Model (Ralph Kimball):</p> <p>The Dimensional Model, also known as the Kimball model, is a bottom-up design approach that emphasizes the importance of simplicity and ease of use. This model is designed to support ad-hoc querying and analysis and is often used for data warehouse implementations in smaller organizations. The Kimball model involves creating a denormalized data model, which is optimized for querying and analysis, and building a star or snowflake schema that supports specific business functions.</p> <p>See more, Kimball Approach</p>"},{"location":"abstract/data_management/data-warehouse/#top-down","title":"Top-down","text":"<p>Top-down design is an approach to data warehouse design that starts with a comprehensive enterprise data model and then designs the data warehouse based on that model. This approach is often used when there is a well-defined set of business requirements and a clear understanding of the data sources and their relationships. Top-down design is often associated with data vault modeling and may use hybrid modeling techniques to accommodate the specific business requirements.</p> <p>Example</p> <p>Third Normal Form Model (Bill Inmon):</p> <p>The 3NF Model, also known as the Inmon model, is a top-down design approach that emphasizes the importance of a comprehensive enterprise data model. This model is designed to support complex business processes and is often used for data warehouse implementations in large organizations. The Inmon model involves creating a normalized data model, which is then used to build data marts that support specific business functions.</p> <p>See more, Inmon Approach</p>"},{"location":"abstract/data_management/data-warehouse/#hybrid","title":"Hybrid","text":"<p>Hybrid design is an approach to data warehouse design that combines elements of both bottom-up and top-down design. This approach recognizes that there may be benefits to both approaches and seeks to find a balance between the two. Hybrid design may use different modeling techniques for different parts of the data warehouse and may involve a mix of top-down and bottom-up development. Hybrid design is often associated with agile modeling and may use a variety of design methods to create a flexible and adaptable data warehouse.</p> <p>Example</p> <p>Data Vault 2.0 Model:</p> <p>The DV 2.0 Model is a hybrid design approach that combines elements of both the Inmon and Kimball models. This model is designed to support flexibility, scalability, and agility, and is often used for data warehouse implementations in organizations that need to handle large amounts of complex and varied data. The Data Vault 2.0 model involves creating a normalized data model that separates business entities and relationships into three types of tables (Hub, Link, and Satellite), which can then be used to build data marts that support specific business functions.</p> <p>See more, Data Vault Model</p> <p>Differences between the three designs:</p> <ul> <li> <p>Approach: Kimball is bottom-up, Inmon is top-down, and Data Vault is a hybrid   approach.</p> </li> <li> <p>Schema: Kimball uses a star or snowflake schema, Inmon uses a 3NF schema,   and Data Vault uses a hub-and-spoke schema.</p> </li> <li> <p>Focus: Kimball focuses on the business process or subject area, Inmon focuses   on the data, and Data Vault focuses on the relationships between the data.</p> </li> <li>Flexibility: Kimball is known for its flexibility, Inmon is known for its   consistency, and Data Vault is known for its ability to handle complex data relationships.</li> <li>Complexity: Kimball is relatively simple, Inmon is more complex, and   Data Vault is the most complex of the three methodologies.</li> </ul>"},{"location":"abstract/data_management/data-warehouse/#summary","title":"Summary","text":"<p>Overall, the choice of design method will depend on the specific needs and circumstances of the organization. The Inmon model may be more appropriate for organizations with complex and varied data sources and a focus on enterprise-wide integration. The Kimball model may be more appropriate for organizations with a focus on ad-hoc querying and analysis and a need for simplicity and ease of use. The Data Vault 2.0 model may be a good choice for organizations that need a flexible and scalable data warehouse that can accommodate changing business requirements over time.</p> Design Method 3NF Model (Inmon) Dimensional Model (Kimball) Data Vault 2.0 Model Description Top-down design approach that emphasizes a comprehensive enterprise data model Bottom-up design approach that emphasizes simplicity and ease of use Hybrid design approach that combines elements of the Inmon and Kimball models Strengths Supports complex business processes, supports enterprise-wide integration, allows for data reuse Supports ad-hoc querying and analysis, easy to understand and use, can be quickly implemented Supports flexibility, scalability, and agility, accommodates changing business requirements, allows for data reuse Weaknesses Can be time-consuming to design and build, may not be well-suited for ad-hoc querying and analysis May not be well-suited for complex business processes or enterprise-wide integration, may not support as much data reuse as the Inmon model Can be more complex to design and build than the Inmon or Kimball models, may not be as well-suited for smaller organizations or simple business processes Focus Enterprise-wide integration Ad-hoc querying and analysis Ad-hoc querying and analysis Advantages Comprehensive data model supports complex business processes; Data is normalized, reducing data redundancy and ensuring data consistency Easy to understand and use for ad-hoc querying and analysis; Denormalized data model optimized for querying and analysis Separates business entities and relationships into three types of tables, providing flexibility and scalability; Supports complex and varied data sources <p>As a Data Engineer, the choice of design method will depend on the specific needs and circumstances of your organization. If you work in a large organization with complex and varied data sources and a focus on enterprise-wide integration, the 3NF Model (Inmon) may be a good choice. If your organization has a focus on ad-hoc querying and analysis and a need for simplicity and ease of use, the Dimensional Model (Kimball) may be a better fit. If your organization needs a flexible and scalable data warehouse that can accommodate changing business requirements over time, the Data Vault 2.0 model may be the best option.</p>"},{"location":"abstract/data_management/data-warehouse/#read-mores","title":"Read Mores","text":"<ul> <li>Guru99: Data Warehousing</li> <li>Guru99: Data Warehouse Architecture</li> <li> IBM: Data Warehouse</li> <li>A Complete Guide to Data Warehouse in 2022</li> <li>CodingNinjas: Inmon vs Kimball Approaches in DWH</li> <li>Nearshore: Data Warehouse Architecture</li> </ul>"},{"location":"abstract/data_mesh/","title":"Data Mesh","text":"<p>Data Mesh is a modern architectural approach in data management and analytics. It shifts away from traditional centralized data management models (like data warehouses and lakes) and advocates for a decentralized approach.</p> <p>Here are the key characteristics and principles of Data Mesh:</p> <ul> <li> <p>Domain-Oriented Decentralized Data Ownership and Architecture</p> <p>Data Mesh emphasizes that data should be managed and owned by domain-specific teams (e.g., sales, marketing, logistics) rather than a centralized data team. This approach allows each team to control and optimize their data based on their specific needs and expertise.</p> </li> <li> <p>Data as a Product</p> <p>Data is treated as a product, with each domain team responsible for the lifecycle of the data products they own. This includes ensuring data quality, reliability, and usability. Data products are built to be discoverable, understandable, trustworthy, and usable by other teams.</p> </li> <li> <p>Self-Serve Data Infrastructure as a Platform</p> <p>To enable domain teams to manage their data products effectively, a self-serve data platform is provided. This platform offers tools and capabilities for data storage, processing, and analytics, ensuring teams can access and use data with autonomy but without needing to manage complex data infrastructure.</p> </li> <li> <p>Federated Computational Governance</p> <p>Data Mesh also involves a federated approach to governance. While teams have autonomy over their data, there are overarching guidelines and policies to ensure compliance, security, and interoperability across the organization.</p> </li> </ul>"},{"location":"abstract/data_mesh/#noted","title":"Noted","text":"<ul> <li>Data Mesh in Practice</li> <li>Navigating Your Data Platform\u2019s Growing Pains: A Path from Data Mess to Data Mesh</li> <li>https://medium.com/@think-data/data-mesh-architecture-in-modern-data-engineering-762ac7f8901b</li> </ul>"},{"location":"abstract/data_mesh/#questions","title":"Questions","text":"<ul> <li>Is data mesh only for analytical data?</li> </ul>"},{"location":"abstract/data_mesh/#read-mores","title":"Read Mores","text":"<ul> <li>Data Mesh: A modern architectural approach</li> </ul>"},{"location":"abstract/data_observability/","title":"Data Observability","text":"<p>Data Observability is an organization's ability to fully understand the health of the data in their systems. Data observability eliminates data downtime by applying best practices learned from DevOps to data pipeline observability.</p>"},{"location":"abstract/data_observability/#getting-started","title":"Getting Started","text":"<p> Data observability Tools use automated monitoring, automated root cause analysis, data lineage and data health insights to detect, resolve, and prevent data anomalies. This leads to healthier pipelines, more productive teams, better data management, and happier customers.</p> <p>For data engineers and developers, data observability is important because data downtime means wasted time and resources; for data consumers, it erodes confidence in your decision-making.</p> <p>Quote</p> <p>Data Downtime: periods of time when data is partial, erroneous, missing, or otherwise inaccurate. Only multiplies as data systems become increasingly complex, supporting an endless ecosystem of sources and consumers.</p> <p>The 5 pillars of data observability:</p> <ol> <li> <p>Freshness:</p> <p>Freshness seeks to understand how up-to-date your data tables are, as well as the cadence at which your tables are updated. Freshness is particularly important when it comes to decision-making; after all, stale data is basically synonymous with wasted time and money.</p> </li> <li> <p>Quality:</p> <p>Your data pipelines might be in working order but the data flowing through them could be garbage. The quality pillar looks at the data itself and aspects such as percent NULLS, percent uniques and if your data is within an accepted range. Quality gives you insight into whether or not your tables can be trusted based on what can be expected from your data.</p> </li> <li> <p>Volume:</p> <p>Volume refers to the completeness of your data tables and offers insights on the health of your data sources. If 200 million rows suddenly turns into 5 million, you should know.</p> </li> <li> <p>Schema:</p> <p>Changes in the organization of your data, in other words, schema, often indicates broken data. Monitoring who makes changes to these tables and when is foundational to understanding the health of your data ecosystem.</p> </li> <li> <p>Lineage:</p> <p>When data breaks, the first question is always \u201cwhere?\u201d Data lineage provides the answer by telling you which upstream sources and downstream ingestors were impacted, as well as which teams are generating the data and who is accessing it. Good lineage also collects information about the data (also referred to as metadata) that speaks to governance, business, and technical guidelines associated with specific data tables, serving as a single source of truth for all consumers.</p> </li> </ol> <p>Together, these components provide valuable insight into the quality and reliability of your data. Let\u2019s take a deeper dive.</p>"},{"location":"abstract/data_observability/#the-key-features-of-data-observability-tools","title":"The key features of data observability tools","text":"<p>Evaluation criteria can be tricky when you may not even have a strong answer to the basic question, \"what are data observability tools?\" A great data observability platform has the following features:</p> <ul> <li> <p>It connects to your existing stack quickly and seamlessly and does not require   modifying your data pipelines, writing new code, or using a particular programming   language. This allows quick time to value and maximum testing coverage without   having to make substantial investments.</p> </li> <li> <p>It monitors your data at-rest and does not require extracting the data from where   it is currently stored. This allows the data observability solution to be performant,   scalable and cost-efficient. It also ensures that you meet the highest levels   of security and compliance requirements.</p> </li> <li> <p>It requires minimal configuration and practically no threshold-setting. Data   observability tools should use machine learning models to automatically learn   your environment and your data. It uses anomaly detection techniques to let you   know when things break. It minimizes false positives by taking into account not   just individual metrics, but a holistic view of your data and the potential impact   from any particular issue. You do not need to spend resources configuring and   maintaining noisy rules within your data observability platform.</p> </li> <li> <p>It requires no prior mapping of what needs to be monitored and in what way.   It helps you identify key resources, key dependencies and key invariants so that   you get broad data observability with little effort.</p> </li> <li> <p>It provides rich context that enables rapid triage and troubleshooting, and   effective communication with stakeholders impacted by data reliability issues.   Data observability tools should not stop at \u201cfield X in table Y has values lower   than Z today.\u201d</p> </li> <li> <p>It prevents issues from happening in the first place by exposing rich information   about data assets so that changes and modifications can be made responsibly and   proactively.</p> </li> </ul>"},{"location":"abstract/data_observability/#references","title":"References","text":"<ul> <li>(TODO): https://sanjmo.medium.com/is-data-observability-critical-to-successful-data-analytics-d09b983b95c6</li> <li>MontecarloData: What is Data Observability</li> <li>https://www.montecarlodata.com/blog-data-observability-tools/</li> <li>https://snowplow.io/blog/data-observability-dashboard/</li> <li>https://www.youtube.com/watch?v=4K33fP46vDw</li> <li>https://www.montecarlodata.com/blog-what-is-data-observability/</li> </ul>"},{"location":"abstract/data_observability/dobs-data-cicd/","title":"Data CICD","text":"<p>https://blog.stackademic.com/ci-cd-for-modern-data-engineering-b64d9e76393a</p>"},{"location":"abstract/data_observability/dobs-data-cicd/#the-data-lifecycle","title":"The Data Lifecycle","text":"<p>Application development starts with Ideation and product requirement and ends with product release and monitoring. On the other hand, data life cycle is unique because it starts with data creation, transformation, deployment and all the way to data deletion (thanks to data privacy regulations that require organizations to delete personal data of users, if requested).</p> <p></p> <ul> <li>BlogDetLeft; CICD for Data</li> </ul>"},{"location":"abstract/data_observability/dobs-data-consistency/","title":"Data Consistency","text":"<p>Data Consistency is one of ten dimensions of data quality. Data is considered consistent if two or more values in different locations are identical. Ask yourself: Is the data internally consistent? If there are redundant data values, do they have the same value? Or, if values are aggregations of each other, are the values consistent with each other?</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#what-are-some-examples-of-inconsistent-data","title":"What are some examples of inconsistent data?","text":"<p>Imagine you\u2019re a lead analytics engineer at Rainforest, an ecommerce company that sells hydroponic aquariums to high-end restaurants. An example of data inconsistency here would be if the engineering team records aquarium models from database transactions that don\u2019t match the models recorded by the sales team from the CRM.</p> <p></p> <p>Another example would be if the monthly profit number is not consistent with the monthly revenue and cost numbers. Some of the ways that this could happen would be if you have concurrent workloads, which could be in the form replication pipelines themselves, or downstream SQL transformations that lead to additional nodes (forks) in your end to end pipelines. The solution to all of this would be proper data management, starting with measuring for data consistency.</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#how-do-you-measure-data-consistency","title":"How do you measure data consistency?","text":"<p>To test your any data quality dimension, you must measure, track, and assess a relevant data quality metric. In the case of data consistency, you can measure the number of passed checks to track the uniqueness of values, uniqueness of entities, corroboration within the system, or whether referential integrity is maintained. Codd\u2019s Referential Integrity constraint is one example of a consistency check.</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#how-to-ensure-data-consistency","title":"How to ensure data consistency?","text":"<p>One way to ensure data consistency is through anomaly detection, sometimes called outlier analysis, which helps you to identify unexpected values or events in a data set.</p> <p>Using the example of two numbers that are inconsistent with one another, anomaly detection software would notify you instantly when data you expect to match doesn\u2019t. The software knows it\u2019s unusual because its machine learning model learns from your historical metadata.</p> <p>Quote</p> <p>\u201cThe important thing is that when things break, I know immediately \u2014 and I can usually fix them before any of my stakeholders find out.\u201d<sup>1</sup></p> <p>In other words, you can say goodbye to the dreaded WTF message from your stakeholders. In that way, automated, real-time anomaly detection is like a friend who has always got your back.</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#how-does-data-consistency-differ-from-application-consistency-vs-strong-consistency","title":"How does data consistency differ from application consistency vs strong consistency?","text":"<p>For purposes of this article, we\u2019ve be focused solely on data consistency as it relates to the actual values themselves. You may see some overlap with Strong Consistency and Application Consistency, other terms in the data space:</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#strong-consistency","title":"Strong Consistency","text":"<p>You may run into this term when looking up database consistency as well, particularly in complex database system architectures. Strong consistency is all about ensuring that everyone in a distributed system is on the same page when it comes to data, and includes the concepts from CAP theorem. It means that no matter which node or replica you\u2019re looking at, they all have the most up-to-date view of the data at any given time. It\u2019s like making sure everyone sees things happening in the same order, as if there\u2019s only one copy of the data. So, when you read something, you can always trust that you\u2019re getting the latest version. Achieving strong consistency usually involves using coordination mechanisms like distributed transactions or consensus algorithms to make sure the data stays intact and in sync across the entire distributed system.</p> <p>Note</p> <p>It\u2019s important here to maintain atomicity of timestamps to ensure you don\u2019t miss any data changes.</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#application-consistency","title":"Application Consistency","text":"<p>Application consistency refers to making sure that the data within an application (app), typically hosted in your database system, is in good shape and follows the rules and requirements set by that application. It\u2019s like ensuring that everything is in order and makes sense according to how the app is supposed to work. When an app is consistent, you can trust that the data is accurate, complete, and meets the specific rules or relationships defined by the app. It\u2019s all about making sure things run smoothly and produce reliable results. To achieve application consistency, developers need to implement checks and safeguards to validate data, handle errors effectively, and enforce the application\u2019s unique rules.</p> <p>Note</p> <p>A crossover here in data governance may be to utilize data validation during a user\u2019s data entry (e.g. email) to ensure that downstream usage of that field can be maintained.</p> <ol> <li> <p> What is Data Consistency? Definition, Examples, and Best Practices \u21a9</p> </li> </ol>"},{"location":"abstract/data_observability/dobs-data-lineage/","title":"Data Lineage","text":""},{"location":"abstract/data_observability/dobs-data-lineage/#references","title":"References","text":"<ul> <li>https://pub.towardsai.net/understanding-data-lineage-from-source-to-destination-e505f2cd19ba</li> <li>https://medium.com/alvin-ai/what-is-data-lineage-and-techniques-to-implement-it-4f1939b11327</li> <li>Mastering Data Lineage: Techniques, Best Practices, and Tools for Success</li> </ul>"},{"location":"abstract/data_observability/dobs-data-orchestration/","title":"Data Orchestration","text":"<p>Quote</p> <p>Data Orchestration is a Conductor for the Modern Data Platform.<sup>1</sup></p> <p>At the core of a Data Orchestration Platform lies a workflow management system for dependably handling tasks that move and transform data so that it can ultimately be used to generate artifacts to inform business decisions, including dashboards, reports, and machine-learning (ML) predictions. However, a data-driven organization requires capabilities beyond workflow management, in order to increase the availability of trusted data to a broad base of business users beyond the centralized information technology group.</p> <ol> <li> <p>Data Orchestration: A Conductor for the Modern Data Platform \u21a9</p> </li> </ol>"},{"location":"abstract/data_observability/dobs-data-quality-metrics/","title":"Data Quality Metrics","text":""},{"location":"abstract/data_observability/dobs-data-quality-metrics/#references","title":"References","text":"<ul> <li>Data Quality Metrics</li> <li>Data Quality Metrics for Data Warehouse</li> </ul>"},{"location":"abstract/data_observability/dobs-data-quality-pyramid/","title":"Data Observability: Data Quality Pyramid","text":"<ul> <li>The Data Quality Pyramid: A Path to Getting Started with Data Observability</li> </ul>"},{"location":"abstract/data_observability/dobs-maintaining-a-viable-monitoring-sys/","title":"Data Observability: Maintaining a Viable Monitoring System","text":"<p>https://medium.com/@wyaddow/maintaining-a-viable-monitoring-system-for-data-observability-b510152ecfa8</p>"},{"location":"abstract/data_pipeline/","title":"Data Pipeline","text":"<ul> <li>Pipeline Design Patterns</li> <li>Workflow Orchestration</li> <li>Data Lineage Tracking</li> <li>Pipeline Monitoring and Observability</li> </ul>"},{"location":"abstract/data_pipeline/#noted","title":"Noted","text":"<ul> <li>Creating a Data Pipeline from Scratch</li> <li>https://medium.com/@kxnk/data-pipelines-pocket-reference-key-learning-points-bb9225ee95a9</li> <li>Mastering Data Engineering: A breakdown of Data Pipeline Stages and Tools</li> </ul>"},{"location":"abstract/data_pipeline/#type-of-data-pipelines","title":"Type of Data Pipelines","text":"<ul> <li>https://hardiks.medium.com/types-of-data-pipelines-you-need-to-look-at-efa9eaac4a79</li> </ul>"},{"location":"abstract/data_pipeline/#optimization","title":"Optimization","text":"<ul> <li>How we think about Data Pipelines is changing</li> </ul>"},{"location":"abstract/data_strategy/","title":"Data Strategy","text":"<p>Data Strategy is a long-term plan that defines the technology, processes, people, and rules required to manage an organization's information assets. All types of businesses collect large amounts of raw data today. However, they need a well-thought-out data management and analytics plan if they want to use this information to make informed decisions and create machine learning (ML) or generative artificial intelligence (AI) applications.</p> <p>A Data Strategy outlines an organization's long-term vision for collecting, storing, sharing, and usage of its data. It makes working with data easier to do at every step of the data journey for everyone who needs it in your organization.</p> <ul> <li>Query Optimization</li> <li>Indexing Strategies</li> <li>Caching Mechanisms</li> <li>Distributed Computing for Big Data</li> </ul>"},{"location":"abstract/data_strategy/#references","title":"References","text":"<ul> <li>What is Data Strategy?</li> <li>TODO - A Data Strategy Comprehensive Guide for a Step-by-Step Data Maturity Assessment</li> </ul>"},{"location":"abstract/data_strategy/strategy-data-driven/","title":"Data Driven","text":""},{"location":"abstract/data_strategy/strategy-data-driven/#references","title":"References","text":"<ul> <li>Medium: Data Driven Management - The Why Who What and How</li> </ul>"},{"location":"abstract/data_strategy/strategy-denormalization-form/","title":"De-Normalization","text":"<p>De-normalization is an optimization technique to make our database respond faster to queries by reducing the number of joins needed to satisfy user needs.</p> <ul> <li> <p>In de-normalization, we mainly aim to reduce the number of tables that are needed   by re-joining these tables together and add redundant data.</p> </li> <li> <p>De-normalization is commonly used with read-intensive, low number of updates and   high number of read queries, systems such as Data Warehouse (DWH).</p> </li> </ul> <p>Quote</p> <p>De-normalization is a strategy used on a previously-normalized database to increase performance. In computing, denormalization is the process of trying to improve the read performance of a database, at the expense of losing some write performance, by adding redundant copies of data or by grouping data. It is often motivated by performance or scalability in relational database software needing to carry out very large numbers of read operations. Denormalization should not be confused with Unnormalized form. Databases/tables must first be normalized to efficiently denormalize them.</p> <p>By Wiki Denormalization</p> <p>Note</p> <p>De-normalization doesn't mean that we won't normalize our tables, It's like we said before, an optimization technique that used after normalizing our table to make it faster in some cases.</p>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#de-normalization-techniques","title":"De-Normalization Techniques","text":"<p>To de-normalize our normalized table, We will follow some methods that will be discussed below.</p> <p>Before doing de-normalization you have to make sure of two things:</p> <ul> <li>The performance of the normalized system doesn't satisfy the user.</li> <li>De-normalization is the right solution for this performance issue.</li> </ul> <p>So briefly, De-normalization is used for:</p> <ul> <li>Reduce the number and need for joins.</li> <li>Reduce the number of needed tables.</li> <li>Reduce foreign keys of your database.</li> </ul>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#adding-redundant-columns","title":"Adding Redundant columns","text":"<p>We can apply it by adding commonly used columns in joins to the joined table for reducing or eliminating join operations.</p> <p>For example,</p> <p>If we have a customers table and orders table, the orders table does have customer_id only (as a foreign key) that referenced to customers table, but It doesn't have customer_name. When we need to retrieve a list of all orders with the customer name, we will have to join these tables together.</p> <pre><code>SELECT\n    C.CUSTOMER_NAME,\n    O.ORDER_NAME\nFROM CUSTOMERS  AS C\nJOIN ORDERS     AS O\n    ON C.CUSTOMER_ID = O.CUSTOMER_ID\n;\n</code></pre> <p>To de-normalize this table, we will add a redundant customer_name column to Orders, Which will increase performance, and we won't need to join this table again.</p> <pre><code>SELECT\n    O.CUSTOMER_NAME,\n    O.ORDER_NAME\nFROM ORDERS O\n;\n</code></pre> <p>Drawbacks of this method:</p> <ul> <li>Maintenance will be costly, and increase update overhead, because updates will   be made for two tables: customers(name) and orders(customer_name).</li> <li>More storage will be needed because customer_name is duplicated now.</li> <li>Increase in Table Size.</li> <li>In case of an update in one value of a customer, you have to update all the   records of that customer in the fact table which will be a very costly operation   for big Fact tables.</li> </ul>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#coming-tables","title":"Coming tables","text":"<p>We can apply it by combine tables that are joined together frequently into one table, which will eliminate join, and increase performance significantly.</p> <p>For example,</p> <p>If we frequently need to generate a report that shows all Employees their full addresses, we will have to join these tables together.</p> <pre><code>SELECT E.* , A.CITY+\", \"+A.STATE+\", \"+A.DISTRICT+\", \"+A.ZIP AS \"FULL ADDRESS\"\nFROM EMPLOYEES E\nJOIN ADDRESSES A\nON A.LOCATION_ID = e.LOCATION_ID;\n</code></pre> <p>So to de-normalize this situation, We will combine employees' table and addresses table into one table, And combine address table attributes into one attribute (Full address) to make querying easier, This solution will increase performance significantly, and eliminate costly joins.</p> <pre><code>SELECT *\nFROM EMPLOYEES;\n</code></pre>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#adding-derived-column","title":"Adding Derived column","text":"<p>Adding derived columns to our table can help us to eliminate joins, and improve the performance of aggregating our data.</p> <p>A derived column is attribute whose value is derived from another attribute. For example: Using date_of_birth attribute to generate age attribute.</p> <p>For example,</p> <p>If we need to generate a report containing Students and their grades, We will join students table and grades table, and start comparing marks with grades to know the grade of each student.</p> <pre><code>SELECT S.NAME, S.MARKS, G.GRADE\nFROM STUDENTS S\nJOIN GRADES G\nON S.MARKS BETWEEN G.MIN_MARK AND G.MAX_MARK;\n</code></pre> <p>So to de-normalize this situation, We will use marks in students' table and compare them to predefined values to generate grades with no need of joining grades with students.</p> <pre><code>SELECT NAME, MARKS,\nCASE\n  WHEN MARKS &gt;= 85 AND MARKS &lt;= 100 THEN \"A\"\n  WHEN MARKS &gt;= 75 AND MARKS  &lt; 85  THEN \"B\"\n  WHEN MARKS &gt;= 65 AND MARKS  &lt; 75  THEN \"C\"\n  WHEN MARKS &gt;= 50 AND MARKS  &lt; 65  THEN \"D\"\n  ELSE \"F\"\nEND AS \"GRADE\"\nFROM STUDENTS;\n</code></pre>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#partitioning-relation","title":"Partitioning Relation","text":"<p>In this approach, We won't combine tables together, We will go for decomposing them into multiple smaller manageable tables, It will decrease the size that we have to read, which will impact the performance of operations in some meaningful way.</p>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#horizontal-partitioning","title":"Horizontal partitioning","text":"<p>Horizontal partitioning or Row Splitting. Split our main table rows into smaller partitions (tables) that will have the same columns.</p> <p>This approach aims to make where clause more efficient by making it search in a smaller amount of data, Filter specify only a subset of the table that related to the query, not the whole table, and reduced I/O overhead.</p>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#vertical-partitioning","title":"Vertical partitioning","text":"<p>Vertical partitioning or Column Splitting. In Vertical partitioning, We distribute table attributes across multiple partitions with primary key duplicated for each partition to make reconstructing of original table easier. We partition our table based on frequently used attributes and rarely used attributes.</p> <p>We need to use this approach When some columns are frequently accessed more than other columns, To reduce table header size, And retrieve only the required attributes.</p> <p>Warning</p> <p>In case you have multiple requirements that needs the data combined you will have to join the tables again which will cause performance problems.</p>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#materialized-views","title":"Materialized Views","text":"<p>Materialized Views can improve performance and decrease time-consuming significantly, by using it to precomputing and store the result of costly queries like join and aggregation as view in your storage disk for future usage.</p> <p>Materialized View is all about run one time, and read many times.</p> <p>When you need to use a query frequently, you can store it as materialized view, So in the future, you can retrieve the result of your query directly from the view stored in your disk, So you don't need to recompute query again.</p> <p>But you should to know that,</p> <ul> <li>Data will be updated once, And to refresh you have to re-run the query again.</li> <li>The unavailable source will block maintenance of view.</li> <li>Data are replicated, And it needs more storage.</li> </ul>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#drawbacks-of-de-normalization","title":"Drawbacks of De-Normalization","text":"<ul> <li>De-normalization can slow updates, with many update overheads.</li> <li>De-normalization can increase your table and database size.</li> <li>De-normalization in some cases can make querying more complex instead of making it easier.</li> <li>More storage will be needed for duplicated data.</li> </ul>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#de-normalization-and-data-warehouses","title":"De-Normalization and Data Warehouses","text":"<p>De-normalization is stable and commonly used with data warehouses, Data warehouse is a Specially created data repository for decision-making, It involves a large historical data repository related to the organization, The typical data warehouse is a subject-oriented corporate database that involves multiple data models implemented on multiple platforms and architectures.</p> <p>There are some aspects to consider when building data warehouses:</p> <ul> <li> <p>Extraction of data from several sources that may be homogeneous or heterogeneous.</p> </li> <li> <p>Initialize data for compatibility in the data warehouse.</p> </li> <li> <p>Cleaning the data with validity, and is done through the database from which the data were taken.</p> </li> <li> <p>Monitor and control the data warehouse while it is uploading data.</p> </li> <li> <p>Update data every period of time.</p> </li> </ul> <p>De-normalization can increase the speed of retrieval and optimize query performance for data warehouses, with some drawbacks of update anomalies, but it won't be a big problem because data warehouse is not typically used for update, It's commonly used for reading operations, which are used for analysis and decision-making as we said, hence, a data warehouse is a great source for applying de-normalization, because it attributes rarely updated.</p>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#use-case-star-schema","title":"Use Case: Star Schema","text":"<p>Dimensional Model: It is a special model that is an alternative to an entity-relationship (ER) model consisting of the ER information and itself, but combines the data in an abbreviated form that makes the model more understandable to perform the queries efficiently, and has the flexibility to change (Dimensional model is the modeling approach of data warehouses).</p> <p>Star schema is the simplest and easiest dimensional model for data warehouses, because of that it's the most suitable schema for query processing, and it's highly de-normalized, but its drawback is its need for a large space to store data.</p> <p>Star schema consists of a fact table with a single table for each dimension.</p> <ul> <li>A fact table in a pure star schema consists of multiple foreign keys, each paired   with a primary key in a dimension, together with the facts containing the measurements.</li> <li>Typically, normalized.</li> <li>Dimension tables not joined for each other.</li> <li>It joined using a fact table that does have a foreign key for each dimension.</li> <li>Typically, heavily de-normalized.</li> </ul>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#summary","title":"Summary","text":"<p>De-normalization aims to add redundancy to your database for better performance, It also a great optimization technique that will help you to decrease query processing time. with drawbacks of reducing the integrity of your system, slowing data manipulation operations, and need more space for storing redundant data.</p>"},{"location":"abstract/data_strategy/strategy-denormalization-form/#references","title":"References","text":"<ul> <li>DataValley: De-normalization When, Why, and How</li> </ul>"},{"location":"abstract/data_strategy/strategy-normalization-form/","title":"Normalization","text":"<p>Normalization is a database design technique that reduces data redundancy and eliminates undesirable characteristics like Insertion, Update and Deletion Anomalies. Normalization rules divides larger tables into smaller tables and links them using relationships. The purpose of Normalization in SQL is to eliminate redundant (repetitive) data and ensure data is stored logically.</p> <p>However, in most practical applications, normalization achieves its best in 3rd Normal Form (3NF).</p> <p>Quote</p> <p>Database normalization is the process of restructuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by Edgar F. Codd as an integral part of his relational model.</p> <p>Normalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints. It is accomplished by applying some formal rules either by a process of synthesis (creating a new database design) or decomposition (improving an existing database design).</p> <p>By Wiki Database Normalization</p> <p>Abstract</p> primary key <p>A single column that uniquely identifies the records of data in that table. It\u2019s a unique identifier such as an employee ID, student ID, voter\u2019s identification number (VIN), and so on.</p> foreign key <p>A field that relates to the primary key in another table. Unlike the primary key, they do not have to be unique. most often they are not. foreign keys can be null even though primary keys can not.</p> composite key <p>A primary key composed of multiple columns used to identify a record uniquely.</p> candidate key <p>A field that able to be primary key.</p> <p>Abstract</p> prime attribute <p>Attributes can be used to uniquely identify a tuple in the table because they have unique values. they also known as Key Attributes.</p> non-prime attribute <p>Attributes of the relation which does not exist in any of the possible candidate keys of the relation. They also known as Non-Key Attributes.</p> <p>Example</p> <pre><code>R = (H, I, J, K, L, M, N, O)\n\nF = {\n  L -&gt; MNO\n  HI -&gt; JKLMNO\n  J -&gt; KL\n  K -&gt; H\n}\n</code></pre> <p>This schema has three (candidate) keys:</p> <pre><code>HI\nIJ\nIK\n</code></pre> <p>While it is immediate to discover tha HI is a candidate key since it determines all the other attributes, you can see that this is true also for IJ and IK by calculating the closure of those attributes:</p> <pre><code>IJ+ = IJ\nIJ+ = IJKL     (by adding the right part of J \u2192 KL)\nIJ+ = IJKLH    (by adding the rigth part of K \u2192 H)\nIJ+ = IJKLHMNO (by adding the right part of L \u2192 MNO)\n</code></pre> <p>Note that it use the notation IJ+. This is called the closure, which means all the possible attributes that can be inferred from what we know in LHS.</p> <p>analogously for IK:</p> <pre><code>IK+ = IK\nIK+ = IKH      (by adding the rigth part of K \u2192 H)\nIK+ = IKHJLMNO (by adding the right part of HI \u2192 JKLMNO)\n</code></pre> <p>For this reason, the prime attributes of the relation are:</p> <pre><code>HIJK\n</code></pre> <p>while the non prime attributes are:</p> <pre><code>LMNO\n</code></pre> <p>Note that there are no other candidate keys since M, N and O appear only on the right side of functional dependencies, so that they cannot \u201ccontribute\u201d to any key. L appears both on a left part and on a right part of functional dependencies, but it is determined by HI, IJ and IK, and does not determine any of these attributes, so it cannot be part of a key. Finally, you cannot remove any attribute from HI, IJ and IK without losing the key property.</p> <p>Read more this example</p>"},{"location":"abstract/data_strategy/strategy-normalization-form/#what-is-anomalies","title":"What is Anomalies?","text":"<p>Anomalies are the problems that occur in the update, delete and insert operations in poorly designed or un-normalized data when all data stored in one table (Flat file).</p> <p>In simple words we can say anomaly is when you have one table which has multiple related information, and you cannot do any kind of operation on single information.</p> <p>Example of Anomalies:</p> <pre><code>R = (courseNo,  tutor,  lab, labSize){\n    (300,       Ahmed,  C3,  150),\n    (301,       John,   A1,  210),\n    (302,       Kamal,  C3,  150)\n}\n</code></pre> InsertDeleteUpdate <p>What if we built a new lab (e.g. <code>lab: A4</code>), but it\u2019s not assigned to any courses or tutors yet, so we won\u2019t be able to insert it to our table because (<code>lab</code>) comes with (<code>course</code>) and (<code>tutor</code>) because it does not have separated table.</p> <p>What if we need to delete (<code>courseNo: 300</code>) that means we will delete the details of (<code>lab: C3</code>) also and that\u2019s not what we want.</p> <p>What if we improved (<code>lab: C3</code>) and now it (<code>labSize: 250</code>), to update it we will have to update all other columns where (<code>lab: C3</code>).</p>"},{"location":"abstract/data_strategy/strategy-normalization-form/#dependency-rules","title":"Dependency Rules","text":"<p>Rule for call the attribute relationships</p>"},{"location":"abstract/data_strategy/strategy-normalization-form/#functional-dependency","title":"Functional Dependency","text":"<p>This is the primary key (single or composite) relationship for identified other attributes. When we say A is identified by B (B is a primary key), then A functionally dependent on B, can be represented graphically as (B -&gt; A)</p>"},{"location":"abstract/data_strategy/strategy-normalization-form/#completefully-functional-dependency","title":"Complete/Fully Functional Dependency","text":"<p>Example</p> <pre><code>R = (customerID, name, salary){\n    (1,          Tom,  18,000),\n    (2,          June, 22,000),\n    (3,          Rose, 15,000)\n}\n</code></pre> <p><code>customerID -&gt; {name, salary}</code> is Fully Functional Dependency (FFD).</p> <ul> <li><code>name</code> and <code>salary</code> were identified by <code>customerID</code> and functionally dependent on it.</li> <li><code>customerID</code> determines <code>name</code> and <code>salary</code>.</li> </ul>"},{"location":"abstract/data_strategy/strategy-normalization-form/#partial-dependency","title":"Partial Dependency","text":"<p>When only one of the prime attributes determines another attribute with no exist of other prime attributes in this relation. OR when not all non-prime attributes depend on all prime attributes.</p> <p>Example</p> <p>{A, B} are our prime attributes, C is non-prime attribute and A -&gt; Z not {A, B} -&gt; C, so it's partial dependency because C is functionally dependent on only one prime attribute not all prime attributes.</p> <pre><code>R = (order, product, productName, quantity){\n    (1,     A,       table,       1),\n    (1,     B,       chair,       4),\n    (2,     A,       table,       2)\n}\n</code></pre> <p><code>{order, product} -&gt; {productName, quantity}</code> is Fully Functional Dependency (FFD) because of using for all key.</p> <p><code>product -&gt; productName</code> is being Partial Dependency because use one key from prime attributes.</p>"},{"location":"abstract/data_strategy/strategy-normalization-form/#transitive-dependency","title":"Transitive Dependency","text":"<p>There is non-prime attribute functionally dependent on another non-prime attribute OR It means that changing a value in one column leads to a change in another column-columns other than prime attributes.</p> <p>A transitive functional dependency is when changing a non-key column, might cause any of the other non-key columns to change</p> <p>Note</p> <p>In transitive dependency non-prime attribute determines another non-prime attribute, In partial dependency when only one of the prime attributes determines another attribute with no exist of other prime attributes (because of that we called it partial dependency).</p> <p>Example</p> <p>We say that A -&gt; C is transitive dependency if it generated from A -&gt; B &amp; B -&gt; C not A -&gt; C directly.</p>"},{"location":"abstract/data_strategy/strategy-normalization-form/#multivalued-dependency","title":"Multivalued Dependency","text":"<p>It usually a relationship consisting of 3 attributes (A, B, C). single value from (A) gives more than one value in (B), single value of (A) gives more than one value in (C), and (B) , (C) are independent of each other.</p> <ul> <li>There are at least 3 attributes A, B, C in a relation and</li> <li>For each value of A there is a well-defined set of values   for B, and a well-defined set of values for C,</li> <li>But the set of values for B is independent on the set of   values for C</li> </ul> <p>Example</p> <p>(emp_no , proj_no, dependents)</p> <p>(employee) do have many (projects), (employee) do have many ( dependents ) like his children, and it\u2019s obviously projects and his dependents are independent of each other, which means if we need to remove one of his projects we don't have to delete one of his dependent. so we have here multivalued dependency which violates 4NF.</p>"},{"location":"abstract/data_strategy/strategy-normalization-form/#join-dependency","title":"Join Dependency","text":"<p>Whenever we can recreate a table by simply joining various tables where each of these tables consists of a subset of the table\u2019s attribute, then this table is known as a Join Dependency.</p>"},{"location":"abstract/data_strategy/strategy-normalization-form/#normalization-order","title":"Normalization Order","text":"<p>The normalization process takes our relational schema throw a series or pipeline of tests to make sure that\u2019s it satisfy a certain normal form, this process proceeds in a top-down manner by evaluating our relational schema against the criteria of normal forms.</p>"},{"location":"abstract/data_strategy/strategy-normalization-form/#1nf-first-normal-form","title":"1NF: First Normal Form","text":"<p>First Normal Form (1NF) is the first step towards a full normalization of your data, to apply 1NF it should have the following criteria:</p> <ul> <li> <p>Each column or single cell contains atomic values</p> </li> <li> <p>Each entity has a primary key</p> </li> <li> <p>No duplicated rows or columns</p> </li> </ul> <p>In other words, each row in the table should have a unique identifier, and each value in the table should be indivisible.</p> <p>Example</p> <pre><code>R = (customerID, Address){\n    (1,          Main street, First Town, BKK 10100),\n    (2,          12 street, A Town, BKK 10120)\n}\n</code></pre> <pre><code>R = (customerID, street,      town,       city, zipCode){\n    (1,          Main street, First Town, BKK,  10100),\n    (2,          12 street,   A Town,     BKK,  10120)\n}\n</code></pre>"},{"location":"abstract/data_strategy/strategy-normalization-form/#2nf-second-normal-form","title":"2NF: Second Normal Form","text":"<p>Second Normal Form (2NF) do more than the 1NF because 1NF only eliminates repeating groups, not redundancy.</p> <ul> <li> <p>It should be in 1NF.</p> </li> <li> <p>It should not have partial dependencies. Each non-prime attribute is full   functionally dependent on the whole primary key (all prime attributes).</p> </li> </ul> <p>Example</p> <pre><code>R = (studentID, studentName, subjectID, grade, prize){\n    (1,         Tom,         S01,       A,     Voucher),\n    (2,         Sara,        S01,       B+,    Nothing),\n    (3,         John,        S02,       A,     Voucher)\n}\n</code></pre> <pre><code>R = (studentID, subjectID, grade, prize){\n    (1,         S01,       A,     Voucher),\n    (2,         S01,       B+,    Nothing),\n    (3,         S02,       A,     Voucher)\n}\n\nRS = (studentID, studentName){\n     (1,         Tom),\n     (2,         Sara),\n     (3,         John)\n}\n</code></pre>"},{"location":"abstract/data_strategy/strategy-normalization-form/#3nf-third-normal-form","title":"3NF: Third Normal Form","text":"<p>Third Normal Form (3NF)</p> <ul> <li> <p>It should be in 2NF.</p> </li> <li> <p>It should not have transitive dependencies.</p> </li> </ul> <p>Example</p> <pre><code>R = (studentID, subjectID, grade, prize){\n    (1,         S01,       A,     Voucher),\n    (2,         S01,       B+,    Nothing),\n    (3,         S02,       A,     Voucher)\n}\n</code></pre> <pre><code>R = (studentID, subjectID, grade){\n    (1,         S01,       A),\n    (2,         S01,       B+),\n    (3,         S02,       A)\n}\n\nRP = (grade, prize){\n     (A,     Voucher),\n     (B+,    Nothing),\n}\n</code></pre>"},{"location":"abstract/data_strategy/strategy-normalization-form/#bcnf-boyce-codd-normal-form","title":"BCNF: Boyce-Codd Normal Form","text":"<p>Boyce-Codd Normal Form (BCNF) will remove all candidate key.</p> <ul> <li> <p>It should be in 3NF.</p> </li> <li> <p>Any attribute in table depends only on super-key.   A -&gt; Z means (A) is super-key of (Z) (even (Z) is a prime attribute)</p> </li> </ul> <p>Note</p> <ul> <li>If our table contains only one prime key, 3NF and BCNF are equivalent.</li> <li>BCNF is a special case of 3NF, and it also called 3.5NF.</li> </ul> <p>Example</p> <pre><code>R = (student, subject, professor){\n    (A,       Math,    P.Tom),\n    (A,       Science, P.Sara),\n    (B,       Math,    P.Kan),\n}\n</code></pre> <ul> <li><code>{student, subject} -&gt; professor</code>, so <code>student</code> and <code>subject</code> are super-key.</li> <li><code>professor -&gt; subject</code>, but subject is prime attribute and professor is not   super-key.</li> </ul> <pre><code>RS = (student, professorID){\n     (A,       1),\n     (A,       2),\n     (B,       3),\n}\n\nRP = (professorID, professor, subject){\n     (1,           P.Tom,     Math),\n     (2,           P.Sara,    Science),\n     (3,           P.Kan,     Math),\n}\n</code></pre>"},{"location":"abstract/data_strategy/strategy-normalization-form/#4nf-fourth-normal-form","title":"4NF: Fourth Normal Form","text":"<p>Fourth Normal Form (4NF)</p> <ul> <li> <p>It should be in BCNF.</p> </li> <li> <p>It should not have multivalued dependencies.</p> </li> </ul> <p>If no database table instance contains two or more, independent and multivalued data describing the relevant entity, then it is in 4th Normal Form.</p> <p>Example</p> <pre><code>R = (projectID, businessUnit, Location){\n    (A01,       BU01,         North),\n    (A01,       BU01,         South),\n    (A01,       BU02,         North),\n    (A01,       BU02,         South),\n}\n</code></pre> <pre><code>R1 = (projectID, businessUnit){\n     (A01,       BU01),\n     (A01,       BU02)\n}\n\nR2 = (projectID, Location){\n     (A01,       North),\n     (A01,       South)\n}\n</code></pre>"},{"location":"abstract/data_strategy/strategy-normalization-form/#5nf-fifth-normal-form","title":"5NF: Fifth Normal Form","text":"<p>Fifth Normal Form (5NF) is also known as Project-Join Normal Form (PJNF). It is used to handle complex many-to-many relationships in a database.</p> <ul> <li>It should not have join dependency, it cannot be decomposed into any number of   smaller tables without loss of data.</li> </ul> <p>In a many-to-many relationship, where each table has a composite primary key, it is possible for a non-trivial functional dependency to exist between the primary key and a non-key attribute. 5NF deals with these situations by decomposing the tables into smaller tables that preserve the relationships between the attributes.</p> <p>Example</p> <pre><code>R = (year,   subjectID, buildingID){\n    (1/2560, EE400,     A012),\n    (1/2560, EE402,     B012),\n    (2/2560, EE400,     B012),\n    (1/2560, EE400,     B012)\n}\n</code></pre> <pre><code>R1 = (year,   subjectID){\n     (1/2560, EE400),\n     (1/2560, EE402),\n     (2/2560, EE400)\n}\n\nR2 = (subjectID, buildingID){\n     (EE400,     A012),\n     (EE400,     B012),\n     (EE402,     B012),\n}\n\nR3 = (year,   buildingID){\n     (1/2560, A012),\n     (1/2560, B012),\n     (2/2560, B012)\n}\n</code></pre> <pre><code>R != R1 join R2 join R3\n</code></pre>"},{"location":"abstract/data_strategy/strategy-normalization-form/#6nf-sixth-normal-form","title":"6NF: Sixth Normal Form","text":"<p>Sixth Normal Form (6NF) (Proposed)</p> <ul> <li>The row contains the primary key, and at most one other attribute</li> </ul> <p>Warning</p> <p>The obvious drawback of 6NF is the proliferation of tables required to represent the information on a single entity. If a table in 5NF has one primary key column and N attributes, representing the same information in 6NF will require N tables; multi-field updates to a single conceptual record will require updates to multiple tables; and inserts and deletes will similarly require operations across multiple tables.</p>"},{"location":"abstract/data_strategy/strategy-normalization-form/#why-do-we-need-to-normalize-our-tables","title":"Why do we need to normalize our tables?","text":"<ul> <li>When (ACID compliant) is required</li> </ul> <p>It improves integrity and consistency of your data.   (ACID = Atomicity Consistency Isolation Durability)</p> <ul> <li>Fewer storage needed</li> </ul> <p>Since we eliminated repeated groups, and divided our tables, we reduced the   size of our tables and database.</p> <ul> <li>less logical I/O cost</li> </ul> <p>When you need to retrieve data, you will retrieve smaller amount of data, and   when you need to add or insert in tables, it will be easier, and more organized.</p> <ul> <li>Queries become easier</li> </ul> <p>If we have un-normalized table that has (location) attribute {City, Zip} as   composite attribute, and we need to count the unique zip codes in our table,   so we will access first location, then we will try to get zip, after normalize   this table we will be able to access zip directly because location will be   divided to two attributes (city) and (zip).</p> <ul> <li>Write-intensive databases</li> </ul> <p>Normalization increases the performance of write-intensive databases Significantly,   because it reduces data modification anomalies, which make it easier to manipulate   your database.</p>"},{"location":"abstract/data_strategy/strategy-normalization-form/#drawbacks-of-normalization","title":"Drawbacks of Normalization","text":"<p>When we need to work with read-intensive databases, you may need to join data from multiple tables, and work with a huge amount of data. In normalized databases, you will need many join operations to combine data from multiple tables to satisfy user needs, which will increase time-consuming, and make it difficult to work with a huge amount of data, so if you need to work with the read-intensive database it's obviously normalization won't be your optimal solution.</p> <p>The drawbacks of data redundancy include:</p> <ul> <li>Data maintenance becomes tedious \u2013 data deletion and data updates become problematic</li> <li>It creates data inconsistencies</li> <li>Insert, Update and Delete anomalies become frequent. An update anomaly, for example, means that the versions of the same record, duplicated in different places in the database, will all need to be updated to keep the record consistent</li> <li>Redundant data inflates the size of a database and takes up an inordinate amount of space on disk</li> </ul>"},{"location":"abstract/data_strategy/strategy-normalization-form/#references","title":"References","text":"<ul> <li>https://en.wikipedia.org/wiki/Third_normal_form/</li> <li>https://en.wikipedia.org/wiki/Database_normalization/</li> <li>database normalization</li> <li>table normalization</li> <li>https://datavalley.technology/normalization-in-depth/</li> <li>https://medium.com/swlh/a-complete-database-normalization-tutorial-732df3748d0e</li> </ul>"},{"location":"abstract/data_strategy/strategy-scd/","title":"Slowly Changing Dimension (SCD)","text":"<ul> <li>https://towardsdatascience.com/unlocking-the-secrets-of-slowly-changing-dimension-scd-a-comprehensive-view-of-8-types-a5ea052e4b36</li> <li>Navigating Slowly Changing Dimensions (SCD) and Data Restatement: A Comprehensive Guide</li> </ul>"},{"location":"abstract/data_strategy/strategy-semantic-layer/","title":"Semantic Layer","text":"<p>One of the key components of modern data warehouses is the \"democratization\" of data. Rather than have a centralized data team curate and publish reports &amp; dashboards, organizations have found immense value from creating self-serve reports &amp; dashboards that users across an organization can query and access the data they need to make data-driven decisions.</p> <p>Below is a typical architecture for a modern data stack using a medallion data warehouse architecture</p> <p></p>"},{"location":"abstract/data_strategy/strategy-semantic-layer/#pain-points-with-the-modern-data-stack-mds","title":"Pain points with the Modern Data Stack (MDS):","text":"<p>However, many organizations have found that in trying to democratize their data, it has caused another set of data problems. Many end users find that the data they need is NOT in the correct format for a visualization or they need to make further calculations to get the results they need to create a report or dashboard.</p> <p>Moreover, analysts may have different names for essentially the same data record. The marketing team, for example, may refer to a user as a \u201cprospect,\u201d while the sales team might call that same business a \u201cclient,\u201d while the finance team calls this user entity a \u201ccounter party.\u201d However, a machine learning model or analytics team might want to analyze all this data and relate the data back to a single user.</p> <p>This often leads analysts to create their own metrics within their local environment or business intelligence tool.</p> <p></p> <p>However, this can create major problems:</p> <ul> <li>localized metrics have no oversight and can be inaccurate.</li> <li>changes in data structure upstream can cause these metrics to break.</li> <li>the work required to calculate metrics is duplicated.</li> <li>metrics created in one BI tool cannot be integrated with other BI tools or used by the data scientist or data engineering team for automation.</li> </ul>"},{"location":"abstract/data_strategy/strategy-semantic-layer/#what-is-a-semantic-layer","title":"What is a Semantic layer?","text":"<p>Quote</p> <p>A semantic layer is a business representation of data and offers a unified and consolidated view of data across an organization. Its important to note that the semantic layer does not hold or store the actual data, it is a metadata and abstraction layer built on the source data.<sup>1</sup></p> <p>This abstraction over the data warehouse maps different data definitions from various data sources into a unified, consistent, and single view of data for analytics and other business purposes. Semantic layers create pre-defined views of processed data that abstract complexity and apply business-oriented definitions. Metrics like revenue or cost are defined.</p> <p>Hence, the need for centralized metrics to give users a single source of truth and semantic layer. Organizations can codify their metrics in a centralized place and have confidence that they\u2019re getting the same number and context around that number wherever they consume data.</p> <p></p> <p>Benefits of the dbt\u2019s semantic layer include:</p> <ul> <li>centralized metrics defined once and used across the organization ensuring accuracy, clarity, and molecularity.</li> <li>metrics can be used in downstream applications including machine learning models, automation tools, reverse-etl tools, and even spreadsheets.</li> <li>increased ease in calculating metrics (dbt uses YAML files).</li> <li>automated documentation to provide metric definition and context.</li> </ul>"},{"location":"abstract/data_strategy/strategy-semantic-layer/#when-not-to-use-a-semantic-layer","title":"When NOT to use a Semantic Layer:","text":"<p>Although the semantic layer can help data teams creating functional self-serving reports and dashboards, it is really only needed when the data infrastructure is relatively mature and there are lots of data users using multiple BI tools. The data warehouse used to build metrics must be relatively clean and formatted correctly into appropriate dimension and fact tables.</p> <p>If you are a data engineer in a startup, building a semantic layer may not be the best use of your time since building a data infrastructure to support a semantic layer will require significant effort.</p> <p>However, the semantic layer can be a great value add for organizations using dbt that want to standardize metric definitions. If you would like to delve deeper into dbt\u2019s semantic layer, please check out their blog. In my next blog post, I will show how you can use the dbt semantics layer in practice in dbt-Cloud.</p>"},{"location":"abstract/data_strategy/strategy-semantic-layer/#conclusion","title":"Conclusion","text":"<p>In conclusion, a semantic layer is an innovative approach to solving the metric decentralization problem associated with self-serving reporting &amp; dashboards. It provides a centralized control mechanism to ensure metrics calculations are consistent &amp; accurate while allowing users the flexibility to use data to improve decision making in their day-to-day operations.</p> <ol> <li> <p>Why data teams need a semantic layer? \u21a9</p> </li> </ol>"},{"location":"abstract/data_strategy/strategy-sensitive-data/","title":"Sensitive Data","text":"<ul> <li>Data Engineering: Architectures &amp; Strategies for Handling Sensitive Data</li> </ul>"},{"location":"abstract/dataops/","title":"Data Ops","text":"<p>Note</p> <p>DataOps is a collection of practices that focuses on breaking down silos between data producers and consumers, improving data quality and transparency of results.</p> <p>DataOps provides a collaboration of data engineering, data science and operations team. It aims to automate the delivery of the right and reliable data to appropriate teams through a much faster approach. And this leads to better data productivity and enhanced human communication.</p> DataOps"},{"location":"abstract/dataops/#noted","title":"Noted","text":"<ul> <li>Continuous Integration/Continuous Deployment for Data</li> <li>Automated Testing for Data Pipelines</li> <li>Data Observability</li> <li>Incident Management for Data Systems</li> </ul> <ul> <li>Data Pipeline Orchestration</li> <li>Continuous Integration/Continuous Deployment (CI/CD) for Data</li> <li>Data Observability</li> <li>Automated Data Quality Checks</li> </ul>"},{"location":"abstract/dataops/#dataops-engineer","title":"DataOps Engineer","text":"<ul> <li> 10 New DevOps Tools to Watch in 2024</li> <li> Is Data Observability Critical to Successful Data Analytics?</li> </ul>"},{"location":"abstract/dataops/#examples","title":"Examples","text":"<ul> <li> DataOps for the Modern Data Warehouse</li> </ul>"},{"location":"abstract/dataops/#references","title":"References","text":"<ul> <li> Data Engineering concepts: Part 7, DevOps, DataOps and MLOps</li> </ul>"},{"location":"abstract/emerging_trends/","title":"Emerging Trends","text":"<ul> <li>Edge Computing for Data</li> <li>Quantum Data Processing</li> <li>Blockchain in Data Management</li> <li>Artificial Intelligence in Data Engineering</li> </ul>"},{"location":"abstract/mlops/","title":"ML Ops","text":"<ul> <li>Data Preparation for Machine Learning</li> <li>Feature Engineering</li> <li>Data Versioning for ML</li> <li>Serving ML Models in Production</li> </ul>"},{"location":"abstract/mlops/#what-is-mlops","title":"What is MLOps?","text":"<p>What is MLOps?</p> <p>MLOps = DevOps + DataOps + ModelOps<sup>1</sup></p> <p>Machine Learning Operations (MLOps) is a set of practices that includes Machine Learning, DevOps and Data Engineering elements. The main aim of the union is reliable and efficient deployment and maintenance of Machine Learning systems in production.</p> Basic Model Training Flow <p>Features of MLOps:</p> <ul> <li>Ensures validation of data, data schemas, and models along with testing and     validation of code</li> <li>Facilitates automated deployment of a ML pipeline that should automatically     deploy a model and corresponding prediction service.</li> <li>Expedites the process of automatically re-training and serving the models.</li> </ul> MLOps Relation Overview"},{"location":"abstract/mlops/#components","title":"Components","text":"MLOps Components"},{"location":"abstract/mlops/#workflow","title":"Workflow","text":"MLOps High-Level Workflow Architecture[^1] <p>The following table summarizes MLOps main practices and how they relate to DevOps and Data Engineering practices:</p> MLOps Main Practices"},{"location":"abstract/mlops/#what-is-mlops-engineer","title":"What is MLOps Engineer?","text":"<p>Quote</p> <p>ML Engineers build and retrain machine learning models. MLOps Engineers enable the ML Engineers. MLOps Engineers build and maintain a platform to enable the development and deployment of machine learning models. They typically do that through standardization, automation, and monitoring. MLOps Engineers reiterate the platform and processes to make the machine learning model development and deployment quicker, more reliable, reproducible, and efficient.</p> <p>However, ML Engineers focus on building, training and validating machine learning models, while MLOps Engineers concentrate primarily on testing, deploying and monitoring models in production environments.</p>"},{"location":"abstract/mlops/#read-mores","title":"Read Mores","text":"<ul> <li> MLOps - What, Why and How</li> <li> MLOps - Benefits That Make It An Upcoming Industry Trend</li> <li> Orchestrate MLOps by using Azure Databricks</li> <li> Architecting MLOps on the Lakehouse</li> <li> MLOps - workflows on Databricks</li> </ul> <ol> <li> <p> Architecting MLOps on the Lakehouse \u21a9</p> </li> </ol>"},{"location":"abstract/mlops/mlops-challenge/","title":"Challenge","text":"<p>Data engineering and machine learning pipelines are both very different but oddly can feel very similar. Many ML engineers I have talked to in the past rely on tools like Airflow to deploy their batch models.</p>"},{"location":"abstract/mlops/mlops-challenge/#data-engineering-pipelines","title":"Data Engineering Pipelines","text":"<p>First, let\u2019s dive into data pipelines. Data pipelines are a series of processes that extract data from different sources, clean it, and then store it in a data warehouse or database. This is important for companies because it helps them make informed decisions. A data pipeline is made up of four parts:</p> <p></p>"},{"location":"abstract/mlops/mlops-challenge/#machine-learning-pipelines","title":"Machine Learning Pipelines","text":"<p>Machine Learning (ML) pipelines don\u2019t work in a straight line like data pipelines. Instead, they involve building, training, and deploying ML models. This is used to automate the entire process of building an ML model, from collecting data to deploying it in production. An ML pipeline is made up of five parts:</p>"},{"location":"abstract/mlops/mlops-challenge/#references","title":"References","text":"<ul> <li>Data Engineering Vs Machine Learning Pipelines</li> </ul>"},{"location":"blogs/","title":"Data Blogs","text":"<p>This  Blog Site will be sharing data knowledge that not included inData Engineering Abstraction.</p>"},{"location":"roles/lead_data_engineer/","title":"Lead Data Engineer","text":"<ol> <li> <p>https://towardsdatascience.com/lead-data-engineer-career-guide-699e806111b4\u00a0\u21a9</p> </li> </ol>"},{"location":"services/","title":"Services","text":"<p>Service and Cloud Provider that use on Modern Data Stack (MDS). You can see the below diagram show a MDS Ecosystem that include the Cloud Services more than the Open-Source Stacks.</p> Modern Data Stack Ecosystem <p> As such, the definition of a MDS cannot be clearly stated since every business tries to adapt modern technologies to their requirements.</p> <p>However, there are definite features of the modern data stack that identify it:</p> <ul> <li>It\u2019s Cloud-Based, requires very little maintenance, is easy to install,   and can scale quickly with little effort.</li> <li>It can be used by Small and Medium-Sized Data Teams, as it has a lot of   out-of-the-box functionality and doesn't rely on the number of data professionals.</li> <li>It offers a Lot of Integration Opportunities for creating a comprehensive   data ecosystem.</li> </ul> <p>Overall, the MDS centerpiece is about democratizing data usage: Making data more accessible, covering different dimensions of business, improving analytics capabilities, and simplifying the infrastructure.</p> Modern Data Stack Architecture <p> If the advantages of MDS do not seem convincing enough, let's have a look at how it differs from the Traditional Data Stack.</p> Traditional Data Stack Modern Data Stack Has coupled structure Has modular structure Complex setup requiring large IT teams Less time on technical configuration Requires serious technical background Suitable for users without extensive technical background Contains traditional RDBMS Works with RDBMS as well as big data, unstructured data <p>The Modern Data Stack provides businesses with a bias for action. Creating the MDS enables organizations to devote more time to analyzing their data and less time engineering their data processing pipelines.</p>"},{"location":"services/#providers-comparison","title":"Providers Comparison","text":"<ul> <li> Compare Clouds</li> <li>Databricks vs Snowflake: A Complete 2024 Comparison</li> <li>Pulumi vs Terraform: The Definitive Guide to Choosing Your IaC Tool</li> </ul>"},{"location":"services/#finops","title":"FinOps","text":"<ul> <li>Proven in Production: A Cost-Effective Modern Data Architecture for Small and Medium Enterprises</li> </ul>"},{"location":"services/ansible/","title":"Ansible","text":""},{"location":"services/ansible/#read-mores","title":"Read Mores","text":""},{"location":"services/aws/","title":"AWS","text":"<p>Amazon Web Services (AWS) is a secure cloud services platform, offering compute power, database storage, content delivery and other functionality to help businesses scale and grow.</p>"},{"location":"services/aws/#read-mores","title":"Read Mores","text":"<ul> <li>AWS Data Analytics and Ingestion Services\u2014 (SAA-CO3 Summary 2024)</li> </ul>"},{"location":"services/aws/aws-policy/","title":"Policies","text":"<p>https://docs.aws.amazon.com/mediaconnect/latest/ug/iam-policy-examples-asm-secrets.html</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetResourcePolicy\",\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecretVersionIds\"\n            ],\n            \"Resource\": [\n                \"arn:aws:secretsmanager:us-west-2:111122223333:secret:aes128-1a2b3c\",\n                \"arn:aws:secretsmanager:us-west-2:111122223333:secret:aes192-4D5e6F\",\n                \"arn:aws:secretsmanager:us-west-2:111122223333:secret:aes256-7g8H9i\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"secretsmanager:ListSecrets\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"services/aws/aws-vpc/","title":"VPC","text":"<p>Virtual Private Cloud (VPC) is service to manage all networks in AWS in virtual, that mean you do not need to do anything on the physical layer.</p> <ul> <li>https://jumpbox.medium.com/aws-vpc-%E0%B8%84%E0%B8%B7%E0%B8%AD%E0%B8%AD%E0%B8%B0%E0%B9%84%E0%B8%A3-%E0%B8%A1%E0%B8%B2%E0%B8%97%E0%B8%B3%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%88%E0%B8%B1%E0%B8%81%E0%B8%81%E0%B8%B1%E0%B8%99-ep-1-588c5a2a0d91</li> </ul>"},{"location":"services/aws/athena/athena-with-delta-lake/","title":"AWS Athena: With DeltaLake","text":"<p>https://medium.com/@gauravthalpati/athena-spark-with-delta-lake-f8cb71616c64</p>"},{"location":"services/aws/ec2/ec2-domain-with-route53/","title":"EC2: Connect Domain Name from Route53","text":"<p>https://medium.com/@arif.w/aws-%E0%B9%80%E0%B8%8A%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%A1%E0%B8%95%E0%B9%88%E0%B8%AD-domain-name-%E0%B9%80%E0%B8%82%E0%B9%89%E0%B8%B2%E0%B8%81%E0%B8%B1%E0%B8%9A-ec2-instance-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-route-53-a73b90dc5864</p>"},{"location":"services/aws/ecs/ecs-deploy-with-fargate/","title":"Deploy with Fargate","text":"<p>ETL stands for Extract, Transform, and Load. An ETL pipeline is essentially just a data transformation process \u2014 extracting data from one place, doing something with it, and then loading it back to the same or a different place.</p>"},{"location":"services/aws/ecs/ecs-deploy-with-fargate/#references","title":"References","text":"<ul> <li>Deploy Long-Running ETL Pipelines to ECS with Fargate</li> </ul>"},{"location":"services/aws/emr/emr-compare-with-databricks/","title":"Elastic MapReduce: Compare with Databricks","text":"<ul> <li>https://medium.com/insiderengineering/benchmarking-amazon-emr-vs-databricks-4c2f7d209d3d</li> </ul>"},{"location":"services/aws/glue/","title":"AWS Glue","text":""},{"location":"services/aws/glue/#getting-started","title":"Getting Started","text":"<p>Let's start by briefly introducing the key concepts we'll cover:</p> <ul> <li>AWS Glue Data Catalog: A centralized metadata repository that stores metadata about data sources, transformations, and targets.</li> <li>AWS Glue Database: A logical container that organizes tables, allowing for better data management.</li> <li>AWS Glue Tables: The structure that represents data in the AWS Glue Data Catalog.</li> <li>Partition in AWS: A way to organize data within a table based on the values of one or more columns.</li> <li>AWS Glue Crawlers: Tools that scan various data stores, extract metadata, and create table definitions.</li> <li>AWS Glue Connection: A resource that contains the properties needed to connect to your source or target data store.</li> <li>AWS Glue Jobs: An ETL process that extracts data from the source, transforms it and loads it into the target.</li> <li>AWS Glue Triggers: Events or conditions that can automatically invoke AWS Glue workflows.</li> <li>AWS Glue Endpoints: URLs that allow external systems to call AWS Glue API operations.</li> </ul>"},{"location":"services/aws/glue/#optimization","title":"Optimization","text":"<p>https://levelup.gitconnected.com/optimizing-and-reducing-aws-glue-costs-e7426fa732af</p>"},{"location":"services/aws/glue/#read-mores","title":"Read Mores","text":"<ul> <li>A Guide to AWS Glue: Data Catalog, Databases, Crawler, Triggers, with S3</li> </ul>"},{"location":"services/aws/glue/glue-data-quality/","title":"Glue Data Quality","text":"<p>Data Quality is fundamental for a variety of reasons, spanning across business, science, government, and numerous other sectors. There are many reasons why it is essential to maintain high data quality, including:</p> <ul> <li>Conveying business decisions: business decisions must be based on accurate and reliable data. Low quality data could lead to incorrect decisions that negatively impact business operations.</li> <li>Precise analyses: data analysis is a fundamental part of many business activities. Low-quality data could lead to inaccurate results and misinterpretations.</li> <li>Regulatory compliance: many companies are subject to strict regulations on data management. Lack of data quality could lead to regulatory violations and financial penalties.</li> <li>Time savings and efficiency: high-quality data simplify business processes. Cleaning and correcting data takes significant time and effort. High-quality data therefore reduce the need for such activities.</li> <li>Customer satisfaction: data quality directly affects customer satisfaction. Incorrect data can lead to errors in customer reports and communications.</li> </ul>"},{"location":"services/aws/glue/glue-data-quality/#what-is-aws-glue-data-quality","title":"What is AWS Glue Data Quality?","text":"<p>AWS Glue Data Quality is a feature of AWS Glue, Amazon\u2019s fully managed extract, transform, and load (ETL) service. This feature provides users with the ability to validate and monitor the quality of data sources, making it easier to maintain high-quality data for analytics and machine learning applications.</p> <p>Below are the main features of Glue Data Quality.</p>"},{"location":"services/aws/glue/glue-data-quality/#automatic-recommendations-of-custom-rules-for-your-data","title":"Automatic recommendations of custom rules for your data","text":"<p>...</p>"},{"location":"services/aws/glue/glue-data-quality/#references","title":"References","text":"<ul> <li>AWS Glue Data Quality: the ultimate guide to turning data into reliable decisions</li> </ul>"},{"location":"services/aws/glue/glue-local-env/","title":"Glue: Setup Local Environment","text":"<p>https://medium.com/@datasanity/setting-up-a-local-environment-for-aws-glue-development-bdb8ca74e608</p>"},{"location":"services/aws/glue/glue-terraform/","title":"Terraform","text":""},{"location":"services/aws/glue/glue-terraform/#references","title":"References","text":"<ul> <li>How I build an ETL pipeline with AWS Glue, Lambda, and Terraform</li> </ul>"},{"location":"services/aws/glue/glue-with-iceberg/","title":"With Iceberg","text":""},{"location":"services/aws/glue/glue-with-iceberg/#getting-started","title":"Getting Started","text":""},{"location":"services/aws/glue/glue-with-iceberg/#define-the-important-libraries","title":"Define the important libraries","text":"<pre><code>import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\ncatalog_nm = \"glue_catalog\"\n\n# The Glue Database Name which has the source table\nin_database=\"&lt;glue-database-input&gt;\"\n\n# The input Glue Table which we will be using as a source for the Iceberg data\nin_table_name=\"covid_19_data\"\n\n# The Glue Database Name which will be used to create an output Iceberg table\ndatabase_op='database_ib'\n\n# The Glue Table Name which will be used as a destination for Iceberg table\ntable_op='covid_dataset_iceberg'\n\n# The S3 path which will be used to store the Iceberg files as output\ns3_output_path='s3://&lt;your-destination-bucket-name&gt;/iceberg-output/'\n\ntable = str(catalog_nm)+ '.`' + str(database_op) + '`.' + str(table_op)\n\nprint(\"\\nINPUT Database : \" + str(in_database))\nprint(\"\\nINPUT Table : \" + str(in_table_name))\nprint(\"\\nOUTPUT IceBerg Database : \" + str(database_op))\nprint(\"\\nOUTPUT IceBerg Table : \" + str(table))\nprint(\"\\nOUTPUT IceBerg S3 Path : \" + str(s3_output_path))\n</code></pre> <p>In line with the script we need to define a important job parameter in the glue which will indicate the Glue job executer to leverage the Iceberg table format as output for the data. For this you need to define a parameter named as</p> <pre><code>--datalake-formats : iceberg\n</code></pre> <p></p>"},{"location":"services/aws/glue/glue-with-iceberg/#define-spark-and-glue-context","title":"Define Spark and Glue context","text":"<pre><code>def create_spark_iceberg(catalog_nm: str = \"glue_catalog\"):\n    \"\"\"\n    Function to initialize a session with iceberg by default\n    :param catalog_nm:\n    :return spark:\n    \"\"\"\n    from pyspark.sql import SparkSession\n    # You can set this as a variable if required\n    warehouse_path = s3_output_path\n\n    spark = (\n        SparkSession.builder\n            .config(f\"spark.sql.catalog.{catalog_nm}\", \"org.apache.iceberg.spark.SparkCatalog\")\n            .config(f\"spark.sql.catalog.{catalog_nm}.warehouse\", warehouse_path)\n            .config(f\"spark.sql.catalog.{catalog_nm}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n            .config(f\"spark.sql.catalog.{catalog_nm}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n            .config(f\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n            .getOrCreate()\n    )\n    return spark\n\nibspark = create_spark_iceberg(catalog_nm)\nibsc = ibspark.sparkContext\nibglueContext = GlueContext(ibsc)\nibjob = Job(ibglueContext)\nibjob.init(args[\"JOB_NAME\"], args)\n</code></pre>"},{"location":"services/aws/glue/glue-with-iceberg/#read-the-source-glue-table-and-write-into-a-destination-glue","title":"Read the source Glue table and write into a destination Glue","text":"<pre><code>#Read the Glue inout table from thr Catalog using a Glue DynamicFrame\nInputDynamicFrameTable = (\n    ibglueContext.create_dynamic_frame\n        .from_catalog(database=in_database, table_name=in_table_name)\n)\n\n#Convert the Glue DynamicFrame into a Spark DataFrame\nInputDynamicFrameTable_DF = InputDynamicFrameTable.toDF()\n\n#Register the Spark DataFrame as TempView\nInputDynamicFrameTable_DF.createOrReplaceTempView(\"InputDataFrameTable\")\nibspark.sql(\"select * from InputDataFrameTable LIMIT 10\").show()\n\n#Filter the source table with country as 'Australia'\ncolname_df = ibspark.sql(\"SELECT * FROM InputDataFrameTable WHERE country='Australia'\")\ncolname_df.createOrReplaceTempView(\"OutputDataFrameTable\")\n\n#Write the filtered Data into an ICEBERG table format in Glue destination table\nib_Write_SQL = f\"\"\"\n    CREATE OR REPLACE TABLE {catalog_nm}.{database_op}.{table_op}\n    USING iceberg\n    TBLPROPERTIES (\"format-version\"=\"2\", \"write_compression\"=\"gzip\")\n    AS SELECT * FROM OutputDataFrameTable;\n    \"\"\"\n\n#Run the Spark SQL query\nibspark.sql(ib_Write_SQL)\n</code></pre>"},{"location":"services/aws/glue/glue-with-iceberg/#references","title":"References","text":"<ul> <li>https://rajaswalavalkar.medium.com/deploy-apache-iceberg-data-lake-on-amazon-s3-using-aws-glue-spark-job-e14feb38e048</li> </ul>"},{"location":"services/aws/iot_core/iot-rule-timestream-grafana/","title":"AWS IoT: Timestream and Grafana","text":"<ul> <li>https://www.youtube.com/watch?v=z8T4hAERuOg</li> </ul>"},{"location":"services/aws/iot_core/iot-rule-to-kinesis/","title":"AWS IoT Core: Rule to S3 via Kinesis","text":""},{"location":"services/aws/iot_core/iot-rule-to-kinesis/#references","title":"References","text":"<ul> <li>https://www.thingrex.com/iot_ingest_cost/</li> <li>https://www.cloudthat.com/resources/blog/a-guide-to-ingest-iot-data-to-kinesis-data-firehose-and-store-in-s3</li> </ul>"},{"location":"services/aws/iot_core/iot-rule-to-s3/","title":"AWS IoT Core: Rule","text":""},{"location":"services/aws/iot_core/iot-rule-to-s3/#set-up-aws-iot","title":"Set up AWS IoT","text":"<ol> <li> <p>Set up AWS IoT Core</p> <ul> <li>Go to AWS IoT Core  Manage    Thing types  Click <code>Create thing type</code></li> <li>On Thing groups  Click <code>Create thing group</code></li> <li> <p>On Security  Policies    Click <code>Create policy</code></p> </li> <li> <p>Create Connect Policy</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"iot:Connect\"],\n      \"Resource\": [\n        \"arn:aws:iot:&lt;region&gt;:&lt;aws_account_id&gt;:client/${iot:Connection.Thing.ThingName}\"\n      ]\n    }\n  ]\n}\n</code></pre> </li> </ul> </li> </ol>"},{"location":"services/aws/iot_core/iot-rule-to-s3/#references","title":"References","text":"<ul> <li>https://docs.aws.amazon.com/iot/latest/developerguide/iot-moisture-tutorial.html</li> <li>https://www.cloudthat.com/resources/blog/step-by-step-guide-to-register-a-new-iot-device-in-aws-cloud?utm_source=blog-website&amp;utm-medium=text-link&amp;utm_campaign=%2Fstep-by-step-guide-to-register-a-new-iot-device-in-aws-cloud%2F</li> <li>https://www.cloudthat.com/resources/blog/step-by-step-guide-to-store-aws-iot-data-into-s3-bucket</li> <li>https://fanchenbao.medium.com/integrate-iot-device-with-aws-iot-using-python-part-i-upload-data-to-mqtt-topic-3f2b30ec6a6</li> <li> <p>https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/cost-effectively-ingest-iot-data-directly-into-amazon-s3-using-aws-iot-greengrass.html#cost-effectively-ingest-iot-data-directly-into-amazon-s3-using-aws-iot-greengrass-best-practices</p> </li> <li> <p>https://docs.aws.amazon.com/solutions/latest/constructs/aws-iot-s3.html</p> </li> <li> <p>https://github.com/awslabs/aws-solutions-constructs/tree/main/source/patterns/@aws-solutions-constructs/aws-iot-s3</p> </li> <li> <p>https://www.youtube.com/watch?v=SN4_2ua6_Ko</p> </li> <li>https://www.youtube.com/watch?v=kGqbqYMjAIE</li> </ul>"},{"location":"services/aws/kinesis/kinesis-data-firehose/","title":"Kinesis Data Firehose","text":"<p>Kinesis Data Firehose (Amazon Data Firehose) is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Amazon OpenSearch Serverless, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, Coralogix, and Elastic. With Amazon Data Firehose, you don't need to write applications or manage resources. You configure your data producers to send data to Amazon Data Firehose, and it automatically delivers the data to the destination that you specified. You can also configure Amazon Data Firehose to transform your data before delivering it.</p>"},{"location":"services/aws/kinesis/kinesis-data-firehose/#references","title":"References","text":"<ul> <li> What Is Amazon Data Firehose?</li> </ul>"},{"location":"services/aws/kinesis/kinesis-data-streams/","title":"Kinesis Data Streams","text":"<p>Kinesis Data Stream is a set of shards. Each shard has a sequence of data records. Each data record has a sequence number that is assigned by Kinesis Data Streams.</p> <p></p> Shard <p>A shard is a uniquely identified sequence of data records in a stream. A stream is composed of one or more shards, each of which provides a fixed unit of capacity. Each shard can support up to 5 transactions per second for reads, up to a maximum total data read rate of 2 MB per second and up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second (including partition keys). The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards.</p> Partition Key <p>A partition key is used to group data by shard within a stream. Kinesis Data Streams segregates the data records belonging to a stream into multiple shards. It uses the partition key that is associated with each data record to determine which shard a given data record belongs to. Partition keys are Unicode strings, with a maximum length limit of 256 characters for each key. An MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards using the hash key ranges of the shards. When an application puts data into a stream, it must specify a partition key.</p> <p>Note</p> <p>Sequence numbers cannot be used as indexes to sets of data within the same stream. To logically separate sets of data, use partition keys or create a separate stream for each dataset.</p>"},{"location":"services/aws/kinesis/kinesis-data-streams/#references","title":"References","text":"<ul> <li> What Is Amazon Kinesis Data Streams?</li> <li> Streaming in Data Engineering</li> </ul>"},{"location":"services/aws/lambda/lambda-cicd/","title":"CICD","text":""},{"location":"services/aws/lambda/lambda-cicd/#references","title":"References","text":"<ul> <li>CI/CD and AWS Lambda : A primer</li> </ul>"},{"location":"services/aws/lambda/lambda-stop-using-for-everything/","title":"AWS Lambda: Stop Using for Everything","text":"<p>https://medium.com/@pooyan_razian/stop-using-aws-lambda-for-everything-9d9b2d3a9763</p>"},{"location":"services/aws/lambda/lambda-with-docker/","title":"AWS Lambda: With Docker","text":"<p>https://towardsdatascience.com/using-poetry-and-docker-to-package-your-model-for-aws-lambda-cd6d448eb88f</p>"},{"location":"services/aws/s3/s3-filter-content/","title":"S3: Filter Content","text":"<p>https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html</p>"},{"location":"services/aws/s3/s3-transform-lambda/","title":"S3: Transform Lambda","text":"<p>https://eoins.medium.com/using-s3-object-lambdas-to-generate-and-transform-on-the-fly-874b0f27fb84</p>"},{"location":"services/aws/s3/s3-trigger-lambda/","title":"S3: Trigger Lambda","text":""},{"location":"services/aws/s3/s3-trigger-lambda/#references","title":"References","text":"<ul> <li>Comparing 2 Ways to Trigger Lambda from S3</li> </ul>"},{"location":"services/aws/secrets/secrets-across-account/","title":"AWS Secrets","text":"<p>https://itarunkumar.medium.com/securely-access-aws-secrets-manager-across-different-aws-accounts-3b0821f4e2e1</p>"},{"location":"services/aws/step_functions/stfn-combine-data-parallel-state/","title":"Combine Data from Parallel State","text":"<p>Note</p> <p>The elements of the output array correspond to the branches in the same order that they appear in the \"Branches\" array</p> <p>Below is an example of a State Machine that looks up user information, and in the parallel state provides the Name and Address in parallel.</p> <p></p> State Machine<pre><code>{\n  \"Comment\": \"Parallel Example.\",\n  \"StartAt\": \"LookupCustomerInfo\",\n  \"States\": {\n    \"LookupCustomerInfo\": {\n      \"Type\": \"Parallel\",\n      \"Next\": \"ReturnCombinedData\",\n      \"Branches\": [\n        {\n          \"StartAt\": \"Name\",\n          \"States\": {\n            \"Name\": {\n              \"Type\": \"Pass\",\n              \"Comment\": \"This can be any state that returns data, since it's a Pass, the values are static\",\n              \"Result\": {\n                \"fname\": \"Jack\",\n                \"lname\": \"Johnson\"\n              },\n              \"ResultPath\": \"$.Name\",\n              \"End\": true\n            }\n          }\n        },\n        {\n          \"StartAt\": \"Address\",\n          \"States\": {\n            \"Address\": {\n              \"Type\": \"Pass\",\n              \"Comment\": \"This can be any state that returns data, since it's a Pass, the values are static\",\n              \"Result\": {\n                \"Number\": \"123\",\n                \"Street\": \"Fake St\",\n                \"Zip\": \"90210\"\n              },\n              \"ResultPath\": \"$.Address\",\n              \"End\": true\n            }\n          }\n       }\n      ]\n    },\n    \"ReturnCombinedData\": {\n      \"Type\": \"Pass\",\n      \"Parameters\": {\n        \"comment\": \"Combining the result\",\n        \"UserDetails\": {\n          \"Name.$\": \"$[0].Name\",\n          \"Address.$\": \"$[1].Address\",\n          \"StaticMessage\": \"This is a static message\"\n        }\n      },\n      \"End\": true\n    }\n  }\n}\n</code></pre> <p>From the previous state machine we would get the following data from the Parallel state:</p> <pre><code>[\n  {\n    \"Name\": {\n      \"fname\": \"Jack\",\n      \"lname\": \"Johnson\"\n    }\n  },\n  {\n    \"Address\": {\n      \"Number\": \"123\",\n      \"Street\": \"Fake St\",\n      \"Zip\": \"90210\"\n    }\n  }\n],\n</code></pre> <p>In the state that comes after the parallel state, which I used a Pass state in this example, I used the Parameters field to combine the user information out from the array and into an object. In the Parameters field, setting the key value pair requires the following format:</p> <pre><code>\"Name.$\": \"$[0].Name\"\n</code></pre> <p>Since this is an array, you would require to select the appropriate location within the array that has the data you need. Since we are setting the name, the data that has the name is in the first position <code>\"$[0]\"</code> since the name operation was done on the first branch of the array. For the address, it is in position <code>\"$[1]\"</code> of the array. In the above state machine, the final successful output would look like the following:</p> <pre><code>{\n  \"comment\": \"Combining the result\",\n  \"UserDetails\": {\n    \"StaticMessage\": \"This is a static message\",\n    \"Address\": {\n      \"Number\": \"123\",\n      \"Street\": \"Fake St\",\n      \"Zip\": \"90210\"\n    },\n    \"Name\": {\n      \"fname\": \"Jack\",\n      \"lname\": \"Johnson\"\n    }\n  }\n}\n</code></pre> Use ResultSelector State Machine<pre><code>{\n  \"StartAt\": \"ParallelBranch\",\n  \"States\": {\n    \"ParallelBranch\": {\n      \"Type\": \"Parallel\",\n      \"ResultPath\": \"$\",\n      \"InputPath\": \"$\",\n      \"OutputPath\": \"$\",\n      \"ResultSelector\": {\n        \"UsersResult.$\": \"$[1].UsersUpload\",\n        \"CustomersResult.$\": \"$[0].customersDataUpload\"\n      },\n      \"Branches\": [\n        {\n          \"StartAt\": \"customersDataUpload\",\n          \"States\": {\n            \"customersDataUpload\": {\n              \"Type\": \"Pass\",\n              \"ResultPath\": \"$.customersDataUpload.Output\",\n              \"Result\": {\n                \"CompletionStatus\": \"success\",\n                \"CompletionDetails\": null\n              },\n              \"Next\": \"Wait2\"\n            },\n            \"Wait2\": {\n              \"Comment\": \"A Wait state delays the state machine from continuing for a specified time.\",\n              \"Type\": \"Wait\",\n              \"Seconds\": 2,\n              \"End\": true\n            }\n          }\n        },\n        {\n          \"StartAt\": \"UsersUpload\",\n          \"States\": {\n            \"UsersUpload\": {\n              \"Type\": \"Pass\",\n              \"Result\": {\n                \"CompletionStatus\": \"success\",\n                \"CompletionDetails\": null\n              },\n              \"ResultPath\": \"$.UsersUpload.Output\",\n              \"Next\": \"Wait1\"\n            },\n            \"Wait1\": {\n              \"Comment\": \"A Wait state delays the state machine from continuing for a specified time.\",\n              \"Type\": \"Wait\",\n              \"Seconds\": 1,\n              \"End\": true\n            }\n          }\n        }\n      ],\n      \"End\": true\n    }\n  },\n  \"TimeoutSeconds\": 129600,\n  \"Version\": \"1.0\"\n}\n</code></pre> <p>And the output will be like:</p> <pre><code>{\n  \"UsersResult\": {\n    \"Output\": {\n      \"CompletionStatus\": \"success\",\n      \"CompletionDetails\": null\n    }\n  },\n  \"CustomersResult\": {\n    \"Output\": {\n      \"CompletionStatus\": \"success\",\n      \"CompletionDetails\": null\n    }\n  }\n}\n</code></pre>"},{"location":"services/aws/step_functions/stfn-combine-data-parallel-state/#references","title":"References","text":"<ul> <li>AWS Step Functions \u2014 Combine Data from Parallel State</li> <li>Parallel States Merge the output in Step Function</li> </ul>"},{"location":"services/aws/step_functions/stfn-getting-started/","title":"Getting Started","text":""},{"location":"services/aws/step_functions/stfn-getting-started/#orchestration","title":"Orchestration","text":"<p> Invoke AWS Step Functions from other services</p>"},{"location":"services/aws/step_functions/stfn-getting-started/#scheduler","title":"Scheduler","text":"Amazon EventBridge Scheduler <p>To solve this challenge, you can run a serverless workflow on a time-based schedule. Amazon EventBridge is a serverless event bus that helps you receive, filter, transform, route, and deliver events from AWS services, your own applications, and software-as-a-service (SaaS) applications. Many AWS services generate events that EventBridge receives. Amazon EventBridge Scheduler is a serverless scheduler that allows you to create, run, and manage tasks at scale from one central, managed service. With AWS Step Functions, you can define state machines that describe your workflow as a series of steps, their relationships, and their inputs and outputs.</p> <p> Schedule a Serverless Workflow with AWS Step Functions and Amazon EventBridge Scheduler</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/","title":"State Machine Language","text":"<p>Warning</p> <p>This topic copy information from Amazon States Language<sup>1</sup>. I think this knowledge is powerfull to reuse this concept for create any state declarative object in the Data Pipeline workflow.</p> <p>Quote</p> <p>A State Machine is represented by a JSON Object.</p> Example: Hello World<pre><code>{\n  \"Comment\": \"A simple minimal example of the States language\",\n  \"StartAt\": \"Hello World\",\n  \"States\": {\n    \"Hello World\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloWorld\",\n      \"End\": true\n    }\n  }\n}\n</code></pre> <p>Top-level fields:</p> <ul> <li>A State Machine MUST have an object field named \"States\", whose fields represent   the states.</li> <li>A State Machine MUST have a string field named \"StartAt\", whose value MUST exactly   match one of names of the \"States\" fields. The interpreter starts running the   machine at the named state.</li> <li>A State Machine MAY have a string field named \"Comment\", provided for human-readable   description of the machine.</li> <li>A State Machine MAY have a string field named \"Version\", which gives the version   of the States language used in the machine. This document describes version 1.0,   and if omitted, the default value of \"Version\" is the string \"1.0\".</li> <li>A State Machine MAY have an integer field named \"TimeoutSeconds\". If provided,   it provides the maximum number of seconds the machine is allowed to run. If the   machine runs longer than the specified time, then the interpreter fails the   machine with a States.Timeout Error Name.</li> </ul>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#concepts","title":"Concepts","text":""},{"location":"services/aws/step_functions/stfn-state-machine-language/#states","title":"States","text":"<p>States are represented as fields of the top-level \"States\" object. The state name, whose length MUST BE less than or equal to 80 Unicode characters, is the field name; state names MUST be unique within the scope of the whole state machine. States describe tasks (units of work), or specify flow control (e.g. Choice).</p> <p>Here is an example state that executes a Lambda function:</p> <pre><code>\"HelloWorld\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloWorld\",\n  \"Next\": \"NextState\",\n  \"Comment\": \"Executes the HelloWorld Lambda function\"\n}\n</code></pre> <p>Note</p> <ul> <li> <p>All states MUST have a \"Type\" field. This document refers to the values   of this field as a state's type, and to a state such as the one in the   example above as a Task State.</p> </li> <li> <p>Any state MAY have a \"Comment\" field, to hold a human-readable comment or   description.</p> </li> <li> <p>Most state types require additional fields as specified in this document.</p> </li> <li> <p>Any state except for Choice, Succeed, and Fail MAY have a field named \"End\"   whose value MUST be a boolean. The term \"Terminal State\" means a state with   with <code>{ \"End\": true }</code>, or a state with <code>{ \"Type\": \"Succeed\" }</code>, or a state with   <code>{ \"Type\": \"Fail\" }</code>.</p> </li> </ul>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#transitions","title":"Transitions","text":"<p>Transitions link states together, defining the control flow for the state machine. After executing a non-terminal state, the interpreter follows a transition to the next state. For most state types, transitions are unconditional and specified through the state's <code>\"Next\"</code> field.</p> <p>All non-terminal states MUST have a <code>\"Next\"</code> field, except for the Choice State. The value of the <code>\"Next\"</code> field MUST exactly and case-sensitively match the name of the another state.</p> <p>States can have multiple incoming transitions from other states.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#timestamps","title":"Timestamps","text":"<p>The Choice and Wait States deal with JSON field values which represent timestamps. These are strings which MUST conform to the RFC3339 profile of ISO 8601, with the further restrictions that an uppercase <code>\"T\"</code> character MUST be used to separate date and time, and an uppercase <code>\"Z\"</code> character MUST be present in the absence of a numeric time zone offset, for example <code>\"2016-03-14T01:59:00Z\"</code>.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#data","title":"Data","text":"<p>The interpreter passes data between states to perform calculations or to dynamically control the state machine's flow. All such data MUST be expressed in JSON.</p> <p>When a state machine is started, the caller can provide an initial JSON text as input, which is passed to the machine's start state as input. If no input is provided, the default is an empty JSON object, <code>{}</code>. As each state is executed, it receives a JSON text as input and can produce arbitrary output, which MUST be a JSON text. When two states are linked by a transition, the output from the first state is passed as input to the second state. The output from the machine's terminal state is treated as its output.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#the-context-object","title":"The Context Object","text":"<p>The interpreter can provide information to an executing state machine about the execution and other implementation details. This is delivered in the form of a JSON object called the \"Context Object\". This version of the States Language specification does not specify any contents of the Context Object.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#paths","title":"Paths","text":"<p>A Path is a string, beginning with \"$\", used to identify components with a JSON text. The syntax is that of JsonPath.</p> <p>When a Path begins with \"$$\", two dollar signs, this signals that it is intended to identify content within the Context Object. The first dollar sign is stripped, and the remaining text, which begins with a dollar sign, is interpreted as the JSONPath applying to the Context Object.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#reference-paths","title":"Reference Paths","text":"<p>A Reference Path is a Path with syntax limited in such a way that it can only identify a single node in a JSON structure: The operators \"@\", \",\", \":\", and \"?\" are not supported - all Reference Paths MUST be unambiguous references to a single value, array, or object (subtree).</p> <p>For example, if state input data contained the values:</p> <pre><code>{\n  \"foo\": 123,\n  \"bar\": [\"a\", \"b\", \"c\"],\n  \"car\": {\n      \"cdr\": true\n  }\n}\n</code></pre> <p>Then the following Reference Paths would return:</p> <pre><code>$.foo =&gt; 123\n$.bar =&gt; [\"a\", \"b\", \"c\"]\n$.car.cdr =&gt; true\n</code></pre> <p>Paths and Reference Paths are used by certain states, as specified later in this document, to control the flow of a state machine or to configure a state's settings or options.</p> <p>Here are some examples of acceptable Reference Path syntax:</p> <pre><code>$.store.book\n$.store\\.book\n$.\\stor\\e.boo\\k\n$.store.book.title\n$.foo.\\.bar\n$.foo\\@bar.baz\\[\\[.\\?pretty\n$.&amp;\u0416\u4e2d.\\uD800\\uDF46\n$.ledgers.branch[0].pending.count\n$.ledgers.branch[0]\n$.ledgers[0][22][315].foo\n$['store']['book']\n$['store'][0]['book']\n</code></pre>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#payload-template","title":"Payload Template","text":"<p>A state machine interpreter dispatches data as input to tasks to do useful work, and receives output back from them. It is frequently desired to reshape input data to meet the format expectations of tasks, and similarly to reshape the output coming back. A JSON object structure called a Payload Template is provided for this purpose.</p> <p>In the Task, Map, Parallel, and Pass States, the Payload Template is the value of a field named \"Parameters\". In the Task, Map, and Parallel States, there is another Payload Template which is the value of a field named \"ResultSelector\".</p> <p>A Payload Template MUST be a JSON object; it has no required fields. The interpreter processes the Payload Template as described in this section; the result of that processing is called the payload.</p> <p>To illustrate by example, the Task State has a field named \"Parameters\" whose value is a Payload Template. Consider the following Task State:</p> <pre><code>\"X\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:states:us-east-1:123456789012:task:X\",\n  \"Next\": \"Y\",\n  \"Parameters\": {\n    \"first\": 88,\n    \"second\": 99\n  }\n}\n</code></pre> <p>In this case, the payload is the object with \"first\" and \"second\" fields whose values are respectively 88 and 99. No processing needs to be performed and the payload is identical to the Payload Template.</p> <p>Values from the Payload Template's input and the Context Object can be inserted into the payload with a combination of a field-naming convention, Paths and Intrinsic Functions.</p> <p>If any field within the Payload Template (however deeply nested) has a name ending with the characters \".\\(\", its value is transformed according to rules below and the field is renamed to strip the \".\\)\" suffix.</p> <p>If the field value begins with only one \"$\", the value MUST be a Path. In this case, the Path is applied to the Payload Template's input and is the new field value.</p> <p>If the field value begins with \"$$\", the first dollar sign is stripped and the remainder MUST be a Path. In this case, the Path is applied to the Context Object and is the new field value.</p> <p>If the field value does not begin with \"$\", it MUST be an Intrinsic Function (see below). The interpreter invokes the Intrinsic Function and the result is the new field value.</p> <p>If the path is legal but cannot be applied successfully, the interpreter fails the machine execution with an Error Name of \"States.ParameterPathFailure\". If the Intrinsic Function fails during evaluation, the interpreter fails the machine execution with an Error Name of \"States.IntrinsicFailure\".</p> <p>A JSON object MUST NOT have duplicate field names after fields ending with the characters \".\\(\" are renamed to strip the \".\\)\" suffix.</p> <pre><code>\"X\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:states:us-east-1:123456789012:task:X\",\n  \"Next\": \"Y\",\n  \"Parameters\": {\n    \"flagged\": true,\n    \"parts\": {\n      \"first.$\": \"$.vals[0]\",\n      \"last3.$\": \"$.vals[-3:]\"\n    },\n    \"weekday.$\": \"$$.DayOfWeek\",\n    \"formattedOutput.$\": \"States.Format('Today is {}', $$.DayOfWeek)\"\n  }\n}\n</code></pre> <p>Suppose that the input to the P is as follows:</p> <pre><code>{\n  \"flagged\": 7,\n  \"vals\": [0, 10, 20, 30, 40, 50]\n}\n</code></pre> <p>Further, suppose that the Context Object is as follows:</p> <pre><code>{\n  \"DayOfWeek\": \"TUESDAY\"\n}\n</code></pre> <p>In this case, the effective input to the code identified in the \"Resource\" field would be as follows:</p> <pre><code>{\n  \"flagged\": true,\n  \"parts\": {\n    \"first\": 0,\n    \"last3\": [30, 40, 50]\n  },\n  \"weekday\": \"TUESDAY\",\n  \"formattedOutput\": \"Today is TUESDAY\"\n}\n</code></pre>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#intrinsic-functions","title":"Intrinsic Functions","text":"<p>The States Language provides a small number of \"Intrinsic Functions\", constructs which look like functions in programming languages and can be used to help Payload Templates process the data going to and from Task Resources. See Appendix B for a full list of Intrinsic Functions</p> <p>Here is an example of an Intrinsic Function named \"States.Format\" being used to prepare data:</p> <pre><code>\"X\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:states:us-east-1:123456789012:task:X\",\n  \"Next\": \"Y\",\n  \"Parameters\": {\n    \"greeting.$\": \"States.Format('Welcome to {} {}\\\\'s playlist.', $.firstName, $.lastName)\"\n  }\n}\n</code></pre> <p>Note</p> <ul> <li>An Intrinsic Function MUST be a string.</li> <li>The Intrinsic Function MUST begin with an Intrinsic Function name. An Intrinsic Function name MUST contain only the characters A through Z, a through z, 0 through 9, \".\", and \"_\".</li> <li>All Intrinsic Functions defined by this specification have names that begin with \"States.\". Other implementations may define their own Intrinsic Functions whose names MUST NOT begin with \"States.\".</li> <li>The Intrinsic Function name MUST be followed immediately by a list of zero or more arguments, enclosed by \"(\" and \")\", and separated by commas.</li> <li>Intrinsic Function arguments may be strings enclosed by apostrophe (<code>'</code>) characters, numbers, <code>null</code>, Paths, or nested Intrinsic Functions.</li> <li>The value of a string, number or <code>null</code> argument is the argument itself. The value of an argument which is a Path is the result of applying it to the input of the Payload Template. The value of an argument which is an Intrinsic Function is the result of the function invocation.\"</li> </ul> <p>Note that in the example above, the first argument of <code>States.Format</code> could have been a Path that yielded the formatting template string.</p> <ul> <li>The following characters are reserved for all Intrinsic Functions and MUST be escaped: ' { } \\</li> </ul> <p>If any of the reserved characters needs to appear as part of the value without serving as a reserved character, it MUST be escaped with a backslash.</p> <p>If the character \" needs to appear as part of the value without serving as an escape character, it MUST be escaped with a backslash.</p> <p>The literal string <code>\\'</code> represents <code>'</code>.   The literal string <code>\\{</code> represents <code>{</code>.   The literal string <code>\\}</code> represents <code>}</code>.   The literal string <code>\\\\</code> represents <code>\\</code>.</p> <p>In JSON, all backslashes contained in a string literal value must be escaped   with another backslash, therefore, the above will equate to:</p> <p>The escaped string <code>\\\\'</code> represents <code>'</code>.   The escaped string <code>\\\\{</code> represents <code>{</code>.   The escaped string <code>\\\\}</code> represents <code>}</code>.   The escaped string <code>\\\\\\\\</code> represents <code>\\</code>.</p> <p>If an open escape backslash <code>\\</code> is found in the Intrinsic Function, the   interpreter will throw a runtime error.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#input-and-output-processing","title":"Input and Output Processing","text":"<p>As described above, data is passed between states as JSON texts. However, a state may want to process only a subset of its input data, and may want that data structured differently from the way it appears in the input. Similarly, it may want to control the format and content of the data that it passes on as output.</p> <p>Fields named \"InputPath\", \"Parameters\", \"ResultSelector\", \"ResultPath\", and \"OutputPath\" exist to support this.</p> <p>Any state except for the Fail and Succeed States MAY have \"InputPath\" and \"OutputPath\".</p> <p>States which potentially generate results MAY have \"Parameters\", \"ResultSelector\" and \"ResultPath\": Task State, Parallel State, and Map State.</p> <p>Pass State MAY have \"Parameters\" and \"ResultPath\" to control its output value.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#using-inputpath-parameters-resultselector-resultpath-and-outputpath","title":"Using InputPath, Parameters, ResultSelector, ResultPath and OutputPath","text":"<p>In this discussion, \"raw input\" means the JSON text that is the input to a state. \"Result\" means the JSON text that a state generates, for example from external code invoked by a Task State, the combined result of the branches in a Parallel or Map State, or the Value of the \"Result\" field in a Pass State. \"Effective input\" means the input after the application of InputPath and Parameters, \"effective result\" means the result after processing it with ResultSelector and \"effective output\" means the final state output after processing the result with ResultSelector, ResultPath and OutputPath.</p> <ul> <li>The value of \"InputPath\" MUST be a Path, which is applied to a State's raw input   to select some or all of it; that selection is used by the state, for example   in passing to Resources in Task States and Choices selectors in Choice States.</li> <li>The value of \"Parameters\" MUST be a Payload Template which is a JSON object,   whose input is the result of applying the InputPath to the raw input. If the   \"Parameters\" field is provided, its payload, after the extraction and embedding,   becomes the effective input.</li> <li>The value of \"ResultSelector\" MUST be a Payload Template, whose input is the   result, and whose payload replaces and becomes the effective result.</li> <li>The value of \"ResultPath\" MUST be a Reference Path, which specifies the raw input's   combination with or replacement by the state's result.</li> <li>The value of \"ResultPath\" MUST NOT begin with \"$$\"; i.e. it may not be used to   insert content into the Context Object.</li> <li>The value of \"OutputPath\" MUST be a Path, which is applied to the state's output   after the application of ResultPath, producing the effective output which serves   as the raw input for the next state.</li> </ul> <p>Note that JsonPath can yield multiple values when applied to an input JSON text. For example, given the text:</p> <pre><code>{ \"a\": [1, 2, 3, 4] }\n</code></pre> <p>Then if the JsonPath <code>$.a[0,1]</code> is applied, the result will be two JSON texts, <code>1</code> and <code>2</code>. When this happens, to produce the effective input, the interpreter gathers the texts into an array, so in this example the state would see the input:</p> <pre><code>[ 1, 2 ]\n</code></pre> <p>The same rule applies to OutputPath processing; if the OutputPath result contains multiple values, the effective output is a JSON array containing all of them.</p> <p>The \"ResultPath\" field's value is a Reference Path that specifies where to place the result, relative to the raw input. If the raw input has a field at the location addressed by the ResultPath value then in the output that field is discarded and overwritten by the state's result. Otherwise, a new field is created in the state output, with intervening fields constructed as necessary. For example, given the raw input:</p> <pre><code>{\n  \"master\": {\n    \"detail\": [1, 2, 3]\n  }\n}\n</code></pre> <p>If the state's result is the number <code>6</code>, and the \"ResultPath\" is <code>$.master.detail</code>, then in the output the <code>detail</code> field would be overwritten:</p> <pre><code>{\n  \"master\": {\n    \"detail\": 6\n  }\n}\n</code></pre> <p>If instead a \"ResultPath\" of <code>$.master.result.sum</code> was used then the result would be combined with the raw input, producing a chain of new fields containing <code>result</code> and <code>sum</code>:</p> <pre><code>{\n  \"master\": {\n    \"detail\": [1, 2, 3],\n    \"result\": {\n      \"sum\": 6\n    }\n  }\n}\n</code></pre> <p>If the value of InputPath is <code>null</code>, that means that the raw input is discarded, and the effective input for the state is an empty JSON object, <code>{}</code>. Note that having a value of <code>null</code> is different from the \"InputPath\" field being absent.</p> <p>If the value of ResultPath is <code>null</code>, that means that the state's result is discarded and its raw input becomes its result.</p> <p>If the value of OutputPath is <code>null</code>, that means the input and result are discarded, and the effective output from the state is an empty JSON object, <code>{}</code>.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#defaults","title":"Defaults","text":"<p>Each of InputPath, Parameters, ResultSelector, ResultPath, and OutputPath are optional. The default value of InputPath is \"\\(\", so by default the effective input is just the raw input. The default value of ResultPath is \"\\)\", so by default a state's result overwrites and replaces the input. The default value of OutputPath is \"$\", so by default a state's effective output is the result of processing ResultPath.</p> <p>Parameters and ResultSelector have no default value. If absent, they have no effect on their input.</p> <p>Therefore, if none of InputPath, Parameters, ResultSelector, ResultPath, or OutputPath are supplied, a state consumes the raw input as provided and passes its result to the next state.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#inputoutput-processing-examples","title":"Input/Output Processing Examples","text":"<p>Consider the example given above, of a Lambda task that sums a pair of numbers. As presented, its input is: <code>{ \"val1\": 3, \"val2\": 4 }</code> and its output is: <code>7</code>.</p> <p>Suppose the input is little more complex:</p> <pre><code>{\n  \"title\": \"Numbers to add\",\n  \"numbers\": { \"val1\": 3, \"val2\": 4 }\n}\n</code></pre> <p>Then suppose we modify the state definition by adding:</p> <pre><code>\"InputPath\": \"$.numbers\",\n\"ResultPath\": \"$.sum\"\n</code></pre> <p>And finally,suppose we simplify Line 4 of the Lambda function to read as follows: <code>return JSON.stringify(total)</code>. This is probably a better form of the function, which should really only care about doing math and not care how its result is labeled.</p> <p>In this case, the output would be:</p> <pre><code>{\n  \"title\": \"Numbers to add\",\n  \"numbers\": { \"val1\": 3, \"val2\": 4 },\n  \"sum\": 7\n}\n</code></pre> <p>The interpreter might need to construct multiple levels of JSON object to achieve the desired effect. Suppose the input to some Task State is:</p> <pre><code>{ \"a\": 1 }\n</code></pre> <p>Suppose the output from the Task is \"Hi!\", and the value of the \"ResultPath\" field is \"$.b.greeting\". Then the output from the state would be:</p> <pre><code>{\n  \"a\": 1,\n  \"b\": {\n    \"greeting\": \"Hi!\"\n  }\n}\n</code></pre>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#runtime-errors","title":"Runtime Errors","text":"<p>Suppose a state's input is the string <code>\"foo\"</code>, and its \"ResultPath\" field has the value \"$.x\". Then ResultPath cannot apply and the interpreter fails the machine with an Error Name of \"States.ResultPathMatchFailure\".</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#errors","title":"Errors","text":"<p>Any state can encounter runtime errors. Errors can arise because of state machine definition issues (e.g. the \"ResultPath\" problem discussed immediately above), task failures (e.g. an exception thrown by a Lambda function) or because of transient issues, such as network partition events.</p> <p>When a state reports an error, the default course of action for the interpreter is to fail the whole state machine.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#error-representation","title":"Error representation","text":"<p>Errors are identified by case-sensitive strings, called Error Names. The States language defines a set of built-in strings naming well-known errors, all of which begin with the prefix \"States.\"; see Appendix A.</p> <p>States MAY report errors with other names, which MUST NOT begin with the prefix \"States.\".</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#retrying-after-error","title":"Retrying after error","text":"<p>Task States, Parallel States, and Map States MAY have a field named \"Retry\", whose value MUST be an array of objects, called Retriers.</p> <p>Each Retrier MUST contain a field named \"ErrorEquals\" whose value MUST be a non-empty array of Strings, which match Error Names.</p> <p>When a state reports an error, the interpreter scans through the Retriers and, when the Error Name appears in the value of a Retrier's \"ErrorEquals\" field, implements the retry policy described in that Retrier.</p> <p>An individual Retrier represents a certain number of retries, usually at increasing time intervals.</p> <p>A Retrier MAY contain a field named \"IntervalSeconds\", whose value MUST be a positive integer, representing the number of seconds before the first retry attempt (default value: 1); a field named \"MaxAttempts\" whose value MUST be a non-negative integer, representing the maximum number of retry attempts (default: 3); a field named \"MaxDelaySeconds\" whose value MUST be positive integer, representing the maximum interval in seconds to wait before the next retry attempt; a field named \"JitterStrategy\", whose value MUST be a string, representing the jitter strategy to use in the retry interval calculation; and a field named \"BackoffRate\", a number which is the multiplier that increases the retry interval on each attempt (default: 2.0). The value of BackoffRate MUST be greater than or equal to 1.0.</p> <p>Note that a \"MaxAttempts\" field whose value is 0 is legal, specifying that some error or errors should never be retried.</p> <p>The States Language does not constrain the value of the \"JitterStrategy\" field. The interpreter will use the specified value to select the jitter strategy to use when calculating the retry interval.</p> <p>Here is an example of a Retrier which will make 2 retry attempts after waits of <code>3</code> and <code>6</code> seconds:</p> <pre><code>\"Retry\" : [\n  {\n    \"ErrorEquals\": [ \"States.Timeout\" ],\n    \"IntervalSeconds\": 3,\n    \"MaxAttempts\": 2,\n    \"BackoffRate\": 2.0\n  }\n]\n</code></pre> <p>Here is the same example, but with a maximum limit of 4 seconds on the retry interval. In this case the Retrier will make the second attempt after 4 seconds, rather than 6 seconds:</p> <pre><code>\"Retry\" : [\n  {\n    \"ErrorEquals\": [ \"States.Timeout\" ],\n    \"IntervalSeconds\": 3,\n    \"MaxAttempts\": 2,\n    \"BackoffRate\": 2.0,\n    \"MaxDelaySeconds\": 4\n  }\n]\n</code></pre> <p>Here is an example of a Retrier which uses an interpreter-defined JitterStrategy called \"SAMPLE\". Instead of waiting for 2 seconds before the first retry attempt or 4 seconds before the second retry attempt, the interpreter will modify these values according to the jitter strategy:</p> <pre><code>\"Retry\" : [\n  {\n    \"ErrorEquals\": [ \"States.Timeout\" ],\n    \"IntervalSeconds\": 2,\n    \"MaxAttempts\": 2,\n    \"BackoffRate\": 2.0,\n    \"JitterStrategy\": \"SAMPLE\"\n  }\n]\n</code></pre> <p>The reserved name \"States.ALL\" in a Retrier's \"ErrorEquals\" field is a wildcard and matches any Error Name. Such a value MUST appear alone in the \"ErrorEquals\" array and MUST appear in the last Retrier in the \"Retry\" array.</p> <p>Here is an example of a Retrier which will retry any error except for \"States.Timeout\", using the default retry parameters.</p> <pre><code>\"Retry\": [\n  {\n    \"ErrorEquals\": [ \"States.Timeout\" ],\n    \"MaxAttempts\": 0\n  },\n  {\n    \"ErrorEquals\": [ \"States.ALL\" ]\n  }\n]\n</code></pre> <p>If the error recurs more times than allowed for by the \"MaxAttempts\" field, retries cease and normal error handling resumes.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#complex-retry-scenarios","title":"Complex retry scenarios","text":"<p>A Retrier's parameters apply across all visits to that Retrier in the context of a single state execution. This is best illustrated by example; consider the following Task State:</p> <pre><code>\"X\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:states:us-east-1:123456789012:task:X\",\n  \"Next\": \"Y\",\n  \"Retry\": [\n    {\n      \"ErrorEquals\": [ \"ErrorA\", \"ErrorB\" ],\n      \"IntervalSeconds\": 1,\n      \"BackoffRate\": 2,\n      \"MaxAttempts\": 2\n    },\n    {\n      \"ErrorEquals\": [ \"ErrorC\" ],\n      \"IntervalSeconds\": 5\n    }\n  ],\n  \"Catch\": [\n    {\n      \"ErrorEquals\": [ \"States.ALL\" ],\n      \"Next\": \"Z\"\n    }\n  ]\n}\n</code></pre> <p>Suppose that this task fails four successive times, throwing Error Names \"ErrorA\", \"ErrorB\", \"ErrorC\", and \"ErrorB\". The first two errors match the first retrier and cause waits of one and two seconds. The third error matches the second retrier and causes a wait of five seconds. The fourth error would match the first retrier but its \"MaxAttempts\" ceiling of two retries has already been reached, so that Retrier fails, and execution is redirected to the \"Z\" state via the \"Catch\" field.</p> <p>Note that once the interpreter transitions to another state in any way, all the Retrier parameters reset.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#fallback-states","title":"Fallback states","text":"<p>Task States, Parallel States, and Map States MAY have a field named \"Catch\", whose value MUST be an array of objects, called Catchers.</p> <p>Each Catcher MUST contain a field named \"ErrorEquals\", specified exactly as with the Retrier \"ErrorEquals\" field, and a field named \"Next\" whose value MUST be a string exactly matching a State Name.</p> <p>When a state reports an error and either there is no Retrier, or retries have failed to resolve the error, the interpreter scans through the Catchers in array order, and when the Error Name appears in the value of a Catcher's \"ErrorEquals\" field, transitions the machine to the state named in the value of the \"Next\" field.</p> <p>The reserved name \"States.ALL\" appearing in a Retrier's \"ErrorEquals\" field is a wildcard and matches any Error Name. Such a value MUST appear alone in the \"ErrorEquals\" array and MUST appear in the last Catcher in the \"Catch\" array.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#error-output","title":"Error output","text":"<p>When a state reports an error and it matches a Catcher, causing a transfer to another state, the state's result (and thus the input to the state identified in the Catcher's \"Next\" field) is a JSON object, called the Error Output. The Error Output MUST have a string-valued field named \"Error\", containing the Error Name. It SHOULD have a string-valued field named \"Cause\", containing human-readable text about the error.</p> <p>A Catcher MAY have an \"ResultPath\" field, which works exactly like a state's top-level \"ResultPath\", and may be used to inject the Error Output into the state's original input to create the input for the Catcher's \"Next\" state. The default value, if the \"ResultPath\" field is not provided, is \"$\", meaning that the output consists entirely of the Error Output.</p> <p>Here is an example of a Catcher that will transition to the state named \"RecoveryState\" when a Lambda function throws an unhandled Java Exception, and otherwise to the \"EndMachine\" state, which is presumably Terminal.</p> <p>Also in this example, if the first Catcher matches the Error Name, the input to \"RecoveryState\" will be the original state input, with the Error Output as the value of the top-level \"error-info\" field. For any other error, the input to \"EndMachine\" will just be the Error Output.</p> <pre><code>\"Catch\": [\n  {\n    \"ErrorEquals\": [ \"java.lang.Exception\" ],\n    \"ResultPath\": \"$.error-info\",\n    \"Next\": \"RecoveryState\"\n  },\n  {\n    \"ErrorEquals\": [ \"States.ALL\" ],\n    \"Next\": \"EndMachine\"\n  }\n]\n</code></pre> <p>Each Catcher can specific multiple errors to handle.</p> <p>When a state has both \"Retry\" and \"Catch\" fields, the interpreter uses any appropriate Retriers first and only applies the matching Catcher transition if the retry policy fails to resolve the error.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#state-types","title":"State Types","text":"<p>As a reminder, the state type is given by the value of the \"Type\" field, which MUST appear in every State object.</p> <p></p> Pass StateTask StateChoice StateWait StateSucceed StateFail StateParallel StateMap State <p>The Pass State (identified by <code>\"Type\":\"Pass\"</code>) by default passes its input to its output, performing no work.</p> <p>A Pass State MAY have a field named \"Result\". If present, its value is treated as the output of a virtual task, and placed as prescribed by the \"ResultPath\" field, if any, to be passed on to the next state. If \"Result\" is not provided, the output is the input. Thus if neither \"Result\" nor \"ResultPath\" are provided, the Pass State copies its input through to its output.</p> <p>Here is an example of a Pass State that injects some fixed data into the state machine, probably for testing purposes.</p> <pre><code>\"No-op\": {\n  \"Type\": \"Pass\",\n  \"Result\": {\n    \"x-datum\": 0.381018,\n    \"y-datum\": 622.2269926397355\n  },\n  \"ResultPath\": \"$.coords\",\n  \"Next\": \"End\"\n}\n</code></pre> InputOutput <pre><code>{\n  \"georefOf\": \"Home\"\n}\n</code></pre> <pre><code>{\n  \"georefOf\": \"Home\",\n  \"coords\": {\n    \"x-datum\": 0.381018,\n    \"y-datum\": 622.2269926397355\n  }\n}\n</code></pre> <p>The Task State (identified by <code>\"Type\":\"Task\"</code>) causes the interpreter to execute the work identified by the state's \"Resource\" field.</p> <p>Here is an example:</p> <pre><code>\"TaskState\": {\n  \"Comment\": \"Task State example\",\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:states:us-east-1:123456789012:task:HelloWorld\",\n  \"Next\": \"NextState\",\n  \"TimeoutSeconds\": 300,\n  \"HeartbeatSeconds\": 60\n}\n</code></pre> <p>A Task State MUST include a \"Resource\" field, whose value MUST be a URI that uniquely identifies the specific task to execute. The States language does not constrain the URI scheme nor any other part of the URI.</p> <p>A Task State MAY include a \"Parameters\" field, whose value MUST be a Payload Template. A Task State MAY include a \"ResultSelector\" field, whose value MUST be a Payload Template.</p> <p>Tasks can optionally specify timeouts. Timeouts (the \"TimeoutSeconds\" and \"HeartbeatSeconds\" fields) are specified in seconds and MUST be positive integers.</p> <p>Both the total and heartbeat timeouts can be provided indirectly. A Task State may have \"TimeoutSecondsPath\" and \"HeartbeatSecondsPath\" fields which MUST be Reference Paths which, when resolved, MUST select fields whose values are positive integers. A Task State MUST NOT include both \"TimeoutSeconds\" and \"TimeoutSecondsPath\" or both \"HeartbeatSeconds\" and \"HeartbeatSecondsPath\".</p> <p>If provided, the \"HeartbeatSeconds\" interval MUST be smaller than the \"TimeoutSeconds\" value.</p> <p>If not provided, the default value of \"TimeoutSeconds\" is 60.</p> <p>If the state runs longer than the specified timeout, or if more time than the specified heartbeat elapses between heartbeats from the task, then the interpreter fails the state with a States.Timeout Error Name.</p> <p>Note</p> <p>A Task State MAY include a \"Credentials\" field, whose value MUST be a JSON object whose value is defined by the interpreter. The States language does not constrain the value of the \"Credentials\" field. The interpreter will use the specified credentials to execute the work identified by the state's \"Resource\" field.</p> <p>A Choice State (identified by <code>\"Type\":\"Choice\"</code>) adds branching logic to a state machine.</p> <p>A Choice State MUST have a \"Choices\" field whose value is a non-empty array. Each element of the array MUST be a JSON object and is called a Choice Rule. A Choice Rule may be evaluated to return a boolean value. A Choice Rule at the top level, i.e. which is a member of the \"Choices\" array, MUST have a \"Next\" field, whose value MUST match a state name.</p> <p>The interpreter attempts pattern-matches against the top-level Choice Rules in array order and transitions to the state specified in the \"Next\" field on the first Choice Rule where there is an exact match between the input value and a member of the comparison-operator array.</p> <p>Here is an example of a Choice State.</p> <pre><code>\"DispatchEvent\": {\n  \"Type\" : \"Choice\",\n  \"Choices\": [\n    {\n      \"Not\": {\n        \"Variable\": \"$.type\",\n        \"StringEquals\": \"Private\"\n      },\n      \"Next\": \"Public\"\n    },\n    {\n      \"And\": [\n        {\n          \"Variable\": \"$.value\",\n          \"IsPresent\": true\n        },\n        {\n          \"Variable\": \"$.value\",\n          \"IsNumeric\": true\n        },\n        {\n          \"Variable\": \"$.value\",\n          \"NumericGreaterThanEquals\": 20\n        },\n        {\n          \"Variable\": \"$.value\",\n          \"NumericLessThan\": 30\n        }\n      ],\n      \"Next\": \"ValueInTwenties\"\n    },\n    {\n      \"Variable\": \"$.rating\",\n      \"NumericGreaterThanPath\": \"$.auditThreshold\",\n      \"Next\": \"StartAudit\"\n    }\n  ],\n  \"Default\": \"RecordEvent\"\n}\n</code></pre> <p>In this example, suppose the machine is started with an input value of:</p> <pre><code>{\n  \"type\": \"Private\",\n  \"value\": 22\n}\n</code></pre> <p>Then the interpreter will transition to the \"ValueInTwenties\" state, based on the \"value\" field.</p> <p>A Choice Rule MUST be either a Boolean Expression or a Data-test Expression.</p> Boolean ExpressionData-test Expression <p>A Boolean Expression is a JSON object which contains a field named \"And\", \"Or\", or \"Not\". If the field name is \"And\" or \"Or\", the value MUST be an non-empty object array of Choice Rules, which MUST NOT contain \"Next\" fields; the interpreter processes the array elements in order, performing the boolean evaluations in the expected fashion, ceasing array processing when the boolean value has been unambiguously determined.</p> <p>The value of a Boolean Expression containing a \"Not\" field MUST be a single Choice Rule, that MUST NOT contain a \"Next\" field; it returns the inverse of the boolean to which the Choice Rule evaluates.</p> <p>A Data-test Expression Choice Rule is an assertion about a field and its value which yields a boolean depending on the data. A Data-test Expression MUST contain a field named \"Variable\" whose value MUST be a Path.</p> <p>Each choice rule MUST contain exactly one field containing a comparison operator. The following comparison operators are supported:</p> Lists <ul> <li>StringEquals, StringEqualsPath</li> <li>StringLessThan, StringLessThanPath</li> <li>StringGreaterThan, StringGreaterThanPath</li> <li>StringLessThanEquals, StringLessThanEqualsPath</li> <li>StringGreaterThanEquals, StringGreaterThanEqualsPath</li> <li> <p>StringMatches</p> <p>Note: The value MUST be a String which MAY contain one or more \"\" characters. The expression yields true if the data value selected by the Variable Path matches the value, where \"\" in the value matches zero or more characters. Thus, <code>foo*.log</code> matches <code>foo23.log</code>, <code>*.log</code> matches <code>zebra.log</code>, and <code>foo*.*</code> matches <code>foobar.zebra</code>. No characters other than \"*\" have any special meaning during matching.</p> <p>If the character \"*\" needs to appear as part of the value without serving as a wildcard, it MUST be escaped with a backslash.</p> <p>If the character \" needs to appear as part of the value without serving as an escape character, it MUST be escaped with a backslash.</p> <p>The literal string <code>\\*</code> represents <code>*</code>. The literal string <code>\\\\</code> represents <code>\\</code>.</p> <p>In JSON, all backslashes contained in a string literal value must be escaped with another backslash, therefore, the above will equate to:</p> <p>The escaped string <code>\\\\*</code> represents <code>*</code>. The escaped string <code>\\\\\\\\</code> represents <code>\\</code>.</p> <p>If an open escape backslash  is found in the StringMatches string, the interpreter will throw a runtime error.</p> </li> <li> <p>NumericEquals, NumericEqualsPath</p> </li> <li>NumericLessThan, NumericLessThanPath</li> <li>NumericGreaterThan, NumericGreaterThanPath</li> <li>NumericLessThanEquals, NumericLessThanEqualsPath</li> <li>NumericGreaterThanEquals, NumericGreaterThanEqualsPath</li> <li>BooleanEquals, BooleanEqualsPath</li> <li>TimestampEquals, TimestampEqualsPath</li> <li>TimestampLessThan, TimestampLessThanPath</li> <li>TimestampGreaterThan, TimestampGreaterThanPath</li> <li>TimestampLessThanEquals, TimestampLessThanEqualsPath</li> <li>TimestampGreaterThanEquals, TimestampGreaterThanEqualsPath</li> <li> <p>IsNull</p> <p>Note: This means the value is the built-in JSON literal <code>null</code>.</p> </li> <li> <p>IsPresent</p> <p>Note: In this case, if the Variable-field Path fails to match anything in the input no exception is thrown and the Choice Rule just returns false.</p> </li> <li> <p>IsNumeric</p> </li> <li>IsString</li> <li>IsBoolean</li> <li>IsTimestamp</li> </ul> <p>For those operators that end with \"Path\", the value MUST be a Path, to be applied to the state's effective input to yield a value to be compared with the value yielded by the Variable path.</p> <p>For each operator which compares values, if the values are not both of the appropriate type (String, number, boolean, or Timestamp) the comparison will return false. Note that a field which is thought of as a timestamp could be matched by a string-typed comparator.</p> <p>The various String comparators compare strings character-by-character with no special treatments such as case-folding, white-space collapsing, or Unicode form normalization.</p> <p>Note that for interoperability, numeric comparisons should not be assumed to work with values outside the magnitude or precision representable using the IEEE 754-2008 \"binary64\" data type. In particular, integers outside of the range [-(2^53)+1, (2^53)-1] might fail to compare in the expected way.</p> <p>A Choice State MAY have a \"Default\" field, whose value MUST be a string whose value MUST match a State name; that state will execute if none of the Choice Rules match. The interpreter will raise a runtime States.NoChoiceMatched error if a Choice State fails to match a Choice Rule and no \"Default\" transition was specified.</p> <p>A Choice State MUST NOT be an End state.</p> <p>A Wait State (identified by <code>\"Type\":\"Wait\"</code>) causes the interpreter to delay the machine from continuing for a specified time. The time can be specified as a wait duration, specified in seconds, or an absolute expiry time, specified as an ISO-8601 extended offset date-time format string.</p> <p>For Examples:</p> Wait with secondWait until an absolute timeWait using a Reference Path <pre><code>\"wait_ten_seconds\" : {\n  \"Type\" : \"Wait\",\n  \"Seconds\" : 10,\n  \"Next\": \"NextState\"\n}\n</code></pre> <pre><code>\"wait_until\" : {\n  \"Type\": \"Wait\",\n  \"Timestamp\": \"2016-03-14T01:59:00Z\",\n  \"Next\": \"NextState\"\n}\n</code></pre> <p>Reworked to look up the timestamp time using a Reference Path to the data, which might look like <code>{ \"expirydate\": \"2016-03-14T01:59:00Z\" }</code>:</p> <pre><code>\"wait_until\" : {\n  \"Type\": \"Wait\",\n  \"TimestampPath\": \"$.expirydate\",\n  \"Next\": \"NextState\"\n}\n</code></pre> <p>A Wait State MUST contain exactly one of \"Seconds\", \"SecondsPath\", \"Timestamp\", or \"TimestampPath\".</p> <p>The Succeed State (identified by <code>\"Type\":\"Succeed\"</code>) either terminates a state machine successfully, ends a branch of a Parallel State, or ends an iteration of a Map State. The output of a Succeed State is the same as its input, possibly modified by \"InputPath\" and/or \"OutputPath\".</p> <p>The Succeed State is a useful target for Choice-State branches that don't do anything except terminate the machine.</p> <p>Here is an example:</p> <pre><code>\"SuccessState\": {\n  \"Type\": \"Succeed\"\n}\n</code></pre> <p>Because Succeed States are terminal states, they have no \"Next\" field.</p> <p>The Fail State (identified by <code>\"Type\":\"Fail\"</code>) terminates the machine and marks it as a failure.</p> <p>Here is an example:</p> <pre><code>\"FailState\": {\n  \"Type\": \"Fail\",\n  \"Error\": \"ErrorA\",\n  \"Cause\": \"Kaiju attack\"\n}\n</code></pre> <p>A Fail State MAY have a field named \"Error\", whose value MUST be a string whose value provides an error name that can be used for error handling (Retry/Catch), operational, or diagnostic purposes. A Fail State MAY have a field named \"Cause\", whose value MUST be a string whose value provides a human-readable message.</p> <p>Both the \"Error\" and \"Cause\" can be provided indirectly. A Fail state MAY have \"ErrorPath\" and \"CausePath\" fields whose values MUST be Reference Paths or Intrinsic Functions which, when resolved, MUST be string values. A Fail state MUST NOT include both \"Error\" and \"ErrorPath\" or both \"Cause\" and \"CausePath\".</p> <p>Here is an example that sets the Error and Cause values dynamically from the state input rather than using static values:</p> <pre><code>\"FailState\": {\n  \"Type\": \"Fail\",\n  \"Comment\": \"my error comment\",\n  \"ErrorPath\": \"$.Error\",\n  \"CausePath\": \"$.Cause\"\n}\n</code></pre> <p>Because Fail States are terminal states, they have no \"Next\" field.</p> <p>The Parallel State (identified by <code>\"Type\":\"Parallel\"</code>) causes parallel execution of \"branches\".</p> <p>Here is an example:</p> <pre><code>\"LookupCustomerInfo\": {\n  \"Type\": \"Parallel\",\n  \"Branches\": [\n    {\n      \"StartAt\": \"LookupAddress\",\n      \"States\": {\n        \"LookupAddress\": {\n          \"Type\": \"Task\",\n          \"Resource\":\n            \"arn:aws:lambda:us-east-1:123456789012:function:AddressFinder\",\n          \"End\": true\n        }\n      }\n    },\n    {\n      \"StartAt\": \"LookupPhone\",\n      \"States\": {\n        \"LookupPhone\": {\n          \"Type\": \"Task\",\n          \"Resource\":\n            \"arn:aws:lambda:us-east-1:123456789012:function:PhoneFinder\",\n          \"End\": true\n        }\n      }\n    }\n  ],\n  \"Next\": \"NextState\"\n}\n</code></pre> <p>A Parallel State causes the interpreter to execute each branch starting with the state named in its \"StartAt\" field, as concurrently as possible, and wait until each branch terminates (reaches a terminal state) before processing the Parallel State's \"Next\" field. In the above example, this means the interpreter waits for \"LookupAddress\" and \"LookupPhoneNumber\" to both finish before transitioning to \"NextState\".</p> <p>A Parallel State MUST contain a field named \"Branches\" which is an array whose elements MUST be objects. Each object MUST contain fields named \"States\" and \"StartAt\" whose meanings are exactly like those in the top level of a State Machine.</p> <p>A state in a Parallel State branch \"States\" field MUST NOT have a \"Next\" field that targets a field outside of that \"States\" field. A state MUST NOT have a \"Next\" field which matches a state name inside a Parallel State branch's \"States\" field unless it is also inside the same \"States\" field.</p> <p>Put another way, states in a branch's \"States\" field can transition only to each other, and no state outside of that \"States\" field can transition into it.</p> <p>If any branch fails, due to an unhandled error or by transitioning to a Fail State, the entire Parallel State is considered to have failed and all the branches are terminated. If the error is not handled by the Parallel State, the interpreter should terminate the machine execution with an error.</p> <p>Unlike a Fail State, a Succeed State within a Parallel merely terminates its own branch. A Succeed State passes its input through as its output, possibly modified by \"InputPath\" and \"OutputPath\".</p> <p>The Parallel State passes its input (potentially as filtered by the \"InputPath\" field) as the input to each branch's \"StartAt\" state. It generates a result which is an array with one element for each branch containing the output from that branch. The elements of the output array correspond to the branches in the same order that they appear in the \"Branches\" array. There is no requirement that all elements be of the same type.</p> <p>The output array can be inserted into the input data using the state's \"ResultPath\" field in the usual way.</p> Example of ResultPath <pre><code>\"FunWithMath\": {\n  \"Type\": \"Parallel\",\n  \"Branches\": [\n    {\n      \"StartAt\": \"Add\",\n      \"States\": {\n        \"Add\": {\n          \"Type\": \"Task\",\n          \"Resource\": \"arn:aws:states:::task:Add\",\n          \"End\": true\n        }\n      }\n    },\n    {\n      \"StartAt\": \"Subtract\",\n      \"States\": {\n        \"Subtract\": {\n          \"Type\": \"Task\",\n          \"Resource\": \"arn:aws:states:::task:Subtract\",\n          \"End\": true\n        }\n      }\n    }\n  ],\n  \"Next\": \"NextState\"\n}\n</code></pre> <p>If the \"FunWithMath\" state was given the JSON array <code>[3, 2]</code> as input, then both the \"Add\" and \"Subtract\" states would receive that array as input. The output of \"Add\" would be <code>5</code>, that of \"Subtract\" would be <code>1</code>, and the output of the Parallel State would be a JSON array:</p> <pre><code>[ 5, 1 ]\n</code></pre> <p>The Map State (identified by <code>\"Type\": \"Map\"</code>) causes the interpreter to process all the elements of an array, potentially in parallel, with the processing of each element independent of the others. This document uses the term \"iteration\" to describe each such nested execution.</p> <p>The Parallel State applies multiple different state-machine branches to the same input, while the Map State applies a single state machine to multiple input elements.</p> <p>There are several fields which may be used to control the execution. To summarize:</p> <ul> <li>The \"ItemProcessor\" (or \"Iterator\" which is now deprecated) field's value is an object that defines a state machine which will process each item or batch of items of the array.</li> <li>The \"ItemsPath\" field's value is a Reference Path identifying where in the effective input the array field is found.</li> <li>The \"ItemReader\" field's value is an object that specifies where to read the items instead of from the effective input.</li> <li>The \"ItemSelector\" (or \"Parameters\" which is now deprecated in Map) field's value is an object that overrides each single element of the item array.</li> <li>The \"ItemBatcher\" field's value is an object that specifies how to batch the items for the ItemProcessor.</li> <li>The \"ResultWriter\" field's value is an object that specifies where to write the results instead of to the Map state's result.</li> <li>The \"MaxConcurrency\" field's value is an integer that provides an upper bound on how many invocations of the Iterator may run in parallel.</li> <li>The \"ToleratedFailurePercentage\" field's value is an integer that provides an upper bound on the percentage of items that may fail.</li> <li>The \"ToleratedFailureCount\" field's value is an integer that provides an upper bound on how many items may fail.</li> </ul> <p>Consider the following example input data:</p> <pre><code>{\n  \"ship-date\": \"2016-03-14T01:59:00Z\",\n  \"detail\": {\n    \"delivery-partner\": \"UQS\",\n    \"shipped\": [\n      { \"prod\": \"R31\", \"dest-code\": 9511, \"quantity\": 1344 },\n      { \"prod\": \"S39\", \"dest-code\": 9511, \"quantity\": 40 },\n      { \"prod\": \"R31\", \"dest-code\": 9833, \"quantity\": 12 },\n      { \"prod\": \"R40\", \"dest-code\": 9860, \"quantity\": 887 },\n      { \"prod\": \"R40\", \"dest-code\": 9511, \"quantity\": 1220 }\n    ]\n  }\n}\n</code></pre> <p>Suppose it is desired to apply a single Lambda function, \"ship-val\", to each of the elements of the \"shipped\" array. Here is an example of an appropriate Map State.</p> <pre><code>\"Validate-All\": {\n  \"Type\": \"Map\",\n  \"InputPath\": \"$.detail\",\n  \"ItemsPath\": \"$.shipped\",\n  \"MaxConcurrency\": 0,\n  \"ItemProcessor\": {\n    \"StartAt\": \"Validate\",\n    \"States\": {\n      \"Validate\": {\n        \"Type\": \"Task\",\n        \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ship-val\",\n        \"End\": true\n      }\n    }\n  },\n  \"ResultPath\": \"$.detail.shipped\",\n  \"End\": true\n}\n</code></pre> <p>In the example above, the \"ship-val\" Lambda function will be executed once for each element of the \"shipped\" field. The input to one iteration will be:</p> <pre><code>{\n  \"prod\": \"R31\",\n  \"dest-code\": 9511,\n  \"quantity\": 1344\n}\n</code></pre> <p>Suppose that the \"ship-val\" function also needs access to the shipment's courier, which would be the same in each iteration. The \"ItemSelector\" field may be used to construct the raw input for each iteration:</p> <pre><code>\"Validate-All\": {\n  \"Type\": \"Map\",\n  \"InputPath\": \"$.detail\",\n  \"ItemsPath\": \"$.shipped\",\n  \"MaxConcurrency\": 0,\n  \"ItemSelector\": {\n    \"parcel.$\": \"$$.Map.Item.Value\",\n    \"courier.$\": \"$.delivery-partner\"\n  },\n  \"ItemProcessor\": {\n    \"StartAt\": \"Validate\",\n    \"States\": {\n      \"Validate\": {\n        \"Type\": \"Task\",\n        \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ship-val\",\n        \"End\": true\n      }\n    }\n  },\n  \"ResultPath\": \"$.detail.shipped\",\n  \"End\": true\n}\n</code></pre> <p>The \"ship-val\" Lambda function will be executed once for each element of the array selected by \"ItemsPath\". In the example above, the raw input to one iteration, as specified by \"ItemSelector\", will be:</p> <pre><code>{\n  \"parcel\": {\n    \"prod\": \"R31\",\n    \"dest-code\": 9511,\n    \"quantity\": 1344\n   },\n   \"courier\": \"UQS\"\n}\n</code></pre> <p>In the examples above, the ResultPath results in the output being the same as the input, with the \"detail.shipped\" field being overwritten by an array in which each element is the output of the \"ship-val\" Lambda function as applied to the corresponding input element.</p> Note Map State input/output processing <p>The \"InputPath\" field operates as usual, selecting part of the raw input - in the example, the value of the \"detail\" field - to serve as the effective input.</p> Reading Items <p>A Map State MAY have an \"ItemReader\" field, whose value MUST be a JSON object and is called the ItemReader Configuration. The ItemReader Configuration causes the interpreter to read items from the resource identified by the ItemReader Configuration's \"Resource\" field.</p> <p>The ItemReader Configuration MUST have a \"Resource\" field, whose value MUST be a URI that uniquely identifies the specific task to execute. The States language does not constrain the URI scheme nor any other part of the URI. The ItemReader Configuration MAY have a \"Parameters\" field, whose value MUST be a Payload Template. The ItemReader Configuration MAY have a \"ReaderConfig\" field whose value is a JSON object which MAY have a \"MaxItems\" field which MUST be a positive integer. The interpreter MAY define additional \"ReaderConfig\" fields.</p> <p>The ItemReader Configuration causes the interpreter to read items from the task identified by the ItemReader's \"Resource\" field. The interpreter will limit the number of items to the maximum number of items specified by the \"ReaderConfig\"'s \"MaxItems\" field. The \"MaxItems\" can be provided indirectly. A \"ReaderConfig\" field MAY have \"MaxItemsPath\" field which MUST be a Reference Path which, when resolved, MUST select a field whose value is a positive integer. A \"ReaderConfig\" field MUST NOT include both \"MaxItems\" and \"MaxItemsPath\".</p> <p>The default result for \"ItemReader\" is \"$\", which is to say the whole effective input.</p> Selecting Items <p>A Map State MAY have an \"ItemsPath\" field, whose value MUST be a Reference Path. The Reference Path is applied to the ItemReader result and MUST identify a field whose value is a JSON array.</p> <p>The default value of \"ItemsPath\" is \"$\", which is to say the whole ItemReader result. So, if a Map State has neither an \"InputPath\" nor an \"ItemReader\" nor an \"ItemsPath\" field, it is assuming that the raw input to the state will be a JSON array.</p> <p>A Map State MAY have an \"ItemSelector\" field, whose value MUST be a Payload Template. The interpreter uses the \"ItemSelector\" field to override each single element of the item array. The result of the \"ItemSelector\" field is called the selected item. The \"ItemSelector\" field performs the same function as the \"Parameters\" field, which is now deprecated in the Map State.</p> <p>The default of \"ItemSelector\" is a single element of the array field identified by the \"ItemsPath\" value.</p> Batching Items <p>A Map State MAY have an \"ItemBatcher\" field, whose value MUST be a JSON object and is called the ItemBatcher Configuration. The ItemBatcher Configuration causes the interpreter to batch selected items into sub-arrays before passing them to each invocation. The interpreter will limit each sub-array to the maximum number of items specified by the \"MaxItemsPerBatch\" field, and to the maximum size in bytes specified by the \"MaxInputBytesPerBatch\" field.</p> <p>The ItemBatcher Configuration MAY have a \"BatchInput\" field, whose value MUST be a Payload Template. An ItemBatcher Configuration MAY have a \"MaxItemsPerBatch\" field, whose value MUST be a positive integer. An ItemBatcher Configuration MAY have a \"MaxInputBytesPerBatch\" field, whose value MUST be a positive integer.</p> <p>The default of \"ItemBatcher\" is the selected item. Put another way, the interpreter will not batch items if no \"ItemBatcher\" field is provided.</p> <p>Both the \"MaxItemsPerBatch\" and \"MaxInputBytesPerBatch\" can be provided indirectly. A Map State may have \"MaxItemsPerBatchPath\" and \"MaxInputBytesPerBatchPath\" fields which MUST be Reference Paths which, when resolved, MUST select fields whose values are positive integers. A Map State MUST NOT include both \"MaxItemsPerBatch\" and \"MaxItemsPerBatchPath\" or both \"MaxInputBytesPerBatch\" and \"MaxInputBytesPerBatchPath\".</p> <p>An ItemBatcher Configuration MUST contain at least one of \"MaxItemsPerBatch\", \"MaxItemsPerBatchPath\", \"MaxInputBytesPerBatch\", or \"MaxInputBytesPerBatchPath\".</p> Processing Items <p>The input to each invocation, by default, is a single element of the   array field identified by the \"ItemsPath\" value, but may be overridden   using the \"ItemSelector\" and/or \"ItemBatcher\" fields. If present, the   interpreter uses the \"ItemSelector\" field to override each single element   of the array field identified by the \"ItemsPath\" value to produce an   array of selected items. If present, the interpreter then uses the   \"ItemBatcher\" field to specify how to batch the selected items array   to produce an array of batched selected items.</p> <p>For each item, within the Map State's \"ItemSelector\" (or deprecated \"Parameters\") field, the Context Object will have an object field named \"Map\" which contains an object field named \"Item\" which in turn contains an integer field named \"Index\" whose value is the (zero-based) array index being processed and a field named \"Value\", whose value is the array element being processed.</p> Writing Results <p>A Map State MAY have a \"ResultWriter\" field, whose value MUST be a JSON object and is called the ResultWriter Configuration. The ResultWriter Configuration causes the interpreter to write results to the resource identified by the ResultWriter's \"Resource\" field.</p> <p>The ResultWriter Configuration MUST have a \"Resource\" field, whose value MUST be a URI that uniquely identifies the specific task to execute. The States language does not constrain the URI scheme nor any other part of the URI. The ResultWriter Configuration MAY have a \"Parameters\" field, whose value MUST be a Payload Template.</p> <p>If a \"ResultWriter\" field is provided, a Map State's result is a JSON object containing fields defined by the interpreter. If a \"ResultWriter\" field is not specified, the result is a JSON array, whose value is either an array containing one element for each element of the ItemsPath input array, in the same order (if an \"ItemBatcher\" field is not specified), or an array containing one element for each batched sub-array of the ItemsPath input array, in the same order (if an \"ItemBatcher\" field is specified).</p> Map State concurrency <p>A Map State MAY have a non-negative integer \"MaxConcurrency\" field. Its default value is zero, which places no limit on invocation parallelism and requests the interpreter to execute the iterations as concurrently as possible.</p> <p>If \"MaxConcurrency\" has a non-zero value, the interpreter will not allow the number of concurrent iterations to exceed that value.</p> <p>A MaxConcurrency value of 1 is special, having the effect that interpreter will invoke the ItemProcessor once for each array element in the order of their appearance in the input, and will not start an iteration until the previous iteration has completed execution.</p> <p>The \"MaxConcurrency\" can be provided indirectly. A Map State may have \"MaxConcurrencyPath\" field which MUST be a Reference Path which, when resolved, MUST select a field whose value is a non-negative integer. A Map State MUST NOT include both \"MaxConcurrency\" and \"MaxConcurrencyPath\".</p> Map State ItemProcessor definition <p>A Map State MUST contain an object field named \"ItemProcessor\" (or deprecated \"Iterator\") which MUST contain fields named \"States\" and \"StartAt\", whose meanings are exactly like those in the top level of a State Machine. The \"ItemProcessor\" field performs the same function as the \"Iterator\" field, which is now deprecated in the Map State.</p> <p>The \"ItemProcessor\" field MAY contain a field named \"ProcessorConfig\", whose value MUST be a JSON object whose value is defined by the interpreter. The interpreter will execute the ItemProcessor according to the ProcessorConfig.</p> <p>A state in the \"States\" field of an \"ItemProcessor\" field MUST NOT have a \"Next\" field that targets a field outside of that \"States\" field. A state MUST NOT have a \"Next\" field which matches a state name inside an \"ItemProcessor\" field's \"States\" field unless it is also inside the same \"States\" field.</p> <p>Put another way, states in an ItemProcessor's \"States\" field can transition only to each other, and no state outside of that \"States\" field can transition into it.</p> <p>If any iteration fails, due to an unhandled error or by transitioning to a Fail state, the entire Map State is considered to have failed and all the iterations are terminated. If the error is not handled by the Map State, the interpreter should terminate the machine execution with an error.</p> <p>Unlike a Fail State, a Succeed State within a Map merely terminates its own iteration. A Succeed State passes its input through as its output, possibly modified by \"InputPath\" and \"OutputPath\".</p> Map State Failure Tolerance <p>A Map State MAY have a \"ToleratedFailurePercentage\" field whose value MUST be a number between zero and 100. Its default value is zero, which means the Map State will fail if any (i.e. more than 0%) of its items fail. A \"ToleratedFailurePercentage\" value of 100 means the interpreter will continue starting iterations even if all items fail.</p> <p>A Map State MAY have a non-negative integer \"ToleratedFailureCount\" field. Its default value is zero, which means the Map State will fail if at least 1 of its items fail. If a \"ToleratedFailurePercentage\" field and a \"ToleratedFailureCount\" field are both specified, the Map State will fail if either threshold is breached.</p> <p>Both the \"ToleratedFailureCount\" and \"ToleratedFailurePercentage\" can be provided indirectly. A Map State may have \"ToleratedFailureCountPath\" and \"ToleratedFailurePercentagePath\" fields which MUST be Reference Paths which, when resolved, MUST select fields whose values are non-negative integers. A Map State MUST NOT include both \"ToleratedFailureCount\" and \"ToleratedFailureCountPath\" or both \"ToleratedFailurePercentage\" and \"ToleratedFailurePercentagePath\".</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#appendices","title":"Appendices","text":""},{"location":"services/aws/step_functions/stfn-state-machine-language/#appendix-a-predefined-error-codes","title":"Appendix A: Predefined Error Codes","text":"Code Description States.ALL A wildcard which matches any Error Name. States.HeartbeatTimeout A Task State failed to heartbeat for a time longer than the \"HeartbeatSeconds\" value. States.Timeout A Task State either ran longer than the \"TimeoutSeconds\" value, or failed to heartbeat for a time longer than the \"HeartbeatSeconds\" value. States.TaskFailed A Task State failed during the execution. States.Permissions A Task State failed because it had insufficient privileges to execute the specified code. States.ResultPathMatchFailure A state's \"ResultPath\" field cannot be applied to the input the state received. States.ParameterPathFailure Within a state's \"Parameters\" field, the attempt to replace a field whose name ends in \".$\" using a Path failed. States.BranchFailed A branch of a Parallel State failed. States.NoChoiceMatched A Choice State failed to find a match for the condition field extracted from its input. States.IntrinsicFailure Within a Payload Template, the attempt to invoke an Intrinsic Function failed. States.ExceedToleratedFailureThreshold A Map state failed because the number of failed items exceeded the configured tolerated failure threshold. States.ItemReaderFailed A Map state failed to read all items as specified by the \"ItemReader\" field. States.ResultWriterFailed A Map state failed to write all results as specified by the \"ResultWriter\" field."},{"location":"services/aws/step_functions/stfn-state-machine-language/#appendix-b-list-of-intrinsic-functions","title":"Appendix B: List of Intrinsic Functions","text":"States.Format <p>This Intrinsic Function takes one or more arguments. The Value of the first MUST be a string, which MAY include zero or more instances of the character sequence <code>{}</code>. There MUST be as many remaining arguments in the Intrinsic Function as there are occurrences of <code>{}</code>. The interpreter returns the first-argument string with each <code>{}</code> replaced by the Value of the positionally-corresponding argument in the Intrinsic Function.</p> <p>If necessary, the <code>{</code> and <code>}</code> characters can be escaped respectively as <code>\\\\{</code> and <code>\\\\}</code>.</p> <p>If the argument is a Path, applying it to the input MUST yield a value that is a string, a boolean, a number, or <code>null</code>. In each case, the Value is the natural string representation; string values are not accompanied by enclosing <code>\"</code> characters. The Value MUST NOT be a JSON array or object.</p> <p>For example, given the following Payload Template:</p> <pre><code>{\n  \"Parameters\": {\n    \"foo.$\": \"States.Format('Your name is {}, we are in the year {}', $.name, 2020)\"\n  }\n}\n</code></pre> InputOutput <p>Suppose the input to the Payload Template is as follows:</p> <pre><code>{\n  \"name\": \"Foo\",\n  \"zebra\": \"stripe\"\n}\n</code></pre> <p>After processing the Payload Template, the new payload becomes:</p> <pre><code>{\n  \"foo\": \"Your name is Foo, we are in the year 2020\"\n}\n</code></pre> States.StringToJson <p>This Intrinsic Function takes a single argument whose Value MUST be a string. The interpreter applies a JSON parser to the Value and returns its parsed JSON form.</p> <p>For example, given the following Payload Template:</p> <pre><code>{\n  \"Parameters\": {\n    \"foo.$\": \"States.StringToJson($.someString)\"\n  }\n}\n</code></pre> InputOutput <p>Suppose the input to the Payload Template is as follows:</p> <pre><code>{\n  \"someString\": \"{\\\"number\\\": 20}\",\n  \"zebra\": \"stripe\"\n}\n</code></pre> <p>After processing the Payload Template, the new payload becomes:</p> <pre><code>{\n  \"foo\": {\n    \"number\": 20\n  }\n}\n</code></pre> States.JsonToString <p>This Intrinsic Function takes a single argument which MUST be a Path. The interpreter returns a string which is a JSON text representing the data identified by the Path.</p> <p>For example, given the following Payload Template:</p> <pre><code>{\n  \"Parameters\": {\n    \"foo.$\": \"States.JsonToString($.someJson)\"\n  }\n}\n</code></pre> InputOutput <p>Suppose the input to the Payload Template is as follows:</p> <pre><code>{\n  \"someJson\": {\n    \"name\": \"Foo\",\n    \"year\": 2020\n  },\n  \"zebra\": \"stripe\"\n}\n</code></pre> <p>After processing the Payload Template, the new payload becomes:</p> <pre><code>{\n  \"foo\": \"{\\\"name\\\":\\\"Foo\\\",\\\"year\\\":2020}\"\n}\n</code></pre> States.Array <p>This Intrinsic Function takes zero or more arguments. The interpreter returns a JSON array containing the Values of the arguments, in the order provided.</p> <p>For example, given the following Payload Template:</p> <pre><code>{\n  \"Parameters\": {\n    \"foo.$\": \"States.Array('Foo', 2020, $.someJson, null)\"\n  }\n}\n</code></pre> InputOutput <p>Suppose the input to the Payload Template is as follows:</p> <pre><code>{\n  \"someJson\": {\n    \"random\": \"abcdefg\"\n  },\n  \"zebra\": \"stripe\"\n}\n</code></pre> <p>After processing the Payload Template, the new payload becomes:</p> <pre><code>{\n  \"foo\": [\n    \"Foo\",\n    2020,\n    {\n      \"random\": \"abcdefg\"\n    },\n    null\n  ]\n}\n</code></pre> States.ArrayPartition <p>Use the <code>States.ArrayPartition</code> intrinsic function to partition a large array. You can also use this intrinsic to slice the data and then send the payload in smaller chunks.</p> <p>This intrinsic function takes two arguments. The first argument is an array, while the second argument defines the chunk size. The interpreter chunks the input array into multiple arrays of the size specified by chunk size. The length of the last array chunk may be less than the length of the previous array chunks if the number of remaining items in the array is smaller than the chunk size.</p> <p>Input validation</p> <ul> <li>You must specify an array as the input value for the function's first argument.</li> <li>You must specify a non-zero, positive integer for the second argument representing   the chunk size value.</li> <li>The input array can't exceed Step Functions' payload size limit of 256 KB.</li> </ul> <p>For example, given the following input array:</p> <pre><code>{\"inputArray\": [1,2,3,4,5,6,7,8,9] }\n</code></pre> <p>You could use the States.ArrayPartition function to divide the array into chunks of four values:</p> <pre><code>\"inputArray.$\": \"States.ArrayPartition($.inputArray,4)\"\n</code></pre> <p>Which would return the following array chunks:</p> <pre><code>{\"inputArray\": [ [1,2,3,4], [5,6,7,8], [9]] }\n</code></pre> <p>In the previous example, the States.ArrayPartition function outputs three arrays. The first two arrays each contain four values, as defined by the chunk size. A third array contains the remaining value and is smaller than the defined chunk size.</p> States.ArrayContains <p>Use the <code>States.ArrayContains</code> intrinsic function to determine if a specific value is present in an array. For example, you can use this function to detect if there was an error in a <code>Map</code> state iteration.</p> <p>This intrinsic function takes two arguments. The first argument is an array, while the second argument is the value to be searched for within the array.</p> <p>Input validation:</p> <ul> <li>You must specify an array as the input value for function's first argument.</li> <li>You must specify a valid JSON object as the second argument.</li> <li>The input array can't exceed Step Functions' payload size limit of 256 KB.</li> </ul> <p>For example, given the following input array:</p> <pre><code>{ \"inputArray\": [1,2,3,4,5,6,7,8,9], \"lookingFor\": 5 }\n</code></pre> <p>You could use the States.ArrayContains function to find the lookingFor value within the inputArray:</p> <pre><code>\"contains.$\": \"States.ArrayContains($.inputArray, $.lookingFor)\"\n</code></pre> <p>Because the value stored in <code>lookingFor</code> is included in the inputArray, <code>States.ArrayContains</code> returns the following result:</p> <pre><code>{\"contains\": true }\n</code></pre> States.ArrayRange <p>Use the <code>States.ArrayRange</code> intrinsic function to create a new array containing a specific range of elements. The new array can contain up to 1000 elements.</p> <p>This function takes three arguments. The first argument is the first element of the new array, the second argument is the final element of the new array, and the third argument is the increment value between the elements in the new array.</p> <p>Input validation:</p> <ul> <li>You must specify integer values for all of the arguments.</li> <li>You must specify a non-zero value for the third argument.</li> <li>The newly generated array can't contain more than 1000 items.</li> </ul> <p>For example, the following use of the <code>States.ArrayRange</code> function will create an array with a first value of 1, a final value of 9, and values in between the first and final values increase by two for each item:</p> <pre><code>\"array.$\": \"States.ArrayRange(1, 9, 2)\"\n</code></pre> <p>Which would return the following array:</p> <pre><code>{\"array\": [1,3,5,7,9] }\n</code></pre> States.ArrayGetItem <p>This intrinsic function returns a specified index's value. This function takes two arguments. The first argument is an array of values and the second argument is the array index of the value to return.</p> <p>For example, use the following <code>inputArray</code> and <code>index</code> values:</p> <pre><code>{ \"inputArray\": [1,2,3,4,5,6,7,8,9], \"index\": 5 }\n</code></pre> <p>From these values, you can use the <code>States.ArrayGetItem</code> function to return the value in the <code>index</code> position 5 within the array:</p> <pre><code>\"item.$\": \"States.ArrayGetItem($.inputArray, $.index)\"\n</code></pre> <p>In this example, <code>States.ArrayGetItem</code> would return the following result:</p> <pre><code>{ \"item\": 6 }\n</code></pre> States.ArrayLength <p>The <code>States.ArrayLength</code> intrinsic function returns the length of an array. It has one argument, the array to return the length of.</p> <p>For example, given the following input array:</p> <pre><code>{ \"inputArray\": [1,2,3,4,5,6,7,8,9] }\n</code></pre> <p>You can use <code>States.ArrayLength</code> to return the length of <code>inputArray</code>:</p> <pre><code>\"length.$\": \"States.ArrayLength($.inputArray)\"\n</code></pre> <p>In this example, <code>States.ArrayLength</code> would return the following JSON object that represents the array length:</p> <pre><code>{ \"length\": 9 }\n</code></pre> States.ArrayUnique <p>The <code>States.ArrayUnique</code> intrinsic function removes duplicate values from an array and returns an array containing only unique elements. This function takes an array, which can be unsorted, as its sole argument.</p> <p>For example, the following <code>inputArray</code> contains a series of duplicate values:</p> <pre><code>{\"inputArray\": [1,2,3,3,3,3,3,3,4] }\n</code></pre> <p>You could use the <code>States.ArrayUnique</code> function as and specify the array you want to remove duplicate values from:</p> <pre><code>\"array.$\": \"States.ArrayUnique($.inputArray)\"\n</code></pre> <p>The <code>States.ArrayUnique</code> function would return the following array containing only unique elements, removing all duplicate values:</p> <pre><code>{\"array\": [1,2,3,4] }\n</code></pre> States.Base64Encode <p>Use the <code>States.Base64Encode</code> intrinsic function to encode data based on MIME Base64 encoding scheme. You can use this function to pass data to other AWS services without using an AWS Lambda function.</p> <p>This function takes a data string of up to 10,000 characters to encode as its only argument.</p> <p>For example, consider the following <code>input</code> string:</p> <pre><code>{\"input\": \"Data to encode\" }\n</code></pre> <p>You can use the <code>States.Base64Encode</code> function to encode the <code>input</code> string as a MIME Base64 string:</p> <pre><code>\"base64.$\": \"States.Base64Encode($.input)\"\n</code></pre> <p>The <code>States.Base64Encode</code> function returns the following encoded data in response:</p> <pre><code>{\"base64\": \"RGF0YSB0byBlbmNvZGU=\" }\n</code></pre> States.Base64Decode <p>Use the <code>States.Base64Decode</code> intrinsic function to decode data based on MIME Base64 decoding scheme. You can use this function to pass data to other AWS services without using a Lambda function.</p> <p>This function takes a Base64 encoded data string of up to 10,000 characters to decode as its only argument.</p> <p>For example, given the following input:</p> <pre><code>{\"base64\": \"RGF0YSB0byBlbmNvZGU=\" }\n</code></pre> <p>You can use the <code>States.Base64Decode</code> function to decode the base64 string to a human-readable string:</p> <pre><code>\"data.$\": \"States.Base64Decode($.base64)\"\n</code></pre> <p>The <code>States.Base64Decode</code> function would return the following decoded data in response:</p> <pre><code>{\"data\": \"Decoded data\" }\n</code></pre> States.Hash <p>Use the <code>States.Hash</code> intrinsic function to calculate the hash value of a given input. You can use this function to pass data to other AWS services without using a Lambda function.</p> <p>This function takes two arguments. The first argument is the data you want to calculate the hash value of. The second argument is the hashing algorithm to use to perform the hash calculation. The data you provide must be an object string containing 10,000 characters or less.</p> <p>The hashing algorithm you specify can be any of the following algorithms:</p> <ul> <li>MD5</li> <li>SHA-1</li> <li>SHA-256</li> <li>SHA-384</li> <li>SHA-512</li> </ul> <p>For example, you can use this function to calculate the hash value of the <code>Data</code> string using the specified <code>Algorithm</code>:</p> <pre><code>{ \"Data\": \"input data\", \"Algorithm\": \"SHA-1\" }\n</code></pre> <p>You can use the <code>States.Hash</code> function to calculate the hash value:</p> <pre><code>\"output.$\": \"States.Hash($.Data, $.Algorithm)\"\n</code></pre> <p>The <code>States.Hash</code> function returns the following hash value in response:</p> <pre><code>{\"output\": \"aaff4a450a104cd177d28d18d7485e8cae074b7\" }\n</code></pre> States.JsonMerge <p>Use the <code>States.JsonMerge</code> intrinsic function to merge two JSON objects into a single object. This function takes three arguments. The first two arguments are the JSON objects that you want to merge.The third argument is a boolean value of <code>false</code>. This boolean value determines if the deep merging mode is enabled.</p> <p>Currently, Step Functions only supports the shallow merging mode; therefore, you must specify the boolean value as <code>false</code>. In the shallow mode, if the same key exists in both JSON objects, the latter object's key overrides the same key in the first object. Additionally, objects nested within a JSON object aren't merged when you use shallow merging.</p> <p>For example, you can use the <code>States.JsonMerge</code> function to merge the following JSON arrays that share the key <code>a</code>.</p> <pre><code>{ \"json1\": { \"a\": {\"a1\": 1, \"a2\": 2}, \"b\": 2, }, \"json2\": { \"a\": {\"a3\": 1, \"a4\": 2}, \"c\": 3 } }\n</code></pre> <p>You can specify the json1 and json2 arrays as inputs in the <code>States.JasonMerge</code> function to merge them together:</p> <pre><code>\"output.$\": \"States.JsonMerge($.json1, $.json2, false)\"\n</code></pre> <p>The <code>States.JsonMerge</code> returns the following merged JSON object as result. In the merged JSON object <code>output</code>, the <code>json2</code> object's key a replaces the <code>json1</code> object's key <code>a</code>. Also, the nested object in <code>json1</code> object's key <code>a</code> is discarded because shallow mode doesn't support merging nested objects.</p> <pre><code>{ \"output\": { \"a\": {\"a3\": 1, \"a4\": 2}, \"b\": 2, \"c\": 3 } }\n</code></pre> States.MathRandom <p>Use the <code>States.MathRandom</code> intrinsic function to return a random number between the specified start and end number. For example, you can use this function to distribute a specific task between two or more resources.</p> <p>This function takes three arguments. The first argument is the start number, the second argument is the end number, and the last argument controls the seed value. The seed value argument is optional.</p> <p>If you use this function with the same seed value, it returns an identical number.</p> <p>Warning</p> <p>Because the <code>States.MathRandom</code> function doesn't return cryptographically secure random numbers, we recommend that you don't use it for security sensitive applications.</p> <p>Input validation:</p> <ul> <li>You must specify integer values for the start number and end number arguments.</li> </ul> <p>For example, to generate a random number from between one and 999, you can use the following input values:</p> <pre><code>{ \"start\": 1, \"end\": 999 }\n</code></pre> <p>To generate the random number, provide the <code>start</code> and <code>end</code> values to the <code>States.MathRandom</code> function:</p> <pre><code>\"random.$\": \"States.MathRandom($.start, $.end)\"\n</code></pre> <p>The <code>States.MathRandom</code> function returns the following random number as a response:</p> <pre><code>{\"random\": 456 }\n</code></pre> States.MathAdd <p>Use the <code>States.MathAdd</code> intrinsic function to return the sum of two numbers. For example, you can use this function to increment values inside a loop without invoking a Lambda function.</p> <p>Input validation:</p> <ul> <li>You must specify integer values for all the arguments.</li> </ul> <p>For example, you can use the following values to subtract one from 111:</p> <pre><code>{ \"value1\": 111, \"step\": -1 }\n</code></pre> <p>Then, use the <code>States.MathAdd</code> function defining <code>value1</code> as the starting value, and <code>step</code> as the value to increment <code>value1</code> by:</p> <pre><code>\"value1.$\": \"States.MathAdd($.value1, $.step)\"\n</code></pre> <p>The <code>States.MathAdd</code> function would return the following number in response:</p> <pre><code>{\"value1\": 110 }\n</code></pre> States.StringSplit <p>Use the <code>States.StringSplit</code> intrinsic function to split a string into an array of values. This function takes two arguments.The first argument is a string and the second argument is the delimiting character that the function will use to divide the string.</p> <p>For example, you can use <code>States.StringSplit</code> to divide the following <code>inputString</code>, which contains a series of comma separated values:</p> <pre><code>{ \"inputString\": \"1,2,3,4,5\", \"splitter\": \",\" }\n</code></pre> <p>Use the <code>States.StringSplit</code> function and define <code>inputString</code> as the first argument, and the delimiting character <code>splitter</code> as the second argument:</p> <pre><code>\"array.$\": \"States.StringSplit($.inputString, $.splitter)\"\n</code></pre> <p>The States.StringSplit function returns the following string array as result:</p> <pre><code>{\"array\": [\"1\",\"2\",\"3\",\"4\",\"5\"] }\n</code></pre> States.UUID <p>Use the <code>States.UUID</code> intrinsic function to return a version 4 universally unique identifier (v4 UUID) generated using random numbers. For example, you can use this function to call other AWS services or resources that need a UUID parameter or insert items in a DynamoDB table.</p> <p>The <code>States.UUID</code> function is called with no arguments specified:</p> <pre><code>\"uuid.$\": \"States.UUID()\"\n</code></pre> <p>The function returns a randomly generated UUID, as in the following example:</p> <pre><code>{\"uuid\": \"ca4c1140-dcc1-40cd-ad05-7b4aa23df4a8\" }\n</code></pre> <ol> <li> <p>Copyright \u00a9 2016 Amazon.com Inc. or Affiliates.</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this specification and associated documentation files (the \"specification\"), to use, copy, publish, and/or distribute, the Specification) subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies of the Specification.</p> <p>You may not modify, merge, sublicense, and/or sell copies of the Specification.</p> <p>THE SPECIFICATION IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SPECIFICATION OR THE USE OR OTHER DEALINGS IN THE SPECIFICATION.</p> <p>Any sample code included in the Specification, unless otherwise specified, is licensed under the Apache License, Version 2.0.\u00a0\u21a9</p> </li> </ol>"},{"location":"services/azure/","title":"Microsoft Azure","text":"<p>Designed by Microsoft in 2010, Microsoft Azure is one of the widely used cloud computing platforms. Azure provides a wide variety of services such as cloud storage, compute services, network services, cognitive services, databases, analytics, and IoT.</p> <p>It makes building, deploying, and managing applications very easy. All the Microsoft Azure fundamentals are also described for a better understanding of readers.</p>"},{"location":"services/azure/az-oauth/","title":"OAuth","text":"Access Scenarios"},{"location":"services/azure/az-oauth/#authentication-vs-authorization","title":"Authentication vs Authorization","text":"<p>Authentication and Authorization are the two terms used in the context of OAuth and API Security. They are used in conjunction with each other and both sound similar, but they refer to entirely different security processes.</p> <ul> <li>Authentication answers the question: Who are you?</li> <li>Authorization answers the question: What are you allowed to do?</li> </ul>"},{"location":"services/azure/az-oauth/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/az-oauth/#1-create-service-principle","title":"1) Create Service Principle","text":"<ul> <li>Go to App Registration  Click New registration</li> <li>On Authentication  Click Add a platform    Pass <code>http://localhost</code> to this field</li> <li>On Certificates &amp; secrets  Click Client secrets  Copy Client ID and Client Secret ID from this creation process</li> </ul>"},{"location":"services/azure/az-oauth/#2-get-authorization-code","title":"2) Get Authorization Code","text":"<pre><code>GET {tenant-id}/oauth2/v2.0/authorize HTTP/1.1\nHost: login.microsoftonline.com\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={client-id}&amp;\nredirect_uri={redirect-uri}&amp;\nresponse_type=code&amp;\nresponse_mode=query&amp;\nscope=offline_access {scopes}&amp;\naccess_type=offline\n</code></pre>"},{"location":"services/azure/az-oauth/#3-request-access-and-refresh-tokens","title":"3) Request Access and Refresh tokens","text":"<pre><code>POST {tenant-id}/oauth2/v2.0/token HTTP/1.1\nHost: login.microsoftonline.com\nContent-Type: application/x-www-form-urlencoded\n\ncode={authorization-code}&amp;\nclient_id={client-id}&amp;\nclient_secret={client-secret}&amp;\nredirect_uri={redirect-uri}&amp;\nscope=offline_access {scopes}&amp;\ngrant_type=authorization_code\n</code></pre>"},{"location":"services/azure/az-oauth/#31-re-generate-access-token","title":"3.1) Re-generate Access Token","text":"<pre><code>POST {tenant-id}/oauth2/v2.0/token HTTP/1.1\nHost: login.microsoftonline.com\nContent-Type: application/x-www-form-urlencoded\n\nrefresh_token={refresh-token}&amp;\nclient_id={client-id}&amp;\nclient_secret={client-secret}&amp;\nscope=offline_access {scopes}&amp;\ngrant_type=refresh_token\n</code></pre>"},{"location":"services/azure/az-oauth/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft -- Get access and refresh tokens</li> <li> Microsoft -- identity platform and OAuth 2.0 authorization code flow</li> <li> Microsoft Graph -- Permissions Reference</li> </ul>"},{"location":"services/azure/az-storage-account/","title":"Azure Storage Account","text":""},{"location":"services/azure/az-storage-account/#fqdn","title":"FQDN","text":"<ul> <li>Blob: <code>https://{storage-account-name}.blob.core.windows.net/</code></li> <li>File: <code>https://{storage-account-name}.file.core.windows.net/</code></li> <li>Queue: <code>https://{storage-account-name}.queue.core.windows.net/</code></li> <li>Table: <code>https://{storage-account-name}.table.core.windows.net/</code></li> <li>DataLake: <code>https://{storage-account-name}.dfs.core.windows.net/</code></li> <li>Static Web: <code>https://{storage-account-name}.z23.web.core.windows.net/</code></li> </ul>"},{"location":"services/azure/az-storage-account/#storage-queue","title":"Storage Queue","text":""},{"location":"services/azure/az-storage-account/#examples","title":"Examples","text":"<ul> <li> Azure SDK Python - Storage Queue Sample</li> <li> Azure Storage Queue Samples</li> </ul>"},{"location":"services/azure/batch/","title":"Azure Batch","text":"<p>Azure Batch is a robust service that provides parallel batch processing to execute intensive workloads of varying size of Azure Compute. It creates a pool of compute nodes (Virtual Machines) to tackle heavy loads.</p> Azure Batch Architecture <ul> <li>https://github.com/baptisteohanes/Demo_AzureBatch/blob/master/python_tutorial_client.py</li> </ul> <p>The following steps explain the Azure Batch workflow scenario:</p> <ul> <li>The relevant data which needs to be processed by the task is uploaded to Azure storage.   From Azure storage, the files are downloaded to the compute nodes to run the tasks.</li> <li>The setup is then completed to configure the pool of compute nodes. As the   workload application is present in the compute node, the data needs to reach   the compute node for application to process it.   After processing the data, it is sent back to the storage.</li> <li>The job is set up and configured to manage a collection of tasks/work items.</li> <li>The task is then set up, configured, and assigned a job. The task defines the   computational work to be done which may consist of retrieving the files stored   in the Microsoft Azure Cloud storage.</li> </ul>"},{"location":"services/azure/batch/#typical-workload-applications-use-cases","title":"Typical Workload Applications / Use Cases","text":"Cloud-aware application <p>Consider this use case based on a sample project in GitHub (Ref1)*. In this instance, the application is aware of the existence of Cloud and can interact with it.</p> <p></p> <p>The block diagram above is an extension of the detailed diagram represented in the Azure batch processing section above. The overall execution process is the same, except one special attribute that the workload application is aware of the Cloud. It is capable to execute read and write operations on Cloud storage.</p> Legacy application <p>This includes standalone applications that do not include Cloud as part of the overall architecture or applications which organizations implemented before Cloud became mainstream.</p> <p></p> <p>Let\u2019s review an example of an application which is not aware of the cloud. Consider this sample project (Ref 2)where an application takes MP4 files from the filesystem and converts them into an AVI format using a FFmpeg tool (which is not Cloud aware) and then saves them to the filesystem.</p> <p>The data processing remains same as mentioned in Azure batch processing with the only difference being the tasks act as the intermediate between I/O container and workload application present in the compute node. Azure Batch has the capability to push the file from Cloud storage to the node before starting the task, and after completion of the task, push it back to Cloud storage.</p>"},{"location":"services/azure/batch/#create-batch-pool","title":"Create Batch Pool","text":"<pre><code>{\n  \"properties\": {\n    \"vmSize\": \"STANDARD_D2a_V4\",\n    \"deploymentConfiguration\": {\n      \"virtualMachineConfiguration\": {\n        \"imageReference\": {\n          \"publisher\": \"canonical\",\n          \"offer\": \"0001-com-ubuntu-server-jammy\",\n          \"sku\": \"22_04-lts\",\n          \"version\": \"latest\"\n        },\n        \"nodeAgentSKUId\": \"batch.node.ubuntu 22.04\"\n      }\n    },\n    \"scaleSettings\": {\n      \"autoScale\": {\n        \"evaluationInterval\": \"PT5M\",\n        \"formula\": \"samples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 5);\\r\\ncappedPoolSize = 1;\\r\\nAvgActiveTask = samples&lt; 70 ? max(0,$ActiveTasks.GetSample(1)) : avg($ActiveTasks.GetSample(1 * TimeInterval_Minute, 2 * TimeInterval_Minute));\\r\\nAvgRunningTask = samples&lt; 70 ? max(0,$RunningTasks.GetSample(1)) : avg($RunningTasks.GetSample(1 * TimeInterval_Minute, 10 * TimeInterval_Minute));\\r\\n$TargetDedicatedNodes = 0;\\r\\nActiveTask = AvgActiveTask &gt; 0 ? 1 : 0;\\r\\nRunningTask = AvgRunningTask &gt; 0 ? 1 : 0;\\r\\n$TargetLowPriorityNodes = min(cappedPoolSize,max(ActiveTask,RunningTask));\\r\\n// Set node deallocation mode - keep nodes active only until tasks finish\\r\\n$NodeDeallocationOption = taskcompletion;\"\n      }\n    },\n    \"startTask\": {\n      \"commandLine\": \"/bin/bash -c \\\"echo 'Set Python 3.10' &amp;&amp; sudo update-alternatives --set python3 /usr/bin/python3.10 || echo 'Skipped: Set Python 3.10' &amp;&amp; echo '########## Add PPA Repository ##########' &amp;&amp; sudo apt update &amp;&amp; sudo add-apt-repository ppa:deadsnakes/ppa || echo 'Skipped: Add Repository' &amp;&amp; echo '########## Install Python V3.8 ##########' &amp;&amp; sudo apt -y install python3.8 || echo 'Skipped: Install Python 3.8' &amp;&amp; sudo apt -y install python3.8-dev &amp;&amp; sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2 &amp;&amp; sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 &amp;&amp; sudo update-alternatives --set python3 /usr/bin/python3.8 &amp;&amp; python3 --version &amp;&amp; sudo apt -y install python3-pip &amp;&amp; sudo apt -y install python3.8-distutils &amp;&amp; python3 -m pip install --upgrade pip &amp;&amp; echo '########## Setting Others Configuration ##########' &amp;&amp; sudo curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - &amp;&amp; sudo curl https://packages.microsoft.com/config/ubuntu/22.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list &amp;&amp; sudo ACCEPT_EULA=Y apt install -y msodbcsql17 &amp;&amp; sudo ACCEPT_EULA=Y apt install -y mssql-tools &amp;&amp; echo 'export PATH=\\\\\\\"$PATH:/opt/mssql-tools/bin\\\\\\\"' &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc &amp;&amp; sudo apt -y install unixodbc-dev &amp;&amp; echo '########## Start Install Python Library ##########' &amp;&amp; pip3 install azure-core==1.17.0 &amp;&amp; pip3 install azure-storage-blob==12.8.1 &amp;&amp; pip3 install networkx==2.5 &amp;&amp; pip3 install numpy==1.19.5 &amp;&amp; pip3 install pandas==1.1.3 &amp;&amp; pip3 install pyarrow==1.0.1 &amp;&amp; pip3 install pyodbc==4.0.35 &amp;&amp; pip3 install pythainlp==2.3.0 &amp;&amp; pip3 install rapidfuzz==1.3.3 &amp;&amp; pip3 install scikit-learn==0.24.1 &amp;&amp; pip3 install scipy==1.6.0 &amp;&amp; pip3 install torch==1.7.1 &amp;&amp; pip3 install tqdm==4.58.0 &amp;&amp; pip3 install azure-keyvault-secrets==4.3.0 &amp;&amp; pip3 install azure-identity==1.6.1 &amp;&amp; pip3 install cffi==1.14.6 &amp;&amp; pip install azure-storage-file-datalake==12.4.0 &amp;&amp; pip install duckdb==0.2.9 &amp;&amp; pip install Office365-REST-Python-Client==2.3.8 &amp;&amp; pip install openpyxl==3.0.9 &amp;&amp; pip install xlsxwriter &amp;&amp; pip install xlrd==1.2.0 &amp;&amp; pip install pytz==2021.1\\\"\",\n      \"userIdentity\": {\n        \"autoUser\": {\n          \"scope\": \"Pool\",\n          \"elevationLevel\": \"Admin\"\n        }\n      },\n      \"maxTaskRetryCount\": 1,\n      \"waitForSuccess\": true\n    },\n    \"taskSlotsPerNode\": 2,\n    \"taskSchedulingPolicy\": {\n      \"nodeFillType\": \"pack\"\n    }\n  },\n  \"identity\": {\n    \"type\": \"UserAssigned\",\n    \"userAssignedIdentities\": {\n      \"/subscriptions/{tenant-id}/resourceGroups/{resource-group-name}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{managed-id-name}\": {}\n    }\n  }\n}\n</code></pre>"},{"location":"services/azure/batch/#python","title":"Python","text":"<ul> <li>https://github.com/uglide/azure-content/blob/master/articles/batch/batch-python-tutorial.md</li> </ul>"},{"location":"services/azure/batch/#read-mores","title":"Read Mores","text":"<ul> <li> Running heavy workloads using Azure Batch Processing</li> <li> Azure Batch cloud scale containers for HPC</li> </ul>"},{"location":"services/azure/batch/az-ba-auto-scalable/","title":"Auto Scalable","text":"<p>Autoscale is not (currently) intended as a sub 1M response to changes but rather to adjust the size of your pool gradually as you run a workload.</p> <p>Quote</p> <p>Batch uses your formula to determine the target number of compute nodes in the pool for the next interval of processing.</p> <p>Since we only evaluate the formula every ~15m it is not like your pool is going to immediately respond to new task pressure if left to its own devices. On the other hand if you are running a long-lived pool, and you want to respond to changes over the course of a day (for example day/night discrepancies in task load) then autoscale is a good fit .</p>"},{"location":"services/azure/batch/az-ba-auto-scalable/#examples","title":"Examples","text":"<p>Simaple re-scale dedicate node (<code>$TargetDedicated</code>) that base on running and active tasks that exists on the Pool,</p> <pre><code>$NodeDeallocationOption = taskcompletion;\n\nminNodes = 1;\nmaxNodes = 10;\n\nactiveTasks = $ActiveTasks.GetSample(1);\nrunningTasks = $RunningTasks.GetSample(1);\n\ntotalTasks = activeTasks + runningTasks;\n\nnodes = min(max(minNodes, totalTasks), maxNodes);\n$TargetDedicated = nodes;\n</code></pre> <p>Note</p> <p>If you want to scale low-priority node, you have change variable to <code>$TargetLowPriorityNodes</code></p>"},{"location":"services/azure/batch/az-ba-auto-scalable/#auto-scale-target-low-priority-nodes","title":"Auto Scale Target Low-Priority Nodes","text":"<p>Set autoscale evaluation interval to <code>15m 00s</code>.</p> Using Working Hour and WeekdayUse Running Task Sample <pre><code>// Fix dedicate target node to zero value\n$TargetDedicatedNodes = 0;\n\n// Get pending tasks for the past 15 minutes.\nsamples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 15);\n\n// Catch current value of low-priority node.\nCurr_TargetLowPriorityNodes = $TargetLowPriorityNodes;\n\n// Get current time with timezone that add 7 hours.\n$CurTime = time() + 7 * TimeInterval_Hour;\n\n// Set working hours (8 - 19) and weekday (Mon - Fri) flag.\n$WorkHours = $CurTime.hour &gt;= 8 &amp;&amp; $CurTime.hour &lt; 20;\n$IsWeekday = $CurTime.weekday &gt;= 1 &amp;&amp; $CurTime.weekday &lt;= 5;\n$IsWorkingWeekdayHour = $WorkHours &amp;&amp; $IsWeekday;\n\n// If we have less than 70% data points, we use the last sample point,\n// otherwise we use the average of last sample point between 1 and 2 minutes.\nAvgActiveTask = samples &lt; 70 ? max(0, $ActiveTasks.GetSample(1)) : avg($ActiveTasks.GetSample(1 * TimeInterval_Minute, 2 * TimeInterval_Minute));\n\n// Fix capacity of the pool sizes for low-priority node.\nCapped_TargetLowPriorityNodes = 1;\n\n// Set calculation low-priority target node value by the minimum value of\n// this capacity and average task value.\nCal_TargetLowPriorityNodes = min(Capped_TargetLowPriorityNodes, AvgActiveTask);\n\n// Set low-priority target node.\n$TargetLowPriorityNodes = $IsWorkingWeekdayHour ? max(Cal_TargetLowPriorityNodes, Curr_TargetLowPriorityNodes) : 0;\n\n// Set node de-allocation mode - keep nodes active only until tasks finish.\n$NodeDeallocationOption = taskcompletion;\n</code></pre> <pre><code>// Fix dedicate target node to zero value\n$TargetDedicatedNodes = 0;\n\n// Get pending tasks for the past 5 minutes.\nSamples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 5);\n\n// Fix capacity of the pool sizes\nCapped_PoolSize = 1;\n\n// If we have less than 70% data points, we use the last sample point,\n// otherwise we use the average of last sample point between 1 and 2 minutes.\n// (for running task we use last sample point between 1 and 10 minutes).\nAvgActiveTask = Samples &lt; 70 ? max(0, $ActiveTasks.GetSample(1)) : avg($ActiveTasks.GetSample(1 * TimeInterval_Minute, 2 * TimeInterval_Minute));\nAvgRunningTask = Samples &lt; 70 ? max(0, $RunningTasks.GetSample(1)) : avg($RunningTasks.GetSample(1 * TimeInterval_Minute, 10 * TimeInterval_Minute));\n\nActiveTask = AvgActiveTask &gt; 0 ? 1 : 0;\nRunningTask = AvgRunningTask &gt; 0 ? 1 : 0;\n\n// Set low-priority target node by the minimum value\n$TargetLowPriorityNodes = min(Capped_PoolSize, max(ActiveTask, RunningTask));\n\n// Set node de-allocation mode - keep nodes active only until tasks finish\n$NodeDeallocationOption = taskcompletion;\n</code></pre>"},{"location":"services/azure/batch/az-ba-auto-scalable/#auto-scale-target-dedicated-nodes","title":"Auto Scale Target Dedicated Nodes","text":"Using Active and Running Task SampleUsing Working Hour and WeekdayUsing Pending Task SampleUsing Max Task Per NodeUsing Working Hour and Weekday with Minimum <pre><code>// Get the average of last sample point between 1 and 2 minutes.\nAvgActiveTask = $ActiveTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second);\nAvgRunningTask = $RunningTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second);\n\n// Combine the average of active and running tasks together.\nTotalTasksVector = AvgActiveTask + AvgRunningTask;\n\nvmsRequiredVector = TotalTasksVector / 4;\nvmsRequired = avg(vmsRequiredVector);\n\n// Set dedicated target node by the minimum value from the vsm required\n// value and capacity of 60 nodes.\n$TargetDedicated = min(max(vmsRequired, 1), 60);\n</code></pre> <p>Warning</p> <p>We strongly recommend you avoid of using <code>GetSample(1)</code> in your autoscale formulas. \\ This is because <code>GetSample(1)</code> is just saying \"give me the last sample you have, no matter how long ago you got it\"  \\ Since you will use these samples to grow/shrink your pool (and your pool costs you money) we recommend that you base the formula on more than 1 samples worth of data. Instead, we suggest you do some trending type analysis and grow your pool based on that.</p> <p>In addition to the best practices comments that mentioned above, you have hit 2 different bugs:</p> <ul> <li> <p>The last 1-2 samples of <code>$RunningTasks</code> are almost always <code>0</code>, so <code>.GetSamples(1)</code>   on <code>RunningTasks</code> often will return <code>0</code> even if there are some running tasks.   We will investigate this and work on a fix, but in the meantime adhering to the   best practice of avoiding <code>GetSamples(1)</code> will help you avoid this issue</p> </li> <li> <p>This one is more painful -- right now there is a bug where even when <code>MultipleTasksPerVM</code>   is not set to the default of 1, <code>$RunningTasks</code> will only report 1 running task   (even though there may be up to <code>N</code>, where <code>N == MaxTasksPerVM</code>).</p> </li> </ul> <p>We're already tracking this bug in our backlog and will get to it ASAP.</p> <p>In the meantime, you can probably edit your formula to think of <code>$RunningTasks</code>   as <code>RunningVMs</code> instead -- which should be able to get you close to what you   want.</p> <p>See the following formula for an example of what I mean:</p> <pre><code>AvgActiveTask = $ActiveTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second);\nAvgRunningTask = $RunningTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second);\n\nvmsRequiredVector = AvgActiveTask / 4 + AvgRunningTask;\nvmsRequired = avg(vmsRequiredVector);\n\n// Set dedicated target node by the minimum value from the vsm required\n// value and capacity of 60 nodes.\n$TargetDedicated = min(max(vmsRequired, 1), 60);\n</code></pre> <p>Note</p> <p>You can use the <code>.GetSample(Interval Lookback Start, Interval Lookback End)</code> API to get a vector of samples, for example:</p> <pre><code>AvgRunningTask = $RunningTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second);\n</code></pre> <p>Might return:</p> <pre><code>AvgRunningTask = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1];\n</code></pre> <p>Or, if you would like more certainty, you can force the evaluation to fail if there are less than a certain percentage of samples (here percentage means that if in a given time interval there were supposed to be <code>60</code> samples, but actually due to networking failures or other issues we were only able to gather 30 samples, the percentage would be 50%).</p> <p>This is how you specify a percentage (note the <code>60</code> as the 3rd parameter):</p> <pre><code>AvgRunningTask = $RunningTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second, 60);\n</code></pre> <p>When specifying a time range, always start with the time range starting at least 1m ago, since it takes about 1m for samples to propagate through the system, so samples in the range <code>(0 * TimeInterval_Second, 60 * TimeInterval_Second)</code> will often not be available. Again you can use the percentage API to force a particular sample percentage.</p> <p>Read More: Autoscale Formula Improvements Needed</p> <pre><code>// Get current time with timezone that add 7 hours.\n$CurTime = time() + 7 * TimeInterval_Hour;\n\n// Set working hours (8 - 17) and weekday (Mon - Fri) flag.\n$WorkHours = $CurTime.hour &gt;= 8 &amp;&amp; $CurTime.hour &lt; 18;\n$IsWeekday = $CurTime.weekday &gt;= 1 &amp;&amp; $CurTime.weekday &lt;= 5;\n$IsWorkingWeekdayHour = $WorkHours &amp;&amp; $IsWeekday;\n\n// Set dedicated target node to 20 if datetime in range else 10.\n$TargetDedicated = $IsWorkingWeekdayHour ? 20 : 10;\n</code></pre> <pre><code>// Get pending tasks for the past 15 minutes.\nSamples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 15);\n\n// If we have less than 70% data points, we use the last sample point,\n// otherwise we use the maximum of last sample point and the history average.\n$Tasks = Samples &lt; 70 ? max(0, $ActiveTasks.GetSample(1)) : max($ActiveTasks.GetSample(1), avg($ActiveTasks.GetSample(TimeInterval_Minute * 15)));\n\n// If number of pending tasks is not 0, set target node to pending tasks,\n// otherwise half of current dedicated target node.\n$TargetVMs = $Tasks &gt; 0 ? $Tasks : max(0, $TargetDedicated / 2);\n\n// The pool size is capped at 20, if target node value is more than that,\n// set it to 20. This value should be adjusted according to your use case.\n$TargetDedicated = max(0, min($TargetVMs, 20));\n\n// Set node de-allocation mode - keep nodes active only until tasks finish\n$NodeDeallocationOption = taskcompletion;\n</code></pre> <p>Another example that adjusts the pool size based on the number of tasks, this formula also takes into account the <code>MaxTasksPerComputeNode</code> value that has been set for the pool. This is particularly useful in situations where parallel task execution on compute nodes is desired.</p> <pre><code>// Determine whether 70% of the samples have been recorded in the past 15 minutes.\n// If not, use last sample.\nSamples = $ActiveTasks.GetSamplePercent(TimeInterval_Minute * 15);\n$Tasks = Samples &lt; 70 ? max(0,$ActiveTasks.GetSample(1)) : max( $ActiveTasks.GetSample(1),avg($ActiveTasks.GetSample(TimeInterval_Minute * 15)));\n\n// Set the number of nodes to add to one-fourth the number of active tasks\n// (the MaxTasksPerComputeNode property on this pool is set to 4, adjust\n// this number for your use case)\n$Cores = $TargetDedicated * 4;\n$ExtraVMs = (($Tasks - $Cores) + 3) / 4;\n$TargetVMs = ($TargetDedicated + $ExtraVMs);\n\n// Attempt to grow the number of compute nodes to match the number of active\n// tasks, with a maximum of 3\n$TargetDedicated = max(0, min($TargetVMs, 3));\n\n// Set node de-allocation mode - keep nodes active only until tasks finish\n$NodeDeallocationOption = taskcompletion;\n</code></pre> <pre><code>// Set maximum node limit to 10.\n$MaxComputeNodeLimit = 10;\n\n// Get current time with timezone that add 7 hours.\n$CurTime = time() + 7 * TimeInterval_Hour;\n\n// Set working hours (8 - 17) and weekday (Mon - Fri) flag.\n$WorkHours = $CurTime.hour &gt;= 8 &amp;&amp; $CurTime.hour &lt; 18;\n$IsWeekday = $CurTime.weekday &gt;= 1 &amp;&amp; $CurTime.weekday &lt;= 5;\n$IsWorkingWeekdayHour = $WorkHours &amp;&amp; $IsWeekday;\n\n// Set minimum capacity of target node to 1 if datetime in range else 0\n$MinCapacity = $IsWorkingWeekdayHour ? 1 : 0;\n\n// Get the last sample point of active and running tasks.\n$LastSampledActiveTasks = $ActiveTasks.GetSample(1);\n$LastSampledRunningTasks = $RunningTasks.GetSample(1);\n$RunningAndWaiting = max($LastSampledActiveTasks, 1) + max($LastSampledRunningTasks, 1);\n\n$NeedCompute = $RunningAndWaiting &gt;= 1;\n\n$NodesToAllocate = $RunningAndWaiting &gt; $MaxComputeNodeLimit ? $MaxComputeNodeLimit : $RunningAndWaiting;\n$TargetDedicated = $NeedCompute ? $NodesToAllocate : $MinCapacity;\n</code></pre>"},{"location":"services/azure/batch/az-ba-auto-scalable/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft Azure Batch Automatic Scaling</li> <li> Azure Batch Pool Auto Scale Formulas</li> <li> Azure Content: Batch Automatic Scaling</li> </ul>"},{"location":"services/azure/batch/az-ba-run-pyspark/","title":"Run PySpark","text":"<p>You simply wrap your Spark code in a Docker container, and schedule 100 of those containers on a pool of 20 Azure Batch nodes, and let them execute the 100 jobs in parallel. You get to use Low Priority VMs out of the box, and you only pay for what you need. No long-running Spark or Hadoop clusters.</p> <p>In order to run this example, we need to set up:</p> <ul> <li>Azure Storage Accounts to read and write your data</li> <li>Azure Container Registry to store the Docker container</li> <li>Azure Batch Accounts to run your batch jobs in.</li> </ul>"},{"location":"services/azure/batch/az-ba-run-pyspark/#getting-started","title":"Getting Started","text":"<p>Chances are you already have these running in Azure. If not, it\u2019s simple click and install on the Azure Portal. For each of these 3 services, you need to get the Keys and add them to a <code>config.py</code>:</p> config.py<pre><code>STORAGE_ACCOUNT_NAME = \"&lt;storage-account-name&gt;\"\nSTORAGE_ACCOUNT_KEY = \"****\"\n\nBATCH_ACCOUNT_NAME = \"&lt;batch-account-name&gt;\"\nBATCH_ACCOUNT_KEY = \"****\"\nBATCH_ACCOUNT_URL = \"https://&lt;batch-account-name&gt;.westeurope.batch.azure.com\"\n\nACR_LOGINSERVER = \"&lt;registry-name&gt;.azurecr.io\"\nACR_USERNAME = \"&lt;registry-name&gt;\"\nACR_PASSWORD = \"****\"\n</code></pre>"},{"location":"services/azure/batch/az-ba-run-pyspark/#create-the-spark-job-script","title":"Create the Spark job script","text":"<p>For this demo, we need a simple spark job. Any job will do, really, The only thing you need to be aware of, is that it has to be able to read from Blob Storage.</p> requirements.txt<pre><code>azure\nazure-storage\nazure-storage-blob\npyspark==2.4.0\n</code></pre> <pre><code>import argparse\nimport config\nfrom pyspark.sql import SparkSession\n\ndef get_azure_spark_connection(storage_account_name, storage_account_key):\n    spark = (\n        SparkSession.builder\n            .config('spark.jars.packages', 'org.apache.hadoop:hadoop-azure:2.7.3')\n            .config('spark.hadoop.fs.azure', \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n            .config(f\"spark.hadoop.fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n            .appName(\"AzureSparkDemo\")\n            .getOrCreate()\n    )\n\n    (\n        spark.sparkContext._jsc\n            .hadoopConfiguration()\n            .set(\"fs.wasbs.impl\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n    )\n    return spark\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-i\", \"--input\", help=\"input file to parse\", type=str)\n    parser.add_argument(\"-o\", \"--output\", help=\"result file to write\", type=str)\n    args = parser.parse_args()\n    spark = get_azure_spark_connection(\n        storage_account_name=config.STORAGE_ACCOUNT_NAME,\n        storage_account_key=config.STORAGE_ACCOUNT_KEY\n    )\n    df = (\n        spark.read.option(\"header\", \"true\")\n            .option(\"delimiter\", \",\")\n            .option(\"inferSchema\", \"true\")\n            .csv(args.input)\n    )\n    df.registerTempTable(\"airlines\")\n    result = spark.sql(\"\"\"\n      SELECT Year,\n          Month,\n          DayOfMonth,\n          avg(ArrDelay) as avg_ArrDelay,\n          avg(DepDelay) as avg_DepDelay\n      FROM airlines\n      GROUP BY Year, Month, DayOfMonth\n    \"\"\")\n    (\n        result\n            .repartition(1)\n            .write\n            .mode(\"overwrite\")\n            .parquet(args.output)\n    )\n</code></pre>"},{"location":"services/azure/batch/az-ba-run-pyspark/#dockerize","title":"Dockerize","text":"<p>We wrap this job in a Docker container and push it to the Azure Container Registry (ACR).</p> <p>A simple <code>Dockerfile</code> can be as follows:</p> Dockerfile<pre><code>FROM python:3.6\n\n# Install OpenJDK 8, and monitoring tooling\nRUN \\\n  apt-get update &amp;&amp; \\\n  apt-get install -y openjdk-8-jdk htop bmon &amp;&amp; \\\n  rm -rf /var/lib/apt/lists/*\n\nRUN pip install --upgrade setuptools\nCOPY requirements.txt /\nRUN pip install -r /requirements.txt\n\nENV PATH=$PATH:/src\nENV PYTHONPATH /src\n\nADD ./ /src\nWORKDIR /src/\n</code></pre> <p>It starts from a <code>python3.6</code> base image, installs <code>openjdk</code> and some monitoring tooling, and then all the python dependencies as listed in <code>requirements.txt</code>. Finally, it copies your code to a <code>src</code> folder.</p> <p>You can build it with the command:</p> <pre><code>docker build -t dataminded/spark_on_azure_batch_demo .\n</code></pre> <p>Now you could already run this script locally as follows:</p> <pre><code>docker run dataminded/spark_on_azure_batch_demo python /src/airline_analytics.py \\\n  --input wasbs://demo@datamindeddata.blob.core.windows.net/raw/airlines/2007.csv.bz2 \\\n  --output wasbs://demo@datamindeddata.blob.core.windows.net/aggregated/airlines/2007.parquet\n</code></pre> <p>Next, you need to log in to the Azure Container Registry, with <code>&lt;registry-name&gt;</code> replaced by your own registry of course. Enter the admin username you see on the Azure portal:</p> <pre><code>$ docker login &lt;registry-name&gt;.azurecr.io\n$ docker tag dataminded/spark_on_azure_batch_demo:latest &lt;registry-name&gt;.azurecr.io/&lt;registry-name&gt;/spark_on_azure_batch_demo:latest\n$ docker push &lt;registry-name&gt;.azurecr.io/&lt;registry-name&gt;/spark_on_azure_batch_demo:latest\n</code></pre> <p>Note</p> <p>That the first time you push this Docker container, it will be huge, about 1GB: it has an ubuntu image, a bunch of spark libraries, a JDK, etc. The next push you do of this container, will only push your latest code changes.</p>"},{"location":"services/azure/batch/az-ba-run-pyspark/#run-on-azure-batch","title":"Run on Azure Batch","text":"<p>We\u2019ve written a sample Spark application, with the right Azure libraries. We\u2019ve dockerized it and uploaded it to Azure Container Registry. Now we\u2019re finally ready to run our code on Azure Batch. This is going to be quite a long python script, but it does a lot of things as well:</p> <ul> <li> <p>It connects to your container registry and uses those docker images</p> </li> <li> <p>Create a Pool if it doesn't exist yet. Here, you can configure which kind of   VMs and how many of them you want in your pool. And more importantly, you can   specify that it are Low Prio VMs, which are cheap.</p> </li> <li> <p>Create a Job within the Pool</p> </li> <li> <p>Create a separate task to process each year of data. In a real-life situation,   you would have a task for each day of data.</p> </li> </ul> <pre><code>import datetime\nimport sys\nimport time\nimport azure.batch.batch_auth as batch_auth\nimport azure.batch.batch_service_client as batch\nimport azure.batch.models as batchmodels\nimport config\n\nsys.path.append('docs')\nsys.path.append('')\n\nIMAGE_NAME = '&lt;registry-name&gt;.azurecr.io/&lt;registry-name&gt;/spark_on_azure_batch_demo'\nIMAGE_VERSION = 'latest'\n\n\ndef create_pool(\n        batch_service_client,\n        container_registry,\n        image_name,\n        pool_id,\n        pool_vm_size,\n        pool_node_count,\n        skip_if_exists = True\n):\n  print(f'Creating pool [{pool_id}]...')\n\n  container_conf = batch.models.ContainerConfiguration(\n    container_image_names=[image_name],\n    container_registries=[container_registry]\n  )\n\n  image_ref_to_use = batch.models.ImageReference(\n    publisher='microsoft-azure-batch',\n    offer='ubuntu-server-container',\n    sku='16-04-lts',\n    version='latest',\n  )\n\n  new_pool = batch.models.PoolAddParameter(\n    id=pool_id,\n    virtual_machine_configuration=batch.models.VirtualMachineConfiguration(\n      image_reference=image_ref_to_use,\n      container_configuration=container_conf,\n      node_agent_sku_id='batch.node.ubuntu 16.04',\n    ),\n    vm_size=pool_vm_size,\n    target_low_priority_nodes=pool_node_count)\n\n  if not skip_if_exists or not batch_service_client.pool.exists(pool_id):\n    batch_service_client.pool.add(new_pool)\n\n\ndef create_job(batch_service_client, job_id, pool_id):\n  print('Creating job [{}]...'.format(job_id))\n\n  job = batch.models.JobAddParameter(\n    id=job_id,\n    pool_info=batch.models.PoolInformation(pool_id=pool_id))\n\n  batch_service_client.job.add(job)\n\n\ndef add_task(\n    batch_service_client,\n    image_name,\n    image_version,\n    job_id,\n    command,\n    name\n):\n  user = batchmodels.UserIdentity(\n      auto_user=batchmodels.AutoUserSpecification(\n        elevation_level=batchmodels.ElevationLevel.admin,\n        scope=batchmodels.AutoUserScope.task,\n      )\n  )\n\n  task_id = name\n  task_container_settings = batch.models.TaskContainerSettings(\n    image_name=image_name + ':' + image_version,\n    container_run_options='--rm -p 4040:4040')\n  task = batch.models.TaskAddParameter(\n    id=task_id,\n    command_line=command,\n    container_settings=task_container_settings,\n    user_identity=user\n  )\n  print(\"running \" + command)\n\n  batch_service_client.task.add(job_id, task)\n\n\ndef wait_for_tasks_to_complete(batch_service_client, job_id, timeout):\n  timeout_expiration = datetime.datetime.now() + timeout\n\n  print(\"Monitoring all tasks for 'Completed' state, timeout in {}...\"\n        .format(timeout), end='')\n\n  while datetime.datetime.now() &lt; timeout_expiration:\n    print('.', end='')\n    sys.stdout.flush()\n    tasks = batch_service_client.task.list(job_id)\n\n    incomplete_tasks = [task for task in tasks if\n                        task.state != batchmodels.TaskState.completed]\n    if not incomplete_tasks:\n      print()\n      return True\n    else:\n      time.sleep(1)\n\n  print()\n  raise RuntimeError(\"ERROR: Tasks did not reach 'Completed' state within \"\n                     \"timeout period of \" + str(timeout))\n\n\ndef daterange(start_date, end_date):\n  for n in range(int((end_date - start_date).days)):\n    yield start_date + datetime.timedelta(n)\n\n\nif __name__ == '__main__':\n\n  start_time = datetime.datetime.now().replace(microsecond=0)\n  print('Start batch job {}'.format(start_time))\n  print()\n\n  image_name = IMAGE_NAME\n  image_version = IMAGE_VERSION\n\n  # Create the blob client, for use in obtaining references to\n  # blob storage containers and uploading files to containers.\n\n  credentials = batch_auth.SharedKeyCredentials(config.BATCH_ACCOUNT_NAME,\n                                                config.BATCH_ACCOUNT_KEY)\n\n  batch_client = batch.BatchServiceClient(credentials,\n                                          base_url=config.BATCH_ACCOUNT_URL)\n\n  job_id: str = f'Job-{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}'\n  pool_id: str = 'Airlines'\n\n  try:\n    container_registry = batch.models.ContainerRegistry(\n      registry_server=config.ACR_LOGINSERVER,\n      user_name=config.ACR_USERNAME,\n      password=config.ACR_PASSWORD)\n\n    create_pool(\n      batch_service_client=batch_client,\n      pool_id=pool_id,\n      container_registry=container_registry,\n      image_name=image_name,\n      pool_node_count=3,\n      pool_vm_size='Standard_E2s_v3',\n      skip_if_exists=True)\n\n    # Create the job that will run the tasks.\n    create_job(batch_client, job_id, pool_id)\n\n    for year in range(2001, 2009):\n      command = (\n        f\"python /src/airline_analytics.py \"\n        f\"--input wasbs://demo@datamindeddata.blob.core.windows.net/raw/airlines/{year}.csv.bz2\"\n        f\"--output wasbs://demo@datamindeddata.blob.core.windows.net/aggregated/airlines/{year}.parquet\"\n      )\n      add_task(\n        batch_service_client=batch_client,\n        image_name=image_name,\n        image_version=image_version,\n        job_id=job_id,\n        command=command,\n        name=f'airlines{year}'\n      )\n\n    # Pause execution until tasks reach Completed state.\n    wait_for_tasks_to_complete(\n      batch_client,\n      job_id,\n      datetime.timedelta(hours=2),\n    )\n    print(\n      \"Success! All tasks reached the 'Completed' state within the specified timeout period.\")\n\n  except batchmodels.BatchErrorException as err:\n    print(err)\n    raise\n\n  # Print out some timing info\n  end_time = datetime.datetime.now().replace(microsecond=0)\n  print()\n  print('Sample end: {}'.format(end_time))\n  print('Elapsed time: {}'.format(end_time - start_time))\n  print()\n</code></pre> <p>And finally, you will see your jobs run on Azure Batch:</p> <p></p> <p></p> <p>Warning</p> <p>That all nodes in a pool are completely separate. This is not a one big Spark cluster concept.</p>"},{"location":"services/azure/batch/az-ba-run-pyspark/#next-steps","title":"Next steps","text":"<p>This solves 90% of the needs of a project we\u2019re working on. However, there are always next steps:</p> <ul> <li>Security:</li> </ul> <p>Account keys are everywhere here. Maybe not in the git repo. But you are going   to fill them in and put them in a docker image. Which is a big No No. Better   would be to work with some sort of Role Based Access Control and a Vault,   where this job is allowed to access the vault for certain keys, and it is allowed   to read and write certain data from Blob storage</p> <ul> <li>Monitoring:</li> </ul> <p>The htop monitoring is nice, and you sure do look like a hardcore hacker with that CLI dashboard. But you would just like to see the Spark UI, really. Just like any normal human being.</p> <ul> <li>Scaling:</li> </ul> <p>Not all jobs fit on a single node. Although you can get up to 64 cores and 432GB (for not even $1 per hour in Low Prio), there might be use cases where you need more. Although, honestly, we all talk about big data a lot, but 99% of jobs that I\u2019ve seen, really don\u2019t need 432GB of RAM. But anyway, in those cases, AZTK is the way to go if you want to stick with Azure Batch.</p> <ul> <li>Auto scaling:</li> </ul> <p>Even better than scaling, we would also like to auto-scale our pool. Now it\u2019s just a fixed size, which you have to turn off manually. Don\u2019t forget, or your \u201cIt\u2019s cheap!\u201d claim won\u2019t survive for long. Ideally, you want to autoscale to dozens or even hundreds of nodes, if you have a large workload. And then you want them to automatically be turned off when they are idle for a couple of minutes.</p> <ul> <li>Kubernetes: \\   We are big believers of Kubernetes as the next platform for big data analytics, and we\u2019re looking into whether it\u2019s stable enough to run Spark workloads. Since Spark 2.4.0, which got released yesterday, it also supports pyspark. The nice thing with this setup, is that you are already halfway there. You\u2019ve wrapped your spark jobs in a docker container, and you put the container in a private registry. Now it\u2019s just a matter of choosing where to launch it.</li> </ul>"},{"location":"services/azure/batch/az-ba-run-pyspark/#references","title":"References","text":"<ul> <li>Medium: Run Spark on Azure Batch using Azure Container Registry</li> <li>GitHub: Spark on Azure Batch Demo</li> </ul>"},{"location":"services/azure/batch/az-ba-start-task/","title":"Start Task","text":""},{"location":"services/azure/batch/az-ba-start-task/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/batch/az-ba-start-task/#command-line","title":"Command Line","text":"<p>Below CMD will remove newline charactor that come from Windows first and then it will execute the start task file with <code>bash</code>.</p> <pre><code>/bin/bash -c \"sed -i 's/\\r$//' start_task.sh &amp;&amp; bash ./start_task.sh\"\n</code></pre>"},{"location":"services/azure/batch/az-ba-start-task/#start-task-file","title":"Start Task File","text":"Python 3.8 start_task.sh<pre><code>#!/bin/bash\n\necho 'Update Ubuntu' &amp;&amp;\nsudo apt update &amp;&amp;\necho 'Import Python 3.8 PPA on Ubuntu' &amp;&amp;\nsudo add-apt-repository ppa:deadsnakes/ppa -y &amp;&amp;\nsudo apt update &amp;&amp;\nsudo apt -y install python3.8 &amp;&amp;\nsudo update-alternatives --set python3 /usr/bin/python3.8 &amp;&amp;\npython3 --version\n</code></pre>"},{"location":"services/azure/batch/az-ba-start-task/#examples","title":"Examples","text":"ETL Python 3.8ETL Python 3.8 with UVODBC SQL Server start_task.sh<pre><code>#!/bin/bash\n\necho 'Set Python 3.10' &amp;&amp;\nsudo update-alternatives --set python3 /usr/bin/python3.10 || echo 'Skipped: Set Python 3.10' &amp;&amp;\necho '########## Add PPA Repository ##########' &amp;&amp;\nsudo apt update &amp;&amp;\nsudo add-apt-repository ppa:deadsnakes/ppa || echo 'Skipped: Add Repository' &amp;&amp;\necho '########## Install Python V3.8 ##########' &amp;&amp;\nsudo apt -y install python3.8 || echo 'Skipped: Install Python 3.8' &amp;&amp;\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2 &amp;&amp;\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 &amp;&amp;\nsudo update-alternatives --set python3 /usr/bin/python3.8 &amp;&amp;\npython3 --version &amp;&amp;\nsudo apt -y install python3-pip &amp;&amp;\nsudo apt -y install python3.8-distutils &amp;&amp;\npython3 -m pip install --upgrade pip &amp;&amp;\necho '########## Start Install Python Library ##########' &amp;&amp;\npip3 install backports.zoneinfo &amp;&amp;\npip3 install azure-storage-file-datalake==12.4.0 &amp;&amp;\npip3 install azure-keyvault-secrets==4.3.0 &amp;&amp;\npip3 install azure-identity==1.6.1 &amp;&amp;\npip3 install cffi==1.16.0 &amp;&amp;\npip install google-cloud-bigquery==3.13.0 &amp;&amp;\npip install pandas_gbq==0.17.0 &amp;&amp;\npip install pandas==2.0.3 &amp;&amp;\npip install pyarrow==14.0.1 &amp;&amp;\npip install -U pytz\n</code></pre> start_task.sh<pre><code>#!/bin/bash\n\necho 'Set Python 3.10' &amp;&amp;\nsudo update-alternatives --set python3 /usr/bin/python3.10 || echo 'Skipped: Set Python 3.10' &amp;&amp;\necho '########## Add PPA Repository ##########' &amp;&amp;\nsudo apt update &amp;&amp;\nsudo add-apt-repository ppa:deadsnakes/ppa || echo 'Skipped: Add Repository' &amp;&amp;\necho '########## Install Python V3.8 ##########' &amp;&amp;\nsudo apt -y install python3.8 || echo 'Skipped: Install Python 3.8' &amp;&amp;\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2 &amp;&amp;\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 &amp;&amp;\nsudo update-alternatives --set python3 /usr/bin/python3.8 &amp;&amp;\npython3 --version &amp;&amp;\nsudo apt -y install python3-pip &amp;&amp;\nsudo apt -y install python3.8-distutils &amp;&amp;\npython3 -m pip install --upgrade pip &amp;&amp;\necho '########## Start Install Python Library ##########' &amp;&amp;\necho $(date -u) &amp;&amp;\nsudo pip install --upgrade pip &amp;&amp;\nsudo pip install uv &amp;&amp;\nsudo uv pip install --system backports.zoneinfo &amp;&amp;\nsudo uv pip install --system azure-storage-file-datalake==12.4.0 &amp;&amp;\nsudo uv pip install --system azure-keyvault-secrets==4.3.0 &amp;&amp;\nsudo uv pip install --system azure-identity==1.6.1 &amp;&amp;\nsudo uv pip install --system cffi==1.16.0 &amp;&amp;\nsudo uv pip install --system google-cloud-bigquery==3.13.0 &amp;&amp;\nsudo uv pip install --system pandas_gbq==0.17.0 &amp;&amp;\nsudo uv pip install --system pandas==2.0.3 &amp;&amp;\nsudo uv pip install --system pyarrow==14.0.1 &amp;&amp;\nsudo uv pip install --system -U pytz\necho $(date -u) \"End of Python Library installation\"\n</code></pre> start_task.sh<pre><code>#!/bin/bash\n\necho '########## Installing Python 3.11 on Ubuntu 22.04 by using the PPA repository ##########' &amp;&amp;\nsudo apt install python3-apt --fix-missing &amp;&amp;\nsudo apt update &amp;&amp;\nsudo add-apt-repository ppa:deadsnakes/ppa || echo 'Skipped: Add Repository' &amp;&amp;\nsudo apt -y install python3.11 &amp;&amp;\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 2 &amp;&amp;\nsudo update-alternatives --config python3 &amp;&amp;\nsudo apt install python-is-python3 &amp;&amp;\npython --version &amp;&amp;\necho '########## Install Extras for Python 3.11 ##########' &amp;&amp;\nsudo apt-get install python3.11-full -y &amp;&amp;\nsudo apt-get install python3-pip -y &amp;&amp;\nsudo python3 -m pip install --upgrade pip &amp;&amp;\necho '########## Setting ODBC Libs ##########' &amp;&amp;\nsudo curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - &amp;&amp;\nsudo curl https://packages.microsoft.com/config/ubuntu/22.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list &amp;&amp;\nsudo ACCEPT_EULA=Y apt-get install -y msodbcsql17 &amp;&amp;\necho '--- Done: msodbcsql17' &amp;&amp;\nsudo ACCEPT_EULA=Y apt-get install -y mssql-tools &amp;&amp;\necho '--- Done: mssql-tools' &amp;&amp;\necho 'export PATH=\\\"$PATH:/opt/mssql-tools/bin\\\"' &gt;&gt; ~/.bashrc &amp;&amp;\nsource ~/.bashrc\n</code></pre> Note install_msodbcsql17.sh<pre><code>if ! [[ \"16.04 18.04 20.04 22.04\" == *\"$(lsb_release -rs)\"* ]];\nthen\n    echo \"Ubuntu $(lsb_release -rs) is not currently supported.\";\n    exit;\nfi\ncurl https://packages.microsoft.com/keys/microsoft.asc | sudo tee /etc/apt/trusted.gpg.d/microsoft.asc\ncurl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nsudo ACCEPT_EULA=Y apt-get install -y msodbcsql17\n# optional: for bcp and sqlcmd\nsudo ACCEPT_EULA=Y apt-get install -y mssql-tools\necho 'export PATH=\"$PATH:/opt/mssql-tools/bin\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n# optional: for unixODBC development headers\nsudo apt-get install -y unixodbc-dev\n</code></pre> <p>Ref from How to Install Microsoft ODBC Driver for SQL Server on Ubuntu</p>"},{"location":"services/azure/batch/az-ba-to-az/","title":"Connect to Azure Services","text":""},{"location":"services/azure/batch/az-ba-to-az/#authentication","title":"Authentication","text":""},{"location":"services/azure/batch/az-ba-to-az/#using-user-assigned-managed-identity","title":"Using User-Assigned Managed Identity","text":"<p>Warning</p> <p>The system-assigned managed identity created in a Batch account is only used for retrieving customer-managed keys from the Key Vault. This identity is not available on Batch pools.</p>"},{"location":"services/azure/batch/az-ba-to-az/#1-create-user-assigned-managed-identity","title":"1) Create User-Assigned Managed Identity","text":"<ul> <li>In the Azure Portal  Go to Managed Identities  Click Create</li> <li>Add the managed identity information  Select Review + create</li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#2-enable-azure-batch-account","title":"2) Enable Azure Batch Account","text":"<ul> <li>Go to Azure Batch Accounts  Click Pools  Select your Batch Pool name</li> <li>Go to Identity  Nav User assigned  Click Add</li> <li>Select your managed identity that was created from above  Click Add</li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#3-credential-code","title":"3) Credential Code","text":"<p>Before getting managed identity, you should install the Azure client library;</p> <pre><code>pip install -U azure-identity\n</code></pre> <pre><code>from azure.identity import ManagedIdentityCredential\n\nmsi_credential = ManagedIdentityCredential()\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-az/#using-certificate","title":"Using Certificate","text":"<p>The computational jobs running on Batch will need to use a certificate to prove their identity to Azure AD, so they can assume the identity of the App you registered.</p> <p>Danger</p> <p>the Azure Batch Account Certificates feature will be retired on February 29, 2024.</p>"},{"location":"services/azure/batch/az-ba-to-az/#1-generate-certificate","title":"1) Generate Certificate","text":"<p>Firstly we need to create a certificate which can be used for authentication. To do that we're going to generate a Certificate Signing Request (CSR) using <code>openssl</code>.</p> <pre><code>$ openssl req \\\n  -newkey rsa:4096 -nodes -keyout \"service-principal.key\" \\\n  -out \"service-principal.csr\"\n</code></pre> <p>We can now sign that Certificate Signing Request (CSR), in this example we're going to self-sign this certificate using the Key we just generated; however it's also possible to do this using a Certificate Authority. In order to do that we're again going to use <code>openssl</code></p> <pre><code>$ openssl x509 \\\n  -signkey \"service-principal.key\" \\\n  -in \"service-principal.csr\" \\\n  -req -days 365 \\\n  -out \"service-principal.crt\"\n</code></pre> <p>This <code>service-principal.crt</code> file you can upload to the App Registration and note the resulting thumbprint. Then, we have an App Registration with a related certificate. Finally, we can generate a <code>.pfx</code> file which can be used to authenticate with Azure:</p> <pre><code>$ openssl pkcs12 -export -out \"service-principal.pfx\" \\\n  -inkey \"service-principal.key\" \\\n  -in \"service-principal.crt\"\n</code></pre> <p>So we will use this <code>service-principal.pfx</code> file, providing the thumbprint we got when we uploaded the certificate to the App Registration for Azure Batch Account.</p> <p>Note</p> <p>We will actually need the thumbprint converted from its hexadecimal representation to base64. We can use sed to replace the colons and remove <code>SHA1 Fingerprint=</code> substring, <code>xxd</code> to convert to bytes, and <code>base64</code> to encode.</p> <pre><code>$ echo $(openssl x509 -in \"service-principal.csr\" -fingerprint -noout) \\\n  | sed 's/SHA1 Fingerprint=//g' \\\n  | sed 's/://g' \\\n  | xxd -r -ps \\\n  | base64\n</code></pre> <p>The <code>service-principal.csr</code> file contains the public key value of the self-signed certificate we generated. We will need to grab that value skipping the first and the last lines.</p> <pre><code>$ tail -n+2 service-principal.csr | head -n-1\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-az/#2-assign-certificate-to-service-principle","title":"2) Assign Certificate to Service Principle","text":"<p>Sometimes called a public key, a certificate is the recommended credential type because they're considered more secure than client secrets.</p> <ul> <li>Go to Azure App registrations  Select <code>Certificates &amp; secrets</code>  Click <code>Certificates</code></li> <li>Upload the certificate that was created from above step. Select the file you   want to upload. It must be one of the following file types: <code>.cer</code>, <code>.pem</code>, <code>.crt</code>  Select <code>Add</code>.</li> </ul> <p>Warning</p> <p>This accepts the following file formats: <code>cer</code>, <code>pem</code> and <code>crt</code>.</p>"},{"location":"services/azure/batch/az-ba-to-az/#3-assign-certificate-to-batch-account","title":"3) Assign Certificate to Batch Account","text":"<p>Assigning the certificate to the account lets Batch assign it to the pools and then to the nodes.</p> <ul> <li>In the <code>Azure portal</code>, in <code>Batch accounts</code>, select your batch account.</li> <li>Select <code>Certificates</code>, select <code>Add</code>.</li> <li>Upload the <code>.pfx</code> file you generated and supply the password</li> <li>Pass the certificate thumbprint.</li> <li>Select <code>Create</code></li> </ul> <p>Now when you create a Batch pool, you can navigate to <code>Certificates</code> within the pool and assign the certificate that you created in your Batch account to that pool. When you do so, ensure you select <code>LocalMachine</code> for the store location. The certificate is loaded on all Batch nodes in the pool.</p> <p>In that setup, the certificates attached to the pool will be available in the folder defined by an environmental variable <code>AZ_BATCH_CERTIFICATES_DIR</code>.</p> <pre><code>${AZ_BATCH_CERTIFICATES_DIR}/sha1-${THUMBPRINT}.pfx\n${AZ_BATCH_CERTIFICATES_DIR}/sha1-${THUMBPRINT}.pfx.pw\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-az/#4-credential-code","title":"4) Credential Code","text":"<p>Before getting managed identity, you should install the Azure client library;</p> <pre><code>pip install -U azure-identity\n</code></pre> <p>If you are using the Azure SDK for python, unfortunately the pfx format is not compatible with the SDK, so we need to convert it:</p> <pre><code>CERT_THUMBPRINT=&lt;your-cert-thumbprint&gt;;\nCERT_IN=\"${AZ_BATCH_CERTIFICATES_DIR}/sha1-${CERT_THUMBPRINT}.pfx\";\nCERT_OUT=\"${AZ_BATCH_CERTIFICATES_DIR}/cert.pem\";\nCERT_PWD=\"${CERT_IN}.pw\";\n</code></pre> <pre><code>$ openssl pkcs12 -in ${CERT_IN} -out ${CERT_OUT} -nokeys -nodes -password file:${CERT_PWD};\n$ openssl pkcs12 -in ${CERT_IN} -nocerts -nodes -password file:${CERT_PWD} \\\n  | openssl rsa -out ${AZ_BATCH_CERTIFICATES_DIR}/cert.key;\n$ cat ${AZ_BATCH_CERTIFICATES_DIR}/cert.key &gt;&gt; CERT_OUT;\n</code></pre> <p>With these steps, we have converted the <code>.pfx</code> certificate file to a <code>.pem</code> style certificate file, which is usable with Python:</p> <pre><code>import os\nfrom azure.identity import CertificateCredential\n\nCERT_PATH: str = os.environ.get('AZ_BATCH_CERTIFICATES_DIR')\n\ncertificate_credential = CertificateCredential(\n    tenant_id=os.environ[\"AZURE_TENANT_ID\"],\n    client_id=os.environ[\"CLIENT_ID\"],\n    certificate_path=f\"{CERT_PATH}/cert.pem\"\n)\n</code></pre> <p>Full Python scripts:</p> <pre><code>import os\n\nCERT_PATH: str = os.environ.get('AZ_BATCH_CERTIFICATES_DIR')\n\ndef gen_pem_cert(cert_thumbprint: str):\n    # Start pem certificate generation\n    os.system(\n        (\n            f\"openssl pkcs12 -in {CERT_PATH}/sha1-{cert_thumbprint}.pfx \"\n            f\"-out {CERT_PATH}/cert.pem -nokeys -nodes \"\n            f\"-password file:{CERT_PATH}/sha1-{cert_thumbprint}.pfx.pw \"\n            f\"2&gt;/dev/null\"\n        )\n    )\n    # Start RSA Key generation\n    os.system(\n        (\n            f\"openssl pkcs12 -in {CERT_PATH}/sha1-{cert_thumbprint}.pfx \"\n            f\"-nocerts -nodes \"\n            f\"-password file:{CERT_PATH}/sha1-{cert_thumbprint}.pfx.pw \"\n            f\"| openssl rsa -out {CERT_PATH}/cert.key \"\n            f\"2&gt;/dev/null\"\n        )\n    )\n    # Combine key with certificate\n    os.system(\n        f\"cat {CERT_PATH}/cert.key &gt;&gt; {CERT_PATH}/cert.pem\"\n    )\n\ndef rm_pem_cert():\n    # Start removing Certificate\n    os.system(f\"rm {CERT_PATH}/cert.key\")\n    os.system(f\"rm {CERT_PATH}/cert.pem\")\n</code></pre> <pre><code>import os\nfrom azure.identity import CertificateCredential\n\nCERT_PATH: str = os.environ.get('AZ_BATCH_CERTIFICATES_DIR')\n\ncredential = CertificateCredential(\n    tenant_id=tenant_id,\n    client_id=client_id,\n    certificate_path=f\"{CERT_PATH}/cert.pem\"\n)\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-az/#services","title":"Services","text":"<p>Tip</p> <p>Wait for at least 15 minutes for role to propagate and then try to access after assign IAM Role to target service.</p>"},{"location":"services/azure/batch/az-ba-to-az/#key-vault","title":"Key Vault","text":""},{"location":"services/azure/batch/az-ba-to-az/#prerequisite","title":"Prerequisite","text":"<ul> <li>Go to Azure Key Vaults  Select your key vault name, <code>kv-demo</code></li> <li>On Access control (IAM)  Click Add    Assign Key Vault Secrets User to your user-assigned managed identity</li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#connection-code","title":"Connection Code","text":"<p>Before develop code, you should install Azure Key Vault client library;</p> <pre><code>pip install azure-keyvault-secrets\n</code></pre> <p>Implement connection code to the Azure Batch Node that use to get any secrets from Azure Key Vault.</p> <pre><code>from azure.identity import ManagedIdentityCredential\nfrom azure.keyvault.secrets import SecretClient\n\n\ndef secret_client(keyvault_name: str):\n    \"\"\"Return Secret Client from Managed Identity Authentication.\"\"\"\n    msi_credential = ManagedIdentityCredential()\n    return SecretClient(\n        vault_url=f\"https://{keyvault_name}.vault.azure.net\",\n        credential=msi_credential\n    )\n</code></pre> Reference Links <ul> <li>https://arsenvlad.medium.com/certificate-based-auth-with-azure-service-principals-from-linux-command-line-a440c4599cae</li> <li>https://msendpointmgr.com/2023/03/11/certificate-based-authentication-aad/</li> <li>https://learn.microsoft.com/en-us/azure/batch/managed-identity-pools</li> <li>https://medium.com/datamindedbe/how-to-access-key-vaults-from-azure-batch-jobs-34388b1adf46</li> <li>https://learn.microsoft.com/en-us/azure/batch/batch-customer-managed-key</li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#synapse","title":"Synapse","text":""},{"location":"services/azure/batch/az-ba-to-az/#prerequisite_1","title":"Prerequisite","text":"<ul> <li> <p>Go to Azure Synapse SQL Pool  Create user from external     provider and grant permission of this user such as Read access,</p> <pre><code>CREATE USER [&lt;user-assigned-name&gt;] FROM EXTERNAL PROVIDER;\n</code></pre> </li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#connection-code_1","title":"Connection Code","text":"<p>Before develop code, you should install DataFrame API library;</p> <pre><code>pip install arrow-odbc polars\n</code></pre> <p>Note</p> <p>I recommend <code>arrow-odbc</code> and <code>polars</code> for loadding performance.</p> <pre><code>import os\nimport polars as pl\nfrom arrow_odbc import read_arrow_batches_from_odbc\n\nreader = read_arrow_batches_from_odbc(\n    query=\"SELECT * FROM &lt;schema-name&gt;.&lt;table-name&gt;\",\n    connection_string=(\n        f\"Driver={{ODBC Driver 17 for SQL Server}};\"\n        f\"Server={os.getenv('MSSQL_HOST')};\"\n        f\"Port=1433;\"\n        f\"Database={os.getenv('MSSQL_DB')};\"\n        f\"Uid={os.getenv('MSSQL_USER')};\"\n        f\"Pwd={os.getenv('MSSQL_PASS')};\"\n    ),\n    max_bytes_per_batch=536_870_912,  # Default: 2 ** 29 (536_870_912)\n    batch_size=1_000_000,  # Default: 65_535\n)\nreader.fetch_concurrently()\nfor batch in reader:\n    df: pl.DataFrame = pl.from_arrow(batch)\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-az/#datalake","title":"DataLake","text":""},{"location":"services/azure/batch/az-ba-to-az/#prerequisite_2","title":"Prerequisite","text":"<ul> <li>Go to Azure Storage Accounts  Select your storage account name</li> <li>On Access control (IAM)  Click Add    Assign Storage Blob Data Contributor to your user-assigned managed identity</li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#connection-code_2","title":"Connection Code","text":"<p>Before develop code, you should install Azure Datalake Storage client library;</p> <pre><code>pip install azure-storage-file-datalake cffi\n</code></pre> <pre><code>from azure.identity import ManagedIdentityCredential\nfrom azure.storage.filedatalake import DataLakeServiceClient\n\n\ndef lake_client(storage_account_name) -&gt; DataLakeServiceClient:\n    \"\"\"Generate ADLS Client from input credential\"\"\"\n    msi_credential = ManagedIdentityCredential()\n    return DataLakeServiceClient(\n        account_url=f\"https://{storage_account_name}.dfs.core.windows.net\",\n        credential=msi_credential\n    )\n</code></pre> More Codes <pre><code>import io\nimport pathlib\n\nimport pyarrow.parquet as pq\nimport pandas as pd\nfrom azure.core.exceptions import ResourceNotFoundError\nfrom azure.identity import ManagedIdentityCredential\nfrom azure.storage.filedatalake import (\n    DataLakeServiceClient,\n    DataLakeFileClient,\n)\n\n\ndef lake_client(storage_account_name) -&gt; DataLakeServiceClient:\n    \"\"\"Generate ADLS Client from input credential\"\"\"\n    msi_credential = ManagedIdentityCredential()\n    return DataLakeServiceClient(\n        account_url=f\"https://{storage_account_name}.dfs.core.windows.net\",\n        credential=msi_credential\n    )\n\n\ndef exists(\n    client: DataLakeServiceClient,\n    container: str,\n    filepath: str,\n) -&gt; bool:\n    \"\"\"Return True if filepath on target container exists.\"\"\"\n    try:\n        (\n            client\n                .get_file_system_client(file_system=container)\n                .get_file_client(filepath)\n                .get_file_properties()\n        )\n        return True\n    except ResourceNotFoundError:\n        return False\n\ndef download(\n    client: DataLakeServiceClient,\n    container: str,\n    filepath: str,\n) -&gt; pathlib.Path:\n    file_client: DataLakeFileClient = (\n        client\n            .get_file_system_client(file_system=container)\n            .get_file_client(filepath)\n    )\n    output_file = pathlib.Path(filepath)\n    output_file.parent.mkdir(exist_ok=True, parents=True)\n    with output_file.open(mode='wb') as local_file:\n        file_client.download_file().readinto(local_file)\n    return output_file\n\n\ndef upload(\n    client: DataLakeServiceClient,\n    container: str,\n    dirpath: str,\n    file: str,\n    df: pd.DataFrame,\n) -&gt; DataLakeFileClient:\n    dir_client: DataLakeFileClient = (\n        client\n            .get_file_system_client(file_system=container)\n            .get_directory_client(directory=dirpath)\n    )\n    file_client = dir_client.create_file(file)\n\n    # If Upload Parquet file\n    io_file = df.to_parquet()\n\n    # Or, file_client.append_data(data=df, offset=0, length=len(df))\n    file_client.upload_data(data=io_file, overwrite=True)\n    file_client.flush_data(len(io_file))\n    return file_client\n\n\ndef to_pyarrow(\n    client: DataLakeServiceClient,\n    container: str,\n    filepath: str,\n) -&gt; pq.Table:\n    file_client: DataLakeFileClient = (\n        client\n            .get_file_system_client(file_system=container)\n            .get_file_client(filepath)\n    )\n    data = file_client.download_file(0)\n    with io.BytesIO() as b:\n        data.readinto(b)\n        table = pq.read_table(b)\n    return table\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-google/","title":"Connection BigQuery","text":""},{"location":"services/azure/batch/az-ba-to-google/#authentication","title":"Authentication","text":""},{"location":"services/azure/batch/az-ba-to-google/#using-service-account","title":"Using Service Account","text":""},{"location":"services/azure/batch/az-ba-to-google/#1-create-service-account","title":"1) Create Service Account","text":"<ul> <li>In the IAM &amp; Admin  Go to Service Accounts  Click CREATE SERVICE ACCOUNT</li> <li>On Permission  Assign BigQuery Job User and   BigQuery Data Editor to this service account</li> </ul>"},{"location":"services/azure/batch/az-ba-to-google/#2-keep-credential-to-key-vault","title":"2) Keep Credential to Key Vault","text":"<ul> <li> <p>When a service account was created, it will generate Json credential file that     has detail like:</p> <pre><code>{\n  \"type\": \"service_account\",\n  \"project_id\": \"&lt;project-id&gt;\",\n  \"private_key_id\": \"&lt;private-key-id&gt;\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n???\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"&lt;service-name&gt;@&lt;project-id&gt;.iam.gserviceaccount.com\",\n  \"client_id\": \"&lt;client-id&gt;\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/&lt;service-name&gt;%40&lt;project-id&gt;.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre> </li> </ul>"},{"location":"services/azure/batch/az-ba-to-google/#3-connection-code","title":"3) Connection Code","text":"<p>On Bath Pool, it should install Python package:</p> <pre><code>$ pip install google-auth\n</code></pre> <pre><code>import json\nfrom google.oauth2.service_account import Credentials\n\njson_info = json.loads(secret_client.get_secret(\"GOOGLE-JSON-STR\").value)\ncredentials = Credentials.from_service_account_info(json_info),\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-google/#using-oauth-token","title":"Using OAuth Token","text":"<p>User credentials are typically obtained via OAuth2.0</p> <pre><code>from google.oauth2.credentials import Credentials\n\ncredentials = Credentials(\n    '&lt;access-token&gt;',\n    # NOTE: If you obtain a refresh token\n    refresh_token='&lt;refresh_token&gt;',\n    token_uri='&lt;token_uri&gt;',\n    client_id='&lt;client_id&gt;',\n    client_secret='&lt;client_secret&gt;',\n)\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-google/#services","title":"Services","text":""},{"location":"services/azure/batch/az-ba-to-google/#bigquery","title":"BigQuery","text":"<p>On Bath Pool, it should install Python package:</p> <pre><code>$ pip install google-cloud-bigquery pandas_gbq\n</code></pre> <p>Note</p> <p>I recommend <code>padas_gbq</code> because it does not implement complex code.</p> <p>Before this connection code, you should implement connection for Azure Key Vault first for getting above secret json.</p> <pre><code>import json\nimport pandas as pd\nimport pandas_gbq as pg\nfrom google.oauth2.service_account import Credentials\n\njson_info = json.loads(secret_client.get_secret(\"GOOGLE-JSON-STR\").value)\npg.to_gbq(\n    pd.read_parquet(\"/dummy-file.parquet\"),\n    destination_table=\"&lt;dataset&gt;.&lt;table-name&gt;\",\n    if_exists='replace',\n    project_id=\"&lt;project-id&gt;\",\n    credentials=Credentials.from_service_account_info(json_info),\n)\n</code></pre>"},{"location":"services/azure/batch/az-ba-with-docker-inside/","title":"Dockerize inside Node","text":""},{"location":"services/azure/batch/az-ba-with-docker-inside/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/batch/az-ba-with-docker-inside/#create-pool-with-dockerize-start-task","title":"Create Pool with Dockerize start task","text":"<p>Create Ubuntu pool and set start task command line:</p> <pre><code>/bin/bash -c\n\"sudo apt-get update &amp;&amp;\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common &amp;&amp;\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - &amp;&amp;\nsudo apt-key fingerprint 0EBFCD88 &amp;&amp;\nsudo add-apt-repository \\\"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\\" &amp;&amp;\nsudo apt-get update &amp;&amp;\nsudo apt-get -y install docker-ce docker-ce-cli containerd.io docker-compose-plugin &amp;&amp;\nsudo usermod -aG docker $USER &amp;&amp;\nsudo systemctl restart docker &amp;&amp;\nsudo apt-get install dos2unix\n\"\n</code></pre> <p>Info</p> <p>We add <code>$USER</code> to docker group because we want to execute <code>docker</code> command without <code>sudo</code>.</p>"},{"location":"services/azure/batch/az-ba-with-docker-inside/#create-docker-image-file","title":"Create Docker image file","text":"Dockerfile<pre><code>FROM ubuntu:16.04\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y cmake build-essential gcc g++ git wget libgl1-mesa-glx\n\nRUN echo \"ttf-mscorefonts-installer msttcorefonts/accepted-mscorefonts-eula select true\" | debconf-set-selections\nRUN apt-get install -y --no-install-recommends msttcorefonts\n\nRUN wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; \\\n    /bin/bash Miniconda3-latest-Linux-x86_64.sh -f -b -p /opt/conda &amp;&amp; \\\n    export PATH=\"/opt/conda/bin:$PATH\"\n\nENV PATH /opt/conda/bin:$PATH\n\nRUN conda install -y numpy scipy scikit-learn pandas matplotlib\n\nRUN pip install azure azure-storage\n\nRUN apt-get autoremove -y &amp;&amp; apt-get clean &amp;&amp; \\\n    conda clean -i -l -t -y &amp;&amp; \\\n    rm -rf /usr/local/src/*\n\nCOPY . .\n\nENV AZURE_BLOB_KEY=\"[AZURE_BLOB_KEY]\"\n\nENTRYPOINT [ \"python\", \"train.py\" ]\n</code></pre>"},{"location":"services/azure/batch/az-ba-with-docker-inside/#create-runner-script","title":"Create runner script","text":"runner.sh<pre><code>#!/bin/bash\n\necho \"Script Name: $0 with process id: $$\";\necho \"Start run docker with image: $1\";\necho \"Receive environment file name: $2\";\n\necho ${AZ_BATCH_CERTIFICATES_DIR};\nmkdir certs/;\ncp ${AZ_BATCH_CERTIFICATES_DIR}/* certs/;\nls certs/;\n\nACR_PWD=$(python3 runner.py 2&gt;&amp;1 &gt;/dev/null);\n\n# Build Docker Container\ndocker --version;\ndocker build -t $1:latest . --no-cache;\ndocker login dataplatdev.azurecr.io -u dataplatdev -p $ACR_PWD;\ndocker pull dataplatdev.azurecr.io/poc/python-test:0.0.8 &gt;/dev/null 2&gt;&amp;1;\ndocker images;\necho\necho \"Delete Old images ...\";\ndocker rmi -f $(docker image ls -f \"dangling=true\" -q);\ndocker images;\necho\n# Run Docker Container\nOLD=$(docker ps --all --quiet --filter=name=\"$1\");\nif [[ -n \"$OLD\" ]]; then docker stop $OLD &amp;&amp; docker rm $OLD; fi;\n# docker run --name $1 --env-file \"./$2\" -v \"$(pwd)\\output:/output\" $1:latest;\ndocker run --name $1 -v \"$(pwd)\\output:/output\" $1:latest;\ndocker ps -a;\nCONTAINER_RC=$(docker inspect --format '{{.State.ExitCode}}' $1);\nexit $CONTAINER_RC;\n</code></pre>"},{"location":"services/azure/batch/az-ba-with-docker-inside/#references","title":"References","text":"<ul> <li>Container ML on Azure Batch</li> </ul>"},{"location":"services/azure/batch/az-ba-with-docker/","title":"Dockerize","text":"<p>Azure Batch can be a great tool for instant batch processing as it creates and manages a pool of compute nodes (virtual machines), installs the applications you want to run, and schedules jobs to run on the nodes.</p>"},{"location":"services/azure/batch/az-ba-with-docker/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/batch/az-ba-with-docker/#azure-container-registry","title":"Azure Container Registry","text":"<ul> <li>Go to Container Registries  Create container registry with prefix name <code>cr</code></li> <li>Add the information of this registry, for example with name: <code>cr-ba-python-dev</code>  Click Create for registry creation</li> <li>After registry creation, Go to <code>cr-ba-python-dev</code> registry    On Access Keys</li> <li>Click Enable on Admin user option</li> <li>Save these values, Login server, Username, and Password</li> <li>Go to your local terminal for prepare docker file</li> <li> <p>Create your <code>Dockerfile</code> file</p> Dockerfile<pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY main.py ./\n\nRUN mkdir -p ./output\n\nCMD [\"python\", \"./main.py\"]\n</code></pre> <p>The main Python file that run on this Docker</p> ./main.py<pre><code>print(\"This is a Main file for testing Dockerize.\")\nwith open('/output/docker_test.txt', 'w') as f:\n    f.write(\"Write Output file on Dockerize.\")\n</code></pre> </li> <li> <p>Test this Docker image able to run on the Local</p> <pre><code>docker build -t \"python-ba\" . --no-cache\ndocker run --name python-btch -v \"${pwd}\\output:/output\" python-btch\n</code></pre> </li> <li> <p>Push your Docker image to Azure Container Registries</p> <pre><code>docker login cr-ba-python-dev.azurecr.io\ndocker tag python-btch:latest cr-ba-python-dev.azurecr.io/btch/python-btch:0.0.1-test\ndocker push cr-ba-python-dev.azurecr.io/btch/python-btch:0.0.1-test\n</code></pre> </li> </ul>"},{"location":"services/azure/batch/az-ba-with-docker/#azure-batch-accounts","title":"Azure Batch Accounts","text":"<ul> <li>Go to your Azure Batch Accounts  Click Pools  Add new pool that Supports Container</li> <li>Click Enable to Custom on Container configuration option</li> <li>Go to Container registries  Add <code>cr-ba-python-dev</code> registry from ACR values</li> <li>Create Pool with name is <code>btch-pool-cntn</code></li> <li>Go to Jobs  Create new job in <code>btch-pool-cntn</code> pool with name <code>btch-job-cntn</code></li> <li> <p>Go to Tasks  Create new task in this job</p> <ul> <li>Go to Image name  Add <code>cr-ba-python-dev.azurecr.io/btch/python-btch:0.0.1-test</code></li> <li> <p>Go to Container run options and add below command</p> <pre><code>--rm --workdir /app\n</code></pre> </li> </ul> </li> <li> <p>Create the Automate Script file</p> <p>Package image version from local to Azure Container Registries</p> <pre><code>@echo off\nset \"version=%~1\"\nif defined version (\n    echo Start package docker image version: %version% ...\n    call docker build -t python-test:latest . --no-cache\n    call docker tag python-test:latest cr-ba-python-dev.azurecr.io/poc/python-test:%version%\n    call docker push cr-ba-python-dev.azurecr.io/poc/python-test:%version%\n    call docker rmi cr-ba-python-dev.azurecr.io/poc/python-test:%version%\n\n    for /f \"tokens=1-3\" %%c IN ('docker image ls ^| Findstr /r \"^cr-ba-python-dev.azurecr.io* ^&lt;none&gt;\"')\n    do (\n        echo Start remove image: `%%c:%%d` with ID: %%e\n        if \"%%d\" equ \"&lt;none&gt;\" (\n            echo Delete image with id ...\n            call docker rmi %%e &gt; nul 2&gt;&amp;1\n        ) else (\n            echo Delete image with name:tag ...\n            call docker rmi \"%%c:%%d\" &gt; nul 2&gt;&amp;1\n        )\n    )\n)\n</code></pre> <p>Additional, run this Task with JSON</p> <pre><code>{\n  \"id\": \"container-job-10\",\n  \"commandLine\": \"\",\n  \"containerSettings\": {\n      \"containerRunOptions\": \"--rm --workdir /app\",\n      \"imageName\": \"cr-ba-python-dev.azurecr.io/poc/python-test:0.0.8\",\n      \"workingDirectory\": \"taskWorkingDirectory\"\n  },\n  \"userIdentity\": {\n      \"autoUser\": {\n          \"scope\": \"pool\",\n          \"elevationLevel\": \"admin\"\n      }\n  }\n}\n</code></pre> </li> </ul> <p>Note</p> <p>About mounting volumn to Azure Batch Node,</p> <p><code>--mount type=bind,source=/datadisks/disk1,target=/data</code></p> <p><code>-v {&lt;volume_id&gt;}:{&lt;path&gt;}</code></p> <p>Warning</p> <p>Azure Data Factory does not support for run Azure Batch with a Docker container in the Custom Activity currently, Read More</p>"},{"location":"services/azure/batch/az-ba-with-docker/#run-with-mount-volume","title":"Run with Mount Volume","text":"<ul> <li> Azure Samples: Compute Automation Configurations - Prepare VM disks</li> <li> Can I use Docker volumes in Container based Azure Batch Pool</li> </ul>"},{"location":"services/azure/batch/az-ba-with-docker/#read-mores","title":"Read Mores","text":"<ul> <li> Use Container for Azure Batch Service</li> </ul>"},{"location":"services/azure/data_factory/adf-ir-sharing/","title":"IR Sharing","text":""},{"location":"services/azure/data_factory/adf-ir-sharing/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft: Data Factory - Create Shared Self Hosted IR</li> </ul>"},{"location":"services/azure/data_factory/adf-link-services/","title":"Link Services","text":""},{"location":"services/azure/data_factory/adf-link-services/#azure-services","title":"Azure Services","text":"KeyVaultFunctionBatchDatabricksDataLakeBlob StorageSQLDatabaseSynapse <pre><code>{\n    \"name\": \"KeyVaultAll\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"description\": \"All of Key Vaults\",\n        \"annotations\": [],\n        \"type\": \"AzureKeyVault\",\n        \"typeProperties\": {\n            \"baseUrl\": \"https://kv-&lt;platform&gt;-dev.vault.azure.net/\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicAzureFunction\",\n    \"properties\": {\n        \"parameters\": {\n            \"FuctionAppUrl\": {\n                \"type\": \"string\"\n            },\n            \"Secret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"AzureFunction\",\n        \"typeProperties\": {\n            \"functionAppUrl\": \"@{linkedService().FuctionAppUrl}\",\n            \"functionKey\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().Secret\",\n                    \"type\": \"Expression\"\n                }\n            },\n            \"authentication\": \"Anonymous\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicAzureBatch\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"AzureBatch\",\n        \"annotations\": [],\n        \"parameters\": {\n            \"BatchURI\": {\n                \"type\": \"String\"\n            },\n            \"Pool\": {\n                \"type\": \"String\"\n            },\n            \"Account\": {\n                \"type\": \"String\"\n            }\n        },\n        \"typeProperties\": {\n            \"batchUri\": \"@linkedService().BatchURI\",\n            \"poolName\": \"@linkedService().Pool\",\n            \"accountName\": \"@linkedService().Account\",\n            \"linkedServiceName\": {\n                \"referenceName\": \"&lt;azure-blob-storage-link-service-name&gt;\",\n                \"type\": \"LinkedServiceReference\"\n            }\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicAzureDatabrick\",\n    \"properties\": {\n        \"description\": \"Databrick Connection used for common dynamic\",\n        \"parameters\": {\n            \"ClusterURL\": {\n                \"type\": \"String\"\n            },\n            \"ClusterID\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"AzureDatabricks\",\n        \"typeProperties\": {\n            \"domain\": \"@concat('https://', linkedService().ClusterURL, '/')\",\n            \"accessToken\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@linkedService().Secret\"\n            },\n            \"existingClusterId\": \"@linkedService().ClusterID\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicDataLake\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"description\": \"Azure Data Lake Gen 2\",\n        \"annotations\": [],\n        \"type\": \"AzureBlobFS\",\n        \"typeProperties\": {\n            \"url\": \"https://&lt;storage-account-name&gt;.dfs.core.windows.net\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicBlobStorage\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"annotations\": [],\n        \"type\": \"AzureBlobStorage\",\n        \"typeProperties\": {\n            \"connectionString\": \"DefaultEndpointsProtocol=https;AccountName=&lt;storage-account-name&gt;;EndpointSuffix=core.windows.net;\",\n            \"accountKey\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"&lt;secret-name&gt;\"\n            }\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"AzureSqlAuto\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"AzureSqlDatabase\",\n        \"parameters\": {\n            \"Username\": {\n                \"type\": \"String\"\n            },\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Server\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"annotations\": [],\n        \"typeProperties\": {\n            \"connectionString\": \"Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=@{linkedService().Server};Initial Catalog=@{linkedService().Database};User ID=@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().Secret\",\n                    \"type\": \"Expression\"\n                }\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicSynapse\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"parameters\": {\n            \"Username\": {\n                \"type\": \"String\"\n            },\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Server\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"AzureSqlDW\",\n        \"typeProperties\": {\n            \"connectionString\": \"Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=@{linkedService().Server};Initial Catalog=@{linkedService().Database};User ID=@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@{linkedService().Secret}\",\n                    \"type\": \"Expression\"\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"services/azure/data_factory/adf-link-services/#storage-system","title":"Storage System","text":"MySQLSQLServerPostgresMongoDBBigQuery <pre><code>{\n    \"name\": \"MySqlAuto\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"MySql\",\n        \"parameters\": {\n            \"Server\": {\n                \"type\": \"String\"\n            },\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Username\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"annotations\": [],\n        \"typeProperties\": {\n            \"connectionString\": \"Server=@{linkedService().Server};Port=3306;Database=@{linkedService().Database};User=@{linkedService().Username};SSLMode=1;UseSystemTrustStore=0\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().Secret\",\n                    \"type\": \"Expression\"\n                }\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicSqlServer\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"parameters\": {\n            \"Username\": {\n                \"type\": \"String\"\n            },\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Server\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"SqlServer\",\n        \"typeProperties\": {\n            \"connectionString\": \"Data Source=@{linkedService().Server};Initial Catalog=@{linkedService().Database};Integrated Security=False;User ID=@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@{linkedService().Secret}\",\n                    \"type\": \"Expression\"\n                }\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicPostgres\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"PostgreSql\",\n        \"annotations\": [],\n        \"parameters\": {\n            \"Server\": {\n                \"type\": \"String\"\n            },\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Username\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"typeProperties\": {\n            \"connectionString\": \"Server=@{linkedService().Server};Database=@{linkedService().Database};Port=5432;UID=@{linkedService().Username};\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@{linkedService().Secret}\"\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"MongoDBAuto\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"MongoDbV2\",\n        \"annotations\": [],\n        \"parameters\": {\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"typeProperties\": {\n            \"connectionString\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@linkedService().Secret\"\n            },\n            \"database\": \"@linkedService().Database\"\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <p>The Connection String that keep in Azure Key Vaults:</p> <pre><code>mongodb://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:27017\n</code></pre> <pre><code>{\n    \"name\": \"DynamicBigQuery\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"GoogleBigQuery\",\n        \"annotations\": [],\n        \"parameters\": {\n            \"Project\": {\n                \"type\": \"String\"\n            },\n            \"ClientID\": {\n                \"type\": \"String\"\n            },\n            \"ClientSecret\": {\n                \"type\": \"String\"\n            },\n            \"RefreshToken\": {\n                \"type\": \"String\"\n            }\n        },\n        \"typeProperties\": {\n            \"project\": \"@replace(linkedService().Project, '''', '')\",\n            \"requestGoogleDriveScope\": false,\n            \"authenticationType\": \"UserAuthentication\",\n            \"clientId\": \"@replace(linkedService().ClientID, '''', '')\",\n            \"clientSecret\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"KVdataplat\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@replace(linkedService().ClientSecret, '''', '')\"\n            },\n            \"refreshToken\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@replace(linkedService().RefreshToken, '''', '')\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"services/azure/data_factory/adf-link-services/#file-system","title":"File System","text":"File SystemSFTPGCSS3 <pre><code>{\n    \"name\": \"DynamicFileSystem\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"parameters\": {\n            \"Server\": {\n                \"type\": \"string\",\n                \"defaultValue\": \"\\\\\\\\path\\\\sub-path\\\\folder\"\n            },\n            \"Username\": {\n                \"type\": \"string\"\n            },\n            \"Secret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"FileServer\",\n        \"typeProperties\": {\n            \"host\": \"@{linkedService().Server}\",\n            \"userId\": \"@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().Secret\",\n                    \"type\": \"Expression\"\n                }\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicSFTP\",\n    \"properties\": {\n        \"parameters\": {\n            \"Server\": {\n                \"type\": \"string\"\n            },\n            \"Username\": {\n                \"type\": \"string\"\n            },\n            \"Secret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"Sftp\",\n        \"typeProperties\": {\n            \"host\": \"@{linkedService().Server}\",\n            \"port\": 22,\n            \"skipHostKeyValidation\": true,\n            \"authenticationType\": \"Basic\",\n            \"userName\": \"@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().Secret\",\n                    \"type\": \"Expression\"\n                }\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicGCS\",\n    \"properties\": {\n        \"type\": \"GoogleCloudStorage\",\n        \"annotations\": [],\n        \"parameters\": {\n            \"ServiceURL\": {\n                \"type\": \"string\"\n            },\n            \"AccessKey\": {\n                \"type\": \"string\"\n            },\n            \"AccessSecret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"typeProperties\": {\n            \"serviceUrl\": \"@linkedService().ServiceURL\",\n            \"accessKeyId\": \"@linkedService().AccessKey\",\n            \"secretAccessKey\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@linkedService().AccessSecret\"\n            }\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicAmazonS3\",\n    \"properties\": {\n        \"type\": \"AmazonS3\",\n        \"parameters\": {\n            \"AccessSecret\": {\n                \"type\": \"string\"\n            },\n            \"AccessKey\": {\n                \"type\": \"string\"\n            },\n            \"S3URL\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"typeProperties\": {\n            \"serviceUrl\": \"@{linkedService().S3URL}\",\n            \"accessKeyId\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().AccessKey\",\n                    \"type\": \"Expression\"\n                }\n            },\n            \"secretAccessKey\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"KVdataplat\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().AccessSecret\",\n                    \"type\": \"Expression\"\n                }\n            },\n            \"authenticationType\": \"AccessKey\"\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    },\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\"\n}\n</code></pre>"},{"location":"services/azure/data_factory/adf-link-services/#http","title":"HTTP","text":"AnonymousWindows <pre><code>{\n    \"name\": \"DynamicHTTPAnonymous\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"HttpServer\",\n        \"parameters\": {\n            \"BaseURL\": {\n                \"type\": \"string\",\n                \"defaultValue\": \"https://dev.domain.com/api/\"\n            }\n        },\n        \"annotations\": [],\n        \"typeProperties\": {\n            \"url\": \"@{linkedService().BaseURL}\",\n            \"enableServerCertificateValidation\": true,\n            \"authenticationType\": \"Anonymous\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicHTTPWindows\",\n    \"properties\": {\n        \"type\": \"HttpServer\",\n        \"parameters\": {\n            \"BaseURL\": {\n                \"type\": \"string\"\n            },\n            \"Username\": {\n                \"type\": \"string\"\n            },\n            \"Secret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"typeProperties\": {\n            \"url\": \"@{linkedService().BaseURL}\",\n            \"enableServerCertificateValidation\": false,\n            \"authenticationType\": \"Windows\",\n            \"userName\": \"@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@linkedService().Secret\"\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <p>Note</p> <p>The Windows username should has domain name before username:</p> <pre><code>domain\\\\username\n</code></pre> <p>Note</p> <p>If you want to use <code>AutoResolveIntegrationRuntime</code>, you can delete key <code>connectVia</code> from above json data.</p>"},{"location":"services/azure/database/az-db-auth/","title":"Auth","text":""},{"location":"services/azure/database/az-db-auth/#users-roles","title":"Users &amp; Roles","text":""},{"location":"services/azure/database/az-db-auth/#getting-users","title":"Getting Users","text":"AllExternal UsersExternal Group UsersSQL UserSQL User without login <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\n</code></pre> <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\nWHERE [type] like 'E'\n</code></pre> <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\nWHERE [type] = 'X'\n</code></pre> <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\nWHERE [type] = 'S'\n</code></pre> <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\nWHERE [type] = 'S'\n</code></pre> <p>Note</p> <p>If you want to list of users on server, you can change information table to</p> <pre><code>...\nFROM [sys].[database_principals]\n...\n</code></pre>"},{"location":"services/azure/database/az-db-auth/#create-user","title":"Create User","text":"External UserExternal GroupSQL UserSQL User without login <pre><code>USE [master];\nCREATE LOGIN [username@email.com] FROM EXTERNAL PROVIDER;\nGO\nUSE [database];\nCREATE USER [username@email.com] FROM LOGIN [username@email.com];\nGO\n</code></pre> <pre><code>USE [master];\nCREATE LOGIN [groupname@email.com] FROM EXTERNAL PROVIDER;\nGO\nUSE [database];\nCREATE USER [groupname@email.com] FROM LOGIN [groupname@email.com];\nGO\n</code></pre> <pre><code>USE [master];\nCREATE LOGIN [username@email.com] WITH PASSWORD = 'P@ssW0rd';\nGO\nUSE [database];\nCREATE USER [username@email.com] FROM LOGIN [username@email.com];\nGO\n</code></pre> <pre><code>USE [database];\nCREATE USER [username@email.com] WITHOUT LOGIN;\nGRANT IMPERSONATE ON USER::[username@email.com] TO [anothername@email.com];\nGO\n</code></pre> <p>Note</p> <p>If you want to delete user,</p> <pre><code>USE [database];\nDROP USER [username@email.com];\nGO\nUSE [master];\nDROP LOGIN [username@email.com];\nGO\n</code></pre>"},{"location":"services/azure/database/az-db-auth/#relationship-of-users-and-roles","title":"Relationship of Users and Roles","text":"<pre><code>SELECT\n    r.[name]                                    AS [Role]\n    , ISNULL(m.[name], 'No members')            AS [Member]\n    , m.create_date                             AS [Created Date]\n    , m.modify_Date                             AS [Modified Date]\nFROM\n    [sys].[database_role_members]               AS rm\nRIGHT OUTER JOIN [sys].[database_principals]    AS r\n    ON rm.[role_principal_id] = r.[principal_id]\nLEFT OUTER JOIN [sys].[database_principals]     AS m\n    ON rm.[member_principal_id] = m.[principal_id]\nWHERE\n    r.[type] = 'R'\nORDER BY\n    r.[name]\n    , ISNULL(m.[name], 'No members')\n;\n</code></pre> <pre><code>Role         |Member          |Created Date           |Modified Date          |\n-------------+----------------+-----------------------+-----------------------+\nDATA ENGINEER|demo@mail.com   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_owner     |dbo             |2003-04-08 00:00:00.000|2021-09-21 00:00:00.000|\ndb_owner     |admin@mail.com  |2021-05-12 00:00:00.000|2021-05-12 00:00:00.000|\ndb_ddladmin  |DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datareader|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datawriter|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\n</code></pre>"},{"location":"services/azure/database/az-db-auth/#create-role","title":"Create Role","text":"<pre><code>CREATE ROLE [role-name];\nALTER ROLE [role-name] ADD MEMBER [username@email.com];\nGO\n</code></pre> <p>Note</p> <p>If you want to remove user from role, you should use</p> <pre><code>ALTER ROLE [role-name] DROP MEMBER [username@email.com];\n</code></pre>"},{"location":"services/azure/database/az-db-auth/#permissions","title":"Permissions","text":""},{"location":"services/azure/database/az-db-auth/#grant","title":"Grant","text":"AllOperationManage DatabaseManage LoginLoad DataSchemaCreator <pre><code>GRANT ALL PRIVILEGES ON DATABASE [database] TO [username@email.com];\n</code></pre> <pre><code>USE [master];\n\n-- Monitor the Appliance\nGRANT VIEW SERVER STATE TO [username@email.com];\n\n-- Terminate Connections\nGRANT ALTER ANY CONNECTION TO [username@email.com];\n\nGO\n</code></pre> <pre><code>USE [database];\n\n-- Manage Databases\nGRANT CONTROL ON DATABASE::[database] TO [username@email.com];\n\nGO\n</code></pre> <pre><code>USE [master]\n\n-- Manage and add logins\nGRANT ALTER ANY LOGIN TO [username@email.com];\n\n-- Grant permissions to view sessions and queries\nGRANT VIEW SERVER STATE TO [username@email.com];\n\n-- Grant permission to end sessions\nGRANT ALTER ANY CONNECTION TO [username@email.com];\nGO\n\nUSE [database];\n\n-- Grant permissions to create and drop users\nGRANT ALTER ANY USER TO [username@email.com];\n\n-- Grant permissions to create and drop roles\nGRANT ALTER ANY ROLE TO [username@email.com];\n\nGO\n</code></pre> <pre><code>USE [master];\n\n-- Grant BULK Load permissions\nGRANT ADMINISTER BULK OPERATIONS TO [username@email.com];\n\nGO\n\nUSE [database];\n\nGRANT CREATE TABLE ON DATABASE::[database] TO [username@email.com];\n\n-- On Schema Usage\nGRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA [schemaname] TO [username@email.com];\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA [schemaname] TO [username@email.com];\n\nGO\n</code></pre> <pre><code>USE [database];\n\nALTER AUTHORIZATION ON SCHEMA::[schemaname] to [username@email.com];\nGRANT USAGE ON SCHEMA::[schemaname] TO [username@email.com];\nGRANT ALTER ON SCHEMA::[schemaname] TO [username@email.com];\n\nGO\n</code></pre> <pre><code>USE [database];\n\nGRANT CREATE TABLE, CREATE VIEW, CREATE PROCEDURE TO [username@email.com];\n\nGO\n</code></pre> <p>Note</p> <p>If you want to revoke granted permission, you can use:</p> <pre><code>REVOKE ...;\n</code></pre> <p>The REVOKE statement can be used to remove granted permissions, and the DENY statement can be used to prevent a principal from gaining a specific permission through a GRANT</p> <p>Example</p> <pre><code>USE Adventureworks\nDROP TABLE IF EXISTS dbo.mytable, dbo.mytable2, Sales.mytable, Sales.mytable2\nDROP USER IF EXISTS TestRole\n\nCREATE USER TestRole WITHOUT LOGIN\nGRANT CREATE TABLE to TestRole\n\nGRANT ALTER ON SCHEMA :: dbo To TestRole;\nEXECUTE AS USER = 'TestRole'\n--OK\nCREATE TABLE dbo.mytable(c1 int)\nGO\n--Fails\nCREATE TABLE Sales.mytable(c1 int)\nGO\nREVERT\nREVOKE ALTER ON SCHEMA :: dbo To TestRole;\n\nGRANT ALTER To TestRole;\nEXECUTE AS USER = 'TestRole'\n--OK\nCREATE TABLE dbo.mytable2(c1 int)\nGO\n--OK\nCREATE TABLE Sales.mytable2(c1 int)\nGO\nREVERT\n</code></pre>"},{"location":"services/azure/database/az-db-auth/#impersonate","title":"Impersonate","text":"<pre><code>GRANT IMPERSONATE ON USER::[username@mail.com] TO [targetname@mail.com];\nGO\nEXECUTE AS USER = 'username@mail.com';\n...\nREVERT;\nGO\n</code></pre> <p>Note</p> <p>If you want to revoke impersonate, you can use:</p> <pre><code>REVOKE IMPERSONATE ON USER::[&lt;user@mail.com&gt;] TO [&lt;target@mail.com&gt;];\n</code></pre>"},{"location":"services/azure/database/az-db-auth/#relationship-of-permissions-and-objects","title":"Relationship of Permissions and Objects","text":"<pre><code>SELECT\n    dp.[name]                                   AS [Principle]\n    , dp.[type_desc]                            AS [Principal Type]\n    , o.[name]                                  AS [Object Name]\n    , p.[permission_name]                       AS [Permission]\n    , p.[state_desc]                            AS [Permission State]\nFROM [sys].[database_permissions]               AS p\nLEFT OUTER JOIN [sys].[all_objects]             AS o\n    ON p.[major_id] = o.[OBJECT_ID]\nINNER JOIN [sys].[database_principals]          AS dp\n    ON p.[grantee_principal_id] = dp.[principal_id]\n</code></pre> <pre><code>Principle        |Principal Type|Object Name              |Permission|Permission State|\n-----------------+--------------+-------------------------+----------+----------------+\ndbo              |SQL_USER      |                         |CONNECT   |GRANT           |\nDWHCTRLADMIN     |SQL_USER      |                         |CONNECT   |GRANT           |\nusername@scg.com |EXTERNAL_USER |                         |CONNECT   |GRANT           |\npublic           |DATABASE_ROLE |query_store_query_variant|SELECT    |GRANT           |\n</code></pre> On Schema <pre><code>SELECT\n    state_desc\n    ,permission_name\n    ,'ON'\n    ,class_desc\n    ,SCHEMA_NAME(major_id)\n    ,'TO'\n    ,USER_NAME(grantee_principal_id)\nFROM [sys].[database_permissions]       AS PERM\nJOIN [sys].[database_principals]        AS Prin\n    ON PERM.[major_ID] = Prin.[principal_id]\n    AND class_desc = 'SCHEMA'\nWHERE\n    user_name(grantee_principal_id) = 'username@email.com'\n</code></pre> <pre><code>state_desc|permission_name|  |class_desc|             |  |                  |\n----------+---------------+--+----------+-------------+--+------------------+\nGRANT     |EXECUTE        |ON|SCHEMA    |             |TO|public            |\nGRANT     |SELECT         |ON|SCHEMA    |             |TO|public            |\nGRANT     |ALTER          |ON|SCHEMA    |DWHCURATED   |TO|username@email.com|\nGRANT     |EXECUTE        |ON|SCHEMA    |DWHMDL       |TO|username@email.com|\nGRANT     |EXECUTE        |ON|SCHEMA    |DWHMDL       |TO|de_vendor         |\n</code></pre> <p>Note</p> <p>For more detail, you can follew store procedure statement, sp_dbpermissions</p>"},{"location":"services/azure/database/az-db-auth/#read-mores","title":"Read Mores","text":"<ul> <li> SQL Server Create User and Grant Permission</li> </ul>"},{"location":"services/azure/database/az-db-monitoring/","title":"Monitoring","text":""},{"location":"services/azure/database/az-db-monitoring/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/database/az-db-monitoring/#partition-state","title":"Partition State","text":"<pre><code>SELECT\n    OBJECT_NAME(s.object_id)        AS [object]\n    ,i.[name]                       AS IndexName\n    ,SUM(s.[used_page_count]) * 8   AS IndexSizeKB\nFROM sys.dm_db_partition_stats      AS s\nINNER JOIN sys.indexes              AS i\n    ON s.[object_id] = i.[object_id]\n    AND s.[index_id] = i.[index_id]\nGROUP BY\n    s.[object_id], i.[name]\nORDER BY\n    OBJECT_NAME(s.object_id), i.[name]\n</code></pre>"},{"location":"services/azure/databricks/","title":"Azure Databricks","text":""},{"location":"services/azure/databricks/#references","title":"References","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/databricks/getting-started/overview</li> </ul>"},{"location":"services/azure/databricks/adb-init-script/","title":"Init Script","text":"<p>Init Script (Initialization Script) is a shell script that runs during startup of each cluster node before the Apache Spark driver or executor JVM starts.</p> <p>Danger</p> <p>Legacy Global and Legacy Cluster-Named init scripts run before other init scripts. These init scripts might be present in workspaces created before February 21, 2023.</p>"},{"location":"services/azure/databricks/adb-init-script/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/databricks/adb-init-script/#cluster-scoped-init-scripts","title":"Cluster-Scoped init scripts","text":"<p>We want to initialize some program before a cluster started like:</p> init_script.sh<pre><code>#!/bin/bash\n\necho \"Start running init script: adb-default\"\necho \"Running on the driver? $DB_IS_DRIVER\"\necho \"Driver IP: $DB_DRIVER_IP\"\n\ntimedatectl set-timezone Asia/Bangkok\n</code></pre> <p>To use the UI to configure a cluster to run an init script, complete the following steps:</p> <ul> <li>On the Cluster Configuration Page  Click the Advanced Options toggle</li> <li>At the bottom of the page  click the Init Scripts tab</li> <li>In the Destination drop-down, select the Workspace destination type</li> <li>Specify a path to the init script like <code>SYS/init_script.sh</code>    Click Add.</li> </ul> <p>Note</p> <p>Each user has a Home directory configured under the <code>/Users</code> directory in the workspace. If a user with the name <code>user1@databricks.com</code> stored an init script called <code>my-init.sh</code> in their home directory, the configured path would be <code>/Users/user1@databricks.com/my-init.sh</code>.</p>"},{"location":"services/azure/databricks/adb-init-script/#global-init-scripts","title":"Global init scripts","text":"<ul> <li>Go to the Admin Settings  Click <code>Global Init Scripts</code></li> <li>Click Add  Name the script and enter it by typing,   pasting, or dragging a text file into the Script field.</li> </ul>"},{"location":"services/azure/databricks/adb-init-script/#read-mores","title":"Read Mores","text":"<ul> <li> Databricks - Init Scripts</li> <li> Global Init Script in Azure databricks</li> </ul>"},{"location":"services/azure/databricks/adb-mount-storage/","title":"Mount Azure Storage","text":"<p>List of mounts:</p> <pre><code>display(dbutils.fs.mounts())\n</code></pre>"},{"location":"services/azure/databricks/adb-mount-storage/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code>dbutils.fs.mount(\n    source = \"wasbs://&lt;container-name&gt;@&lt;storage-account-name&gt;.blob.core.windows.net\",\n    mount_point = \"/mnt/&lt;mount-path&gt;\",\n    extra_configs = {\n        \"fs.azure.account.key.&lt;storage-account-name&gt;.blob.core.windows.net\":\n        dbutils.secrets.get(scope=\"&lt;scope-name&gt;\", key=\"adls-account-key\"),\n    }\n)\n</code></pre>"},{"location":"services/azure/databricks/adb-mount-storage/#datalake","title":"DataLake","text":"<pre><code>adls_account = \"&lt;storage-account-name&gt;\"\nadls_container = \"&lt;container-name&gt;\"\nadls_dir = \"&lt;dir-path&gt;\"\nmount_point = \"/mnt/&lt;mount-path&gt;\"\n\nclient_id = dbutils.secrets.get(scope=\"&lt;scope-name&gt;\", key=\"adb-client-id\")\nclient_secret_id = dbutils.secrets.get(scope=\"&lt;scope-name&gt;\", key=\"adb-client-secrete-id\")\ntenant_id = dbutils.secrets.get(scope=\"&lt;scope-name&gt;\", key=\"adb-tenant-id\")\n\nendpoint: str = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n\nif adls_dir:\n    source: str = f\"abfss://{adls_container}@{adls_account}.dfs.core.windows.net/{adls_dir}\"\nelse:\n    source: str = f\"abfss://{adls_container}@{adls_account}.dfs.core.windows.net\"\n\n# Connecting using Service Principal secrets and OAuth\nconfigs: Dict[str, str] = {\n    \"fs.azure.account.auth.type\": \"OAuth\",\n    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n    \"fs.azure.account.oauth2.client.id\": client_id,\n    \"fs.azure.account.oauth2.client.secret\": client_secret_id,\n    \"fs.azure.account.oauth2.client.endpoint\": endpoint\n}\n\n# Mount ADLS Storage to DBFS only if the directory is not already mounted\nif not any(\n        mount.mountPoint == mount_point\n        for mount in dbutils.fs.mounts()\n):\n    dbutils.fs.mount(\n        source = source,\n        mount_point = mount_point,\n        extra_configs = configs\n    )\n</code></pre>"},{"location":"services/azure/databricks/adb-mount-storage/#unmount-command","title":"Unmount command","text":"<pre><code>mount_point = \"/mnt/&lt;mount-path&gt;\"\n\nif any(\n        mount.mountPoint == mount_point\n        for mount in dbutils.fs.mounts()\n):\n    dbutils.fs.unmount(mount_point)\n</code></pre>"},{"location":"services/azure/databricks/adb-mount-storage/#references","title":"References","text":"<ul> <li>https://vvin.medium.com/mount-adls-gen2-to-databricks-file-system-using-service-principal-oauth-2-0-47527e339178</li> <li>https://docs.databricks.com/en/storage/azure-storage.html</li> </ul>"},{"location":"services/azure/databricks/adb-to-azure-sql/","title":"To Azure SQL","text":""},{"location":"services/azure/databricks/adb-to-azure-sql/#azure-service-principal","title":"Azure Service Principal","text":""},{"location":"services/azure/databricks/adb-to-azure-sql/#1-create-service-principal","title":"1) Create Service Principal","text":"<p>To register your application and create your service principal:</p> <ul> <li>Go to <code>Azure Active Directory</code> <code>App registrations</code> <code>New registration</code></li> <li>Add the information of this app like <code>name</code> is <code>cnct-adb-dev</code> (The name of app   should be formatted like <code>{app}-{resource-shortname}-{environment}</code>)</li> <li>Click register for create</li> </ul> <p>You will then be required to generate a secret:</p> <ul> <li>Go to <code>App registrations</code> <code>Certificates&amp;secrets</code> <code>New Client Secret</code></li> <li>Save this value to <code>Azure Key Vaults</code></li> </ul> <p>Note</p> <p>We write both the <code>Client ID</code> and <code>Secret</code> to Key Vault for a number of reasons:</p> <ul> <li>The <code>Secret</code> is sensitive and like a <code>Storage Key</code> or <code>Password</code>, we don't want    this to be hardcoded or exposed anywhere in our application.</li> <li>Normally we would have an instance of <code>Databricks</code> and <code>Key Vault</code> per environment    and when we come to referencing the secrets, we want the secrets names to remain    the same, so the code in our Databricks notebooks referencing the <code>Secrets</code> doesn't    need to be modified when we deploy to different environments.</li> </ul> <p>Abstract</p> <p>The App Registration is the template used to create the security principal (like a User) which can be authenticated and authorized.</p>"},{"location":"services/azure/databricks/adb-to-azure-sql/#2-create-user-in-azure-sql","title":"2) Create User in Azure SQL","text":"<p>The app registration still needs permission to log into <code>Azure SQL</code> and access the objects within it. You\u2019ll need to Create that user (App &amp; Service Principal) in the database and then grant it permissions on the underlying objects.</p> <pre><code>CREATE USER [cnct-adb-dev] FROM EXTERNAL PROVIDER;\n</code></pre> <p>Permission:</p> Read OnlyOverwrite <pre><code>GRANT SELECT ON SCHEMA::dbo TO [cnct-adb-dev];\n</code></pre> <pre><code>GRANT SELECT ON SCHEMA::dbo TO [cnct-adb-dev];\nGRANT INSERT ON SCHEMA::dbo TO [cnct-adb-dev];\nGRANT ALTER ON SCHEMA::dbo TO [cnct-adb-dev];\nGRANT CREATE TABLE TO [cnct-adb-dev];\n</code></pre>"},{"location":"services/azure/databricks/adb-to-azure-sql/#3-connection-code","title":"3) Connection Code","text":""},{"location":"services/azure/databricks/adb-to-azure-sql/#method-01-spark-connector","title":"Method 01: Spark Connector","text":"<p>To connect to <code>Azure SQL</code>, you will need to install the SQL Spark Connector and the Microsoft Azure Active Directory Authentication Library (ADAL) for Python code.</p> <ul> <li> <p>Go to your cluster in Databricks and Install necessary packages:</p> </li> <li> <p>Maven: <code>com.microsoft.azure:spark-mssql-connector_2.12_3.0:1.0.0-alpha</code></p> </li> <li> <p>PYPI: <code>adal</code></p> </li> <li> <p>Also, if you haven\u2019t already, Create a Secret Scope   to your <code>Azure Key Vault</code> where your <code>Client ID</code>, <code>Secret</code>, and <code>Tenant ID</code> have   been generated.</p> </li> <li> <p>Get <code>Access Token</code> from Service Principle authentication request</p> </li> </ul> <pre><code>import adal\n\ncontext = adal.AuthenticationContext(\n    f\"https://login.windows.net/{dbutils.secrets.get(scope='defaultScope', key='TenantId')}\"\n)\ntoken = context.acquire_token_with_client_credentials(\n    \"https://database.windows.net/\",\n    dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"),\n    dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"),\n)\naccess_token = token[\"accessToken\"]\n</code></pre> Read TableWrite Table <pre><code>df = (\n    spark.read\n        .format(\"com.microsoft.sqlserver.jdbc.spark\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;server-instance-name&gt;.database.windows.net\")\n        .option(\"databaseName\", \"{dev}\")\n        .option(\"accessToken\", access_token)\n        .option(\"encrypt\", \"true\")\n        .option(\"hostNameInCertificate\", \"*.database.windows.net\")\n        .option(\"dbtable\", \"[dbo].[&lt;table-name&gt;]\")\n        .option(\"batchsize\", 2500)\n        .option(\"mssqlIsolationLevel\", \"READ_UNCOMMITTED\")\n        .load()\n)\n</code></pre> <p>Note</p> <p>This connector by default uses <code>READ_COMMITTED</code> isolation level when performing the bulk insert into the database. If you wish to override the isolation level, use the <code>mssqlIsolationLevel</code> option as show above.</p> <pre><code>(\n    df.write\n        .format(\"com.microsoft.sqlserver.jdbc.spark\")\n        .mode(\"append\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;server-instance-name&gt;.database.windows.net\")\n        .option(\"dbtable\", \"[dbo].[&lt;table-name&gt;]\")\n        .option(\"accessToken\", access_token)\n        .option(\"schemaCheckEnabled\", \"false\")\n        .save()\n)\n</code></pre> <p>Note</p> <p>When <code>schemaCheckEnabled</code> is <code>false</code>, we can write to the destination table which has less column than dataframe.</p> <p>Read More</p> <p>Note</p> <p>Executing custom SQL through the connector. The previous Azure SQL Connector for Spark provided the ability to execute custom SQL code like DML or DDL statements through the connector. This functionality is out-of-scope of this connector since it is based on the DataSource APIs. This functionality is readily provided by libraries like <code>pyodbc</code>, or you can use the standard java sql interfaces as well.</p>"},{"location":"services/azure/databricks/adb-to-azure-sql/#method-02-jdbc-connector","title":"Method 02: JDBC Connector","text":"<p>This method reads or writes the data row by row, resulting in performance issues. Not Recommended.</p> <p>In Databricks Runtime 11.3 LTS and above, you can use the sqlserver keyword to use the included driver for connecting to SQL server.</p> Format JDBCFormat SQLServer ReadRead (Legacy)Write AppendWrite Overwrite <pre><code>df = (\n    spark.read\n        .format(\"jdbc\")\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;host&gt;:1433;\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .option(\"encrypt\", \"true\")\n        .load()\n)\n</code></pre> <pre><code>df = (\n    spark.read\n        .jdbc(\n            url=\"jdbc:sqlserver://&lt;host&gt;:1433;database=&lt;database&gt;\",\n            table=\"&lt;schema-name&gt;.&lt;table-name&gt;\",\n            properties={\n                \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n                \"authentication\": \"ActiveDirectoryServicePrincipal\",\n                \"UserName\": dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"),\n                \"Password\": dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"),\n            },\n        )\n)\n</code></pre> <pre><code>(\n    df.write\n        .mode(\"append\")\n        .format(\"jdbc\")\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;host&gt;:1433;\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .save()\n)\n</code></pre> <pre><code>(\n    df.write\n        .mode(\"overwrite\")\n        .format(\"jdbc\")\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;host&gt;:1433;\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .save()\n)\n</code></pre> ReadWrite AppendWrite Overwrtie <pre><code>df = (\n    spark.read\n        .format(\"sqlserver\")\n        .option(\"host\", \"&lt;host-name&gt;.database.windows.net\")\n        .option(\"port\", \"1433\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .option(\"encrypt\", \"true\")\n        .option(\"hostNameInCertificate\", \"*.database.windows.net\")\n        .load()\n)\n</code></pre> <pre><code>(\n    df.write\n        .mode(\"append\")\n        .format(\"sqlserver\")\n        .option(\"host\", \"&lt;host:***.database.windows.net&gt;\")\n        .option(\"port\", \"1433\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .save()\n)\n</code></pre> <pre><code>(\n    df.write\n        .mode(\"overwrite\")\n        .format(\"sqlserver\")\n        .option(\"host\", \"&lt;host:***.database.windows.net&gt;\")\n        .option(\"port\", \"1433\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .option(\"truncate\", true)\n        save()\n)\n</code></pre> <p>When using mode <code>overwrite</code> if you do not use the option truncate on recreation of the table, indexes will be lost. , a columnstore table would now be a heap. If you want to maintain existing indexing please also specify option truncate with value true. For example, <code>.option(\"truncate\",\"true\")</code>.</p> <p>Microsoft: Spark - SQL Server Connector</p> <p>References:</p> <ul> <li>Databricks: External Data - SQL Server</li> <li>Apache Spark: SQL Data Sources - JDBC</li> </ul>"},{"location":"services/azure/databricks/adb-to-azure-sql/#sql-authentication","title":"SQL Authentication","text":""},{"location":"services/azure/databricks/adb-to-azure-sql/#1-create-sql-user-password","title":"1) Create SQL User &amp; Password","text":"<pre><code>USE [master];\nCREATE LOGIN [cnct-adb-dev] WITH PASSWORD = 'P@ssW0rd'\nGO\n\nUSE [&lt;database-name&gt;];\nCREATE USER [cnct-adb-dev] FOR LOGIN [cnct-adb-dev];\nGO\n</code></pre> <p>Grant Permission:</p> Read OnlyOverwrite <pre><code>GRANT SELECT ON SCHEMA::dbo TO [cnct-adb-dev];\n</code></pre> <pre><code>GRANT SELECT ON SCHEMA::dbo TO [cnct-adb-dev];\nGRANT INSERT ON SCHEMA::dbo TO [cnct-adb-dev];\nGRANT CREATE TABLE TO [cnct-adb-dev];\nGRANT ALTER ON SCHEMA::dbo TO [cnct-adb-dev];\n</code></pre>"},{"location":"services/azure/databricks/adb-to-azure-sql/#2-connection-code","title":"2) Connection Code","text":""},{"location":"services/azure/databricks/adb-to-azure-sql/#method-01-odbc-connector","title":"Method 01: ODBC Connector","text":"<p>Install ODBC Driver on cluster:</p> <pre><code>%sh\ncurl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nsudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17\n</code></pre> <pre><code>import pyodbc\n\nconn = pyodbc.connect(\n    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n    f'SERVER=&lt;host&gt;;DATABASE=&lt;database_name&gt;;UID=[cnct-adb-dev];PWD=P@ssW0rd;'\n    f'Authentication=SqlPassword;Encrypt=yes;'\n)\n</code></pre> <p>Reference:</p> <ul> <li>StackOverFlow: Using PyODBC in Azure Databricks for Connect to SQL Server</li> <li>Microsoft: SQL ODBC - Using Azure AD</li> </ul>"},{"location":"services/azure/databricks/adb-to-azure-sql/#method-02-jdbc-connector_1","title":"Method 02: JDBC Connector","text":"Format SQLServerFormat JDBC <pre><code>df = (\n    spark.read\n        .format(\"sqlserver\")\n        .option(\"host\", \"hostName\")\n        .option(\"port\", \"1433\")\n        .option(\"user\", \"[cnct-adb-dev]\")\n        .option(\"password\", \"password\")\n        .option(\"database\", \"databaseName\")\n        .option(\"dbtable\", \"schemaName.tableName\") # (if schemaName not provided, default to \"dbo\")\n        .load()\n)\n</code></pre> <pre><code>df = (\n    spark.read\n        .format(\"jdbc\")\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;host-url&gt;:1433;database=&lt;database-name&gt;\")\n        .option(\"dbtable\", \"tableName\")\n        .option(\"user\", \"[cnct-adb-dev]\")\n        .option(\"password\", \"password\")\n        .load()\n)\n</code></pre>"},{"location":"services/azure/databricks/adb-to-azure-sql/#references","title":"References","text":"<ul> <li>TheDataSwamp: Databricks</li> <li>Databricks: External Data - SQL Server</li> <li>Microsoft</li> <li>Microsoft JDBC Driver for SQL Server</li> </ul>"},{"location":"services/azure/databricks/adb-to-bigquery/","title":"Databricks: To BigQuery","text":""},{"location":"services/azure/databricks/adb-to-bigquery/#using-json-encoding","title":"Using JSON Encoding","text":"<pre><code>credentials_json_str: str = dbutils.secrets.get(scope=\"&lt;scope-name&gt;\", key=\"&lt;secret-key-name&gt;\")\n\ndf = (\n    spark.read\n        .format(\"bigquery\")\n        .option(\"credentials\", base64.b64encode(credentials_json_str.encode()).decode('utf-8'))\n        .option(\"parentProject\", \"&lt;project-id&gt;\")\n        .option(\"table\", \"&lt;dataset&gt;.&lt;table-name&gt;\")\n        .load()\n)\ndf.show()\n</code></pre> <p>Bug</p> <p>GitHub: Error getting access token from metadata server</p>"},{"location":"services/azure/databricks/adb-to-bigquery/#using-google_application_credentials","title":"Using GOOGLE_APPLICATION_CREDENTIALS","text":"<pre><code>import os\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"&lt;/path/to/key/file&gt;\"\n</code></pre>"},{"location":"services/azure/databricks/adb-to-bigquery/#using-filepath","title":"Using Filepath","text":"<pre><code>df = (\n    spark.read\n        .format(\"bigquery\")\n        .option(\"credentialsFile\", \"&lt;/path/to/key/file&gt;\")\n        .option(\"table\", \"&lt;dataset&gt;.&lt;table-name&gt;\")\n        .load()\n)\n</code></pre>"},{"location":"services/azure/databricks/adb-to-bigquery/#access-token","title":"Access Token","text":"<pre><code># Globally\nspark.conf.set(\"gcpAccessToken\", \"&lt;access-token&gt;\")\n\n# Per read/Write\ndf = (\n    spark.read\n        .format(\"bigquery\")\n        .option(\"gcpAccessToken\", \"&lt;acccess-token&gt;\")\n)\n</code></pre>"},{"location":"services/azure/databricks/adb-to-bigquery/#references","title":"References","text":"<ul> <li>(https://docs.databricks.com/en/external-data/bigquery.html#step-2-set-up-databricks)</li> <li>https://github.com/GoogleCloudDataproc/spark-bigquery-connector</li> </ul>"},{"location":"services/azure/databricks/adb-to-eventhubs/","title":"To EventHubs","text":"<p>Azure Event Hubs with Spark</p> <pre><code>groupId = com.microsoft.azure\nartifactId = azure-eventhubs-spark_2.12\nversion = 2.3.22\n</code></pre> <p>Warning</p> <p>We should use 1 consumer group per query stream because it will raise <code>ReceiverDisconnectedException</code>. Read More about multiple readers.</p>"},{"location":"services/azure/databricks/adb-to-eventhubs/#using-shared-access-key","title":"Using Shared Access Key","text":""},{"location":"services/azure/databricks/adb-to-eventhubs/#connection-code","title":"Connection Code","text":"EventhubKafka Protocol <pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession\n        .builder\n        .appName('App Connect Eventhub')\n        .config(\"spark.jars.packages\", \"com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.22\")\n        .config(\"spark.locality.wait\", \"15s\")  # Default: 3s\n        .getOrCreate()\n)\n</code></pre> <pre><code>connectionString: str = (\n    f\"Endpoint=sb://{eventhubs_namespace}.servicebus.windows.net/;\"\n    f\"SharedAccessKeyName={sharekey_name};\"\n    f\"SharedAccessKey={sharekey};\"\n    f\"EntityPath={eventhubs_name}\"\n)\nehConf = {\n  'eventhubs.connectionString' : spark._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n  'eventhubs.consumerGroup' : \"$Default\",\n}\n</code></pre> <pre><code>df = (\n  spark\n    .readStream\n    .format(\"eventhubs\")\n    .options(**ehConf)\n    .option(\"maxEventsPerTrigger\", 1_000_000)  # Default: &lt;partition&gt; * 1_000\n    .option(\"useExclusiveReceiver\", False)  # Default: True\n    .option(\"receiverTimeout\", \"PT000100\")  # Default: 60 sec\n    .option(\"operationTimeout\", \"PT000100\")  # Default: 60 sec\n    .load()\n)\n</code></pre> <p>Note</p> <p>This option require enable Kafka on the Azure Event Hubs.</p> <pre><code>connectionString: str = (\n    f\"Endpoint=sb://{eventhubs_namespace}.servicebus.windows.net/;\"\n    f\"SharedAccessKeyName={sharekey_name};\"\n    f\"SharedAccessKey={sharekey};\"\n    f\"EntityPath={eventhubs_name}\"\n)\nEH_SASL: str = (\n    f'org.apache.kafka.common.security.plain.PlainLoginModule required '\n    f'username=\"$ConnectionString\" '\n    f'password=\"{connectionString}\";'\n)\n</code></pre> <pre><code>df = (\n    spark\n        .readStream\n        .format(\"kafka\")\n        .option(\"subscribe\", f\"{eventhubs_name}\")\n        .option(\"kafka.bootstrap.servers\", f\"{eventhubs_namespace}.servicebus.windows.net:9093\")\n        .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n        .option(\"kafka.security.protocol\", \"SASL_SSL\")\n        .option(\"kafka.sasl.jaas.config\", EH_SASL)\n        .option(\"kafka.request.timeout.ms\", \"60000\")\n        .option(\"kafka.session.timeout.ms\", \"30000\")\n        .option(\"kafka.group.id\", \"$Default\")\n        .option(\"failOnDataLoss\", \"true\")\n        .option(\"spark.streaming.kafka.allowNonConsecutiveOffsets\", \"true\")\n        .load()\n)\n</code></pre>"},{"location":"services/azure/databricks/adb-to-eventhubs/#read-mores","title":"Read Mores","text":"<ul> <li>https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#user-configuration</li> <li>https://medium.com/@kaviprakash.2007/structured-streaming-using-azure-databricks-and-event-hub-6b0bcbf029c4</li> <li>Connect your Apache Spark application with Azure Event Hubs</li> <li>Using Apache Spark with Azure Event Hubs for Apache Kafka Ecosystems</li> </ul>"},{"location":"services/azure/databricks/adb-to-iothub/","title":"To IoT Hub","text":"<p>Azure Event Hubs with Spark</p> <pre><code>groupId = com.microsoft.azure\nartifactId = azure-eventhubs-spark_2.12\nversion = 2.3.22\n</code></pre>"},{"location":"services/azure/databricks/adb-to-iothub/#using-shared-access-key","title":"Using Shared Access Key","text":""},{"location":"services/azure/databricks/adb-to-iothub/#connection-code","title":"Connection Code","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession\n        .builder\n        .appName('App Connect IoT Hub')\n        .config(\"spark.jars.packages\", \"com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.22\")\n        .config(\"spark.locality.wait\", \"15s\")  # Default: 3s\n        .getOrCreate()\n)\n</code></pre> <pre><code>connectionString: str = (\n    f\"Endpoint=sb://{eventhubs_compatible_name}.servicebus.windows.net/;\"\n    f\"SharedAccessKeyName={sharekey_name};\"\n    f\"SharedAccessKey={sharekey};\"\n    f\"EntityPath={endpoint_name}\"\n)\nehConf = {\n    'eventhubs.connectionString' : spark._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n    'eventhubs.consumerGroup' : \"$Default\",\n    'eventhubs.partition.count': \"1\",\n    'ehName': f\"{IoTHubs-EventHub-Compatible-Name}\",\n}\n</code></pre> <pre><code>df = (\n  spark\n    .readStream\n    .format(\"eventhubs\")\n    .options(**ehConf)\n    .option(\"maxEventsPerTrigger\", 1_000_000)  # Default: &lt;partition&gt; * 1_000\n    .option(\"useExclusiveReceiver\", False)  # Default: True\n    .option(\"receiverTimeout\", \"PT000100\")  # Default: 60 sec\n    .option(\"operationTimeout\", \"PT000100\")  # Default: 60 sec\n    .load()\n)\n</code></pre>"},{"location":"services/azure/databricks/adb-to-iothub/#read-mores","title":"Read Mores","text":"<ul> <li>https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#user-configuration</li> <li>https://medium.com/@kaviprakash.2007/structured-streaming-using-azure-databricks-and-event-hub-6b0bcbf029c4</li> </ul>"},{"location":"services/azure/databricks/adb-to-kinesis/","title":"To Kinesis","text":"<p>On July 11, 2017, we announced the general availability of Apache Spark 2.2.0 as part of Databricks Runtime 3.0 (DBR) for the Unified Analytics Platform. To augment the scope of Structured Streaming on DBR, we support AWS Kinesis Connector as a source (to read streams from), giving developers the freedom to do three things.</p> <p></p> <p>Warning</p> <p>In Databricks Runtime 11.3 LTS and above, the <code>Trigger.Once</code> setting is deprecated. Databricks recommends you use <code>Trigger.AvailableNow</code> for all incremental batch processing workloads.<sup>1</sup></p>"},{"location":"services/azure/databricks/adb-to-kinesis/#iam-policy","title":"IAM Policy","text":"<p>By default, the Kinesis connector resorts to Amazon\u2019s default credential provider chain</p> ReadPut <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"kinesis:Get*\",\n                \"kinesis:DescribeStreamSummary\"\n            ],\n            \"Resource\": [\n                \"arn:aws:kinesis:us-east-1:111122223333:stream/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"kinesis:ListStreams\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"kinesis:PutRecord\"\n            ],\n            \"Resource\": [\n                \"arn:aws:kinesis:us-east-1:111122223333:stream/*\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"services/azure/databricks/adb-to-kinesis/#schema","title":"Schema","text":"<p>Kinesis returns records with the following schema:</p> <pre><code>from pyspark.sql.types import TimestampType, StringType, StructType, StructField, BinaryType\n\nschema = StructType(\n    [\n        StructField(\"partitionKey\", StringType(), True),\n        StructField(\"data\", BinaryType(), False),\n        StructField(\"stream\", StringType(), False),\n        StructField(\"shardId\", StringType(), False),\n        StructField(\"sequenceNumber\", StringType(), False),\n        StructField(\"approximateArrivalTimestamp\", TimestampType(), False),\n    ],\n)\n</code></pre>"},{"location":"services/azure/databricks/adb-to-kinesis/#connection-code","title":"Connection Code","text":"ReadStreamWriteStream <pre><code>df = (\n    spark.readStream\n        .format(\"kinesis\")\n        .option(\"streamName\", \"&lt;aws-kinesis-stream-name&gt;\")\n        .option(\"initialPosition\", \"latest\")\n        .option(\"format\", \"json\")\n        .option(\"awsAccessKey\", \"&lt;aws-access-key&gt;\")\n        .option(\"awsSecretKey\", \"&lt;aws-access-secret-key\")\n        .option(\"region\", \"&lt;aws-region&gt;\")\n        .option(\"inferSchema\", \"true\")\n        .load()\n)\n</code></pre> <p>Note</p> <p>initialPosition:</p> <ul> <li><code>latest</code>: Read from the latest position that data ingest.</li> <li><code>trim_horizon</code> or <code>earliest</code>:  Read all data that keep in shard.</li> <li><code>at_timestamp</code>: Specify time value such as   <code>{\"at_timestamp\": \"06/25/2020 10:23:45 PDT\", \"format\": \"MM/dd/yyyy HH:mm:ss ZZZ\"}</code></li> </ul> <p>Read more about <code>StartingPosition</code></p> <pre><code>(\n    df\n        .writeStream\n        .format(\"kinesis\")\n        .outputMode(\"update\")\n        .option(\"streamName\", \"&lt;aws-kinesis-stream-name&gt;\")\n        .option(\"region\", \"&lt;aws-region&gt;\")\n        .option(\"awsAccessKeyId\", \"&lt;aws-access-key&gt;\")\n        .option(\"awsSecretKey\", \"aws-access-secret-key\")\n        .option(\"checkpointLocation\", \"/path/to/checkpoint\")\n        .start()\n        .awaitTermination()\n)\n</code></pre>"},{"location":"services/azure/databricks/adb-to-kinesis/#references","title":"References","text":"<ul> <li> Apache Spark\u2019s Structured Streaming with Amazon Kinesis on Databricks</li> <li> Connect to Amazon Kinesis</li> <li> Amazon Kinesis Data Streams Connector for Spark Structured Streaming</li> </ul> <ol> <li> <p> Configure Structured Streaming trigger intervals \u21a9</p> </li> </ol>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/","title":"Databricks: To Synapse SQL Pool","text":"<p>When you want to read and write data on Azure Synapse Analytic SQL Pool via Azure Databricks, that has 2 types of Azure Synapse SQL Pool:</p> <ul> <li>Serverless SQL Pool</li> <li>Dedicate SQL Pool</li> </ul> <p>Note</p> <p>Why do we need staging storage?</p> <p>Staging folder is needed to store some temporary data whenever we read/write data from/to <code>Azure Synapse</code>. Whenever we read/write data, we actually leverage PolyBase to move the data, which staging storage is used to achieve high performance.</p>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#access-serverless-sql-pool","title":"Access Serverless SQL Pool","text":""},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#1-create-database-scope","title":"1) Create Database Scope","text":"<p>If you want to see the list of existing database scope credential, you can use this command:</p> <pre><code>SELECT * FROM [sys].[database_scoped_credentials];\n</code></pre>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#2-create-external-data-source","title":"2) Create External Data Source","text":"<p>Create external datasource for connection from Synapse Serverless to Azure Data Lake Storage.</p> <pre><code>IF NOT EXISTS (\n    SELECT *\n    FROM [sys].[external_data_sources]\n    WHERE NAME = 'dataplatdev_curated_adb'\n)\n    CREATE EXTERNAL DATA SOURCE [dataplatdev_curated_adb]\n    WITH (\n        CREDENTIAL = [adb_cred],\n        LOCATION = 'abfss://{curated}@{dataplatdev}.dfs.core.windows.net'\n    );\nGO\n</code></pre> <p>Abstract</p> <pre><code>SELECT * FROM [sys].[external_data_sources];\n</code></pre>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#3-create-user-in-serverless-sql-pool","title":"3) Create User in Serverless SQL Pool","text":"<p>Create login user and grant permission reference above database scope credential</p> <pre><code>CREATE LOGIN [adbuser] WITH PASSWORD = 'Gl2vimQkvpZp'\n;\nGRANT REFERENCES ON DATABASE SCOPED CREDENTIAL::[adb_cred] TO [adbuser];\nGO\n</code></pre> <p>Create temp view for read data from above external datasource</p> <pre><code>CREATE OR ALTER VIEW [CURATED].[VW_DELTA_SALES]\nAS SELECT *\n   FROM OPENROWSET(\n       BULK '/{delta_silver}/{table_sales}',\n       DATA_SOURCE = 'dataplatdev_curated_adb',\n       FORMAT = 'DELTA'\n   ) AS [R]\n;\n\nGRANT SELECT ON OBJECT::[CURATED].[VW_DELTA_SALES] TO [adbuser]\n;\n</code></pre> <p>More Detail, Control storage account access for serverless SQL pool in Azure Synapse Analytics</p>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#4-connection-code","title":"4) Connection Code","text":""},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#method-01-jdbc-connector","title":"Method 01: JDBC Connector","text":"<p>This method reads or writes the data row by row, resulting in performance issues. Not Recommended.</p> <p>Set Spark Config:</p> Azure Data Lake Gen 2Azure Blob Storage <pre><code>spark.conf.set(\n    \"fs.azure.account.key.{dataplatdev}.dfs.core.windows.net\",\n    \"&lt;storage-account-access-key&gt;\"\n)\nsc._jsc.hadoopConfiguration().set(\n    \"fs.azure.account.key.{dataplatdev}.dfs.core.windows.net\",\n    \"&lt;storage-account-access-key&gt;\"\n)\n</code></pre> <pre><code>spark.conf.set(\n    \"fs.azure.account.key.{dataplatdev}.blob.core.windows.net\",\n    \"&lt;storage-account-access-key&gt;\"\n)\nsc._jsc.hadoopConfiguration().set(\n    \"fs.azure.account.key.{dataplatdev}.blob.core.windows.net\",\n    \"&lt;storage-account-access-key&gt;\"\n)\n</code></pre> <p>Make JDBC URL:</p> <pre><code>URL = (\n    f\"jdbc:sqlserver://{server}:1433;database={database};user={username};password={password};\"\n    f\"encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\"\n)\n</code></pre> Azure Data Lake Gen 2Azure Blob Storage <pre><code>df = (\n  spark.read\n      .format(\"jdbc\")\n      .option(\"url\", URL)\n      .option(\"tempDir\", \"abfss://{curated}@{storage-account}.dfs.core.windows.net/&lt;folder-for-temporary-data&gt;\")\n      .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n      .option(\"query\", \"SELECT * FROM [CURATED].[VW_DELTA_SALES]\")\n      .load()\n  )\n</code></pre> <pre><code>df = (\n  spark.read\n      .format(\"jdbc\")\n      .option(\"url\", URL)\n      .option(\"tempDir\", \"wasbs://{curated}@{storage-account}.blob.core.windows.net/&lt;folder-for-temporary-data&gt;\")\n      .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n      .option(\"query\", \"SELECT * FROM [CURATED].[VW_DELTA_SALES]\")\n      .load()\n  )\n</code></pre> <p>Reference:</p> <ul> <li>Microsoft Databricks JDBC</li> <li>Spark SQL Data Source JDBC</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#method-02-spark-connector","title":"Method 02: Spark Connector","text":"<p>This method uses bulk insert to read/write data. There are a lot more options that can be further explored. First Install the Library using Maven Coordinate in the Data-bricks cluster, and then use the below code. Recommended for Azure SQL DB or Sql Server Instance</p> <p>Install Driver on cluster:</p> <ul> <li>Maven: <code>com.microsoft.azure:spark-mssql-connector_2.12:1.2.0</code></li> </ul> SPARK VERSION MAVEN DEPENDENCY Spark 2.4.x groupeId : com.microsoft.azure  artifactId : spark-mssql-connector  version : 1.0.2 Spark 3.0.x groupeId : com.microsoft.azure  artifactId : spark-mssql-connector_2.12  version : 1.1.0 Spark 3.1.x groupeId : com.microsoft.azure  spark-mssql-connector_2.12  version : 1.2.0 <p>Read More Supported Version</p> TableCustom Query <pre><code>df = (\n  spark.read\n    .format(\"com.microsoft.sqlserver.jdbc.spark\")\n    .option(\"url\", \"jdbc:sqlserver://&lt;server-name&gt;:1433;database=&lt;database-name&gt;;\")\n    .option(\"user\", username)\n    .option(\"password\", password)\n    .option(\"mssqlIsolationLevel\", \"READ_UNCOMMITTED\")\n    .option(\"encrypt\", \"true\")\n    .option(\"dbTable\", \"[&lt;schema&gt;].[&lt;table-or-view&gt;]\")\n    .load()\n)\n</code></pre> <pre><code>df = (\n  spark.read\n    .format(\"com.microsoft.sqlserver.jdbc.spark\")\n    .option(\"url\", \"jdbc:sqlserver://&lt;server-name&gt;:1433;database=&lt;database-name&gt;;\")\n    .option(\"user\", username)\n    .option(\"password\", password)\n    .option(\"mssqlIsolationLevel\", \"READ_UNCOMMITTED\")\n    .option(\"encrypt\", \"true\")\n    .option(\"query\", \"SELECT * FROM [sys].[external_data_sources]\")\n    .load()\n)\n</code></pre> <p>Reference:</p> <ul> <li>Microsoft SQL Spark Connector</li> <li>SQL Spark Connector</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#access-dedicate-sql-pool","title":"Access Dedicate SQL Pool","text":"<p>When connect to Azure Synapse Dedicated SQL Pool, we will use special spark connector, <code>com.databricks.spark.sqldw</code> method. This method previously uses Poly-base to read and write data to and from <code>Azure Synapse</code> using a staging server (mainly, blob storage or a Data Lake storage directory), but now data are being read and write using Copy, as the Copy method has improved performance. Recommended for Azure Synapse</p> <p>Note</p> <p>This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.</p>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#sql-authentication","title":"SQL Authentication","text":""},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#1-connection-code","title":"1) Connection Code","text":"<pre><code>spark.conf.set(\"spark.databricks.sqldw.writeSemantics\", \"copy\")\n</code></pre> <pre><code>df = (\n    spark.read\n        .format(\"com.databricks.spark.sqldw\")\n        .option(\"url\", f\"jdbc:sqlserver://&lt;work-space-name&gt;;database=&lt;database-name&gt;;\")\n        .option(\"user\", \"&lt;username&gt;\")\n        .option(\"password\", \"&lt;password&gt;\")\n        .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n        .option(\"dbTable\", \"&lt;your-table-name&gt;\")\n        .option(\"tempDir\", \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/&lt;directory-name&gt;\")\n        .option(\"hostNameInCertificate\", \"*.sql.azuresynapse.net\")\n        .option(\"loginTimeout\", \"30\")\n        .option(\"encrypt\", \"true\")\n        .option(\"trustServerCertificate\", \"true\")\n        .load()\n)\n</code></pre> <p>Reference:</p> <ul> <li>https://bennyaustin.com/2020/02/05/pysparkupsert/</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#azure-service-principle","title":"Azure Service Principle","text":""},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#1-create-service-principal","title":"1) Create Service Principal","text":"<ul> <li>Go to <code>Azure Active Directory</code> <code>App registrations</code> <code>New registration</code></li> <li>Add the information of this app like <code>name</code> is <code>adb_to_synapse</code></li> <li>Click register for create</li> <li>Go to <code>App registrations</code> <code>Certificates&amp;secrets</code> <code>New Client Secret</code></li> <li>Save this value to <code>Azure Key Vaults</code></li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#2-create-user-in-azure-synapse","title":"2) Create User in Azure Synapse","text":"<ul> <li>Give it some permissions (On the Dedicated SQL pool, we can add a user and   assign it to the proper role),</li> </ul> <pre><code>CREATE USER [adb_to_synapse] FROM EXTERNAL PROVIDER;\nsp_addrolemember 'db_owner','adb_to_synapse';\nGO\n</code></pre> <p>Warning</p> <p>The permission of the user should be owner of database because it is currently required for Databricks to run <code>CREATE DATABASE SCOPED CREDENTIAL</code>.</p> <p>Note</p> <p>If you do not want to give owner permission to your Service Principle, you can grant <code>CONTROL</code>.</p> <pre><code>CREATE ROLE [databricks_reader];\nEXEC sp_addrolemember 'databricks_reader', 'adb_to_synapse';\nGRANT CONTROL TO [adb_to_synapse];\n</code></pre>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#3-azure-storage-temp-account","title":"3) Azure Storage Temp Account","text":"<ul> <li>Go to <code>Storage account</code> <code>Access Control (IAM)</code> <code>Add role assignment</code></li> <li>Select Role: <code>Storage Blob Data Contributor</code></li> <li>Select: <code>register application</code></li> <li>Click on save.</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#4-connection-code_1","title":"4) Connection Code","text":"<p>OAuth Configuration:</p> <pre><code>spark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type\",  \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id\", \"&lt;service-principal-id&gt;\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret\", \"&lt;service-principal-secret&gt;\")\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", \"https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token\")\n\nspark.conf.set(\"spark.databricks.sqldw.jdbc.service.principal.client.id\", \"&lt;service-principal-id&gt;\")\nspark.conf.set(\"spark.databricks.sqldw.jdbc.service.principal.client.secret\", \"&lt;service-principal-secret&gt;\")\n</code></pre> <p>JDBC URL Pattern:</p> <pre><code>URL: str = (\n    \"jdbc:sqlserver://&lt;work-space-name&gt;.sql.azuresynapse.net:1433;\"\n    \"database=&lt;database-name&gt;;\"\n    \"encrypt=true;trustServerCertificate=true;\"\n    \"hostNameInCertificate=*.sql.azuresynapse.net;\"\n    \"loginTimeout=30\"\n)\n</code></pre> <pre><code>df = (\n  spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://&lt;work-space-name&gt;.sql.azuresynapse.net:1433;\")\n    .option(\"tempDir\", \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/&lt;directory-name&gt;\")\n    .option(\"enableServicePrincipalAuth\", \"true\")\n    .option(\"dbTable\", \"[&lt;schema&gt;].[&lt;table-name&gt;]\")\n    .load()\n)\n</code></pre> <p>References:</p> <ul> <li>https://pl.seequality.net/load-synapse-analytics-sql-pool-with-azure-databricks/</li> <li>https://learn.microsoft.com/en-us/answers/questions/327270/azure-databricks-to-azure-synapse-service-principa?orderby=newest</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#send-ddl-or-dml-to-azure-synapse-sql-pool","title":"Send DDL or DML to Azure Synapse SQL Pool","text":"<p>When execute DDL or DML statement to Azure Synapse SQL Pool, that has 2 solutions: JDBC, and ODBC drivers.</p>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#jdbc-driver","title":"JDBC Driver","text":""},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#1-create-jdbc-connection","title":"1) Create JDBC Connection","text":"<pre><code>URL = f\"jdbc:sqlserver://{server}:1433;database={database};\"\n\nprops = spark._sc._gateway.jvm.java.util.Properties()\nprops.putAll({\n    'username': username,\n    'password': password,\n    'Driver': \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n})\n\nConnection = spark._sc._gateway.jvm.java.sql.Connection\n\ndriver_manager = spark._sc._gateway.jvm.java.sql.DriverManager\nconnection: Connection = driver_manager.getConnection(URL, props)\n</code></pre>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#2-connection-code","title":"2) Connection Code","text":"Statement ExecutionBatch ExecutionCall Store Procedure <pre><code>ResultSet = spark._sc._gateway.jvm.java.sql.ResultSet\nResultSetMetaData = spark._sc._gateway.jvm.java.sql.ResultSetMetaData\nConnection = spark._sc._gateway.jvm.java.sql.Connection\nStatement = spark._sc._gateway.jvm.java.sql.Statement\n\nstmt: Statement = connection.createStatement()  # Statement\n</code></pre> <pre><code>query: str = f\"\"\"\n  SELECT * FROM [&lt;schema&gt;].[&lt;table-name&gt;]\n\"\"\"\n\nrs: ResultSet = stmt.executeQuery(query)  # ResultSet\nmetadata: ResultSetMetaData = rs.getMetaData()  # ResultSetMetaData\ncol_numbers = metadata.getColumnCount()\n\ncol_names: list = []\nfor i in range(1, col_numbers + 1):\n    if column:\n      col_names.append(metadata.getColumnName(i))\n    else:\n      col_names.append(f\"col_{i}\")\n\nresults: list = []\nwhile rs.next():\n  result: dict = {}\n  for i in range(col_numbers):\n    name: str = col_names[i]\n    result[name] = rs.getString(name)\n  results.append(result)\n</code></pre> <pre><code>stmt.close()\nconnection.close()\n</code></pre> <pre><code>PreparedStatement = spark._sc._gateway.jvm.java.sql.PreparedStatement\n\npreps: PreparedStatement = connection.prepareStatement(\n  \"INSERT INTO [dev].[people]\"\n  \"VALUES (?, ?, ?);\"\n)\nrows = [\n  [\"Gandhi\", \"politics\", 12],\n  [\"Turing\", \"computers\", 31],\n]\nfor row in rows:\n  for idx, data in enumerate(row, start=1):\n    if isinstance(data, int):\n      preps.setInt(idx, data)\n    else:\n      preps.setString(idx, data)\n  preps.addBatch()\n\nconnection.setAutoCommit(False)\nresult_number: int = preps.executeBatch()\npreps.clearBatch()\nconnection.setAutoCommit(True)\n</code></pre> <p>Note: Add parameter <code>rewriteBatchedStatements=true</code> to JDBC URL for improve execute performance from before add this parameter, <pre><code>INSERT INTO jdbc (`name`) VALUES ('Line 1: Lorem ipsum ...')\nINSERT INTO jdbc (`name`) VALUES ('Line 2: Lorem ipsum ...')\n</code></pre> Then, after add this parameter to JDBC URL, <pre><code>INSERT INTO jdbc (`name`) VALUES ('Line 1: Lorem ipsum ...'), ('Line 2: Lorem ipsum ...')\n</code></pre></p> <pre><code>exec_statement = connection.prepareCall(\n    f\"\"\"{{CALL {schema}.usp_stored_procedure(\n      {master_id}, {parent_id}, {child_id}, '{table}', ?,\n      ?, ?, ?, ?\n    )}}\"\"\"\n)\nexec_statement.setString(5, 'data')\n\nexec_statement.registerOutParameter(1, spark._sc._gateway.jvm.java.sql.Types.INTEGER)\nexec_statement.registerOutParameter(2, spark._sc._gateway.jvm.java.sql.Types.VARCHAR)\nexec_statement.registerOutParameter(3, spark._sc._gateway.jvm.java.sql.Types.VARCHAR)\nexec_statement.registerOutParameter(4, spark._sc._gateway.jvm.java.sql.Types.VARCHAR)\n\nexec_statement.executeUpdate()\n\nres1 = exec_statement.getInt(1)\nres2 = exec_statement.getString(2)\nres3 = exec_statement.getString(3)\nres4 = exec_statement.getString(4)\n\nexec_statement.close()\nconnection.close()\n</code></pre> <p>Reference:</p> <ul> <li>How to Call MSSQL Stored Procedure</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#odbc-driver","title":"ODBC Driver","text":""},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#1-create-odbc-connection","title":"1) Create ODBC Connection","text":"<pre><code>%sh\ncurl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nsudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17\n</code></pre> <pre><code>import pyodbc\nserver = '&lt;server-name&gt;'\ndatabase = '&lt;database-name&gt;'\nusername = '&lt;username&gt;'\npassword = '&lt;password&gt;'\n\nconn = pyodbc.connect(\n  f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n  f'SERVER={server};DATABASE={database};UID={username};PWD={password}'\n)\n</code></pre> <p>Reference:</p> <ul> <li>Using PyODBC in Azure Databricks for Connecting with MSSQL</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse-sql-pool/#references","title":"References","text":"<ul> <li>(https://docs.databricks.com/data/data-sources/azure/synapse-analytics.html)</li> <li>(https://joeho.xyz/blog-posts/how-to-connect-to-azure-synapse-in-azure-databricks/)</li> <li>(https://learn.microsoft.com/en-us/answers/questions/653154/databricks-packages-for-batch-loading-to-azure.html)</li> <li> (https://stackoverflow.com/questions/55708079/spark-optimise-writing-a-dataframe-to-sql-server/55717234) (***)</li> <li>(https://docs.databricks.com/external-data/synapse-analytics.html)</li> <li>(https://learn.microsoft.com/en-us/azure/synapse-analytics/security/how-to-set-up-access-control)</li> </ul>"},{"location":"services/azure/databricks/adb-uc-privileges/","title":"Privileges","text":"<p>Initially, users have no access to data in a metastore. Azure Databricks account admins, workspace admins, and metastore admins have default privileges for managing Unity Catalog.</p> <p>All securable objects in Unity Catalog have an owner. Object owners have all privileges on that object, including the ability to grant privileges to other principals.</p> <p>Note</p> <p>Privileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Account admins can also grant privileges directly on a metastore.</p>"},{"location":"services/azure/databricks/adb-uc-privileges/#manage-privileges","title":"Manage Privileges","text":""},{"location":"services/azure/databricks/adb-uc-privileges/#references","title":"References","text":"<ul> <li>Manage privileges in Unity Catalog</li> <li>Unity Catalog privileges and securable objects</li> </ul>"},{"location":"services/azure/databricks/adb-uc-setup/","title":"Setup Unity Catalog","text":"<p>Unity Catalog is a data governance solution for Databricks, designed to provide a unified approach across all of your Databricks workspaces.</p> <p>Important</p> <p>Azure Databricks workspace should be Premium pricing tier.</p>"},{"location":"services/azure/databricks/adb-uc-setup/#prerequisite","title":"Prerequisite","text":"<p>To enable the Databricks Account Console and establish your first Account Admin, you will need to engage someone who has the Microsoft Entra ID (Azure Active Directory) Global Administrator role</p> <p>For security purposes, only someone with the Global Administrator role has permission to assign the first account admin role. After completing these steps, you can remove the Global Administrator role from the Azure Databricks account.</p> <ol> <li>Create Resource Group</li> <li>Create Premium Tier Azure Databricks Workspace</li> <li>Create Azure DataLake Gen2 Storage Account and Container</li> <li>Create Access Connector for Azure Databricks</li> <li>Grant Storage Blob Data Contributor to Access Connector for Azure Databricks on Azure DataLake Gen2 Storage Account</li> <li>Enable Unity Catalog by creating Metastore and assigning to Workspace</li> </ol> <p>Note</p> <p>If you do not create new Access Connector and use default provisioning, it will not use any managed identity on this Access Connector. The default Accesss Connector name is <code>unity-catalog-access-connector</code></p>"},{"location":"services/azure/databricks/adb-uc-setup/#getting-started","title":"Getting Started","text":"<ul> <li>Go to Azure Databricks Workspace  Click on Manage Account  Login into Account console</li> <li>Click on the Data tab  Create Metastores tab</li> <li>Provide information to create metastore (1 metastore per Region):<ul> <li>Metastore Name and Region (The best practice is choose same region and resource group)</li> <li>Azure DataLake Storage Gen2 (Example: <code>https://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/&lt;path&gt;</code>)</li> <li>Access Connector ID (Resource ID of your Access Connector)</li> </ul> </li> <li>Assign the workspace map to this metastore  Click on Enable Unity Catalog</li> </ul>"},{"location":"services/azure/databricks/adb-uc-setup/#external-catalog","title":"External Catalog","text":"<p>Create storage credentials:</p> <ul> <li>On your Azure Databricks Workspace  Go to Data Explorer External Data  Select Storage Credentials</li> <li>Click Add and then select Add a storage credential  Select Service Principal</li> <li>Enter the Storage credential name of your choice  Provide   your service principle information  Click Create</li> </ul> <p>Create external location:</p> <ul> <li>In the Data Explorer  Select External Locations  Click Add an external location</li> <li>Enter the External location name and Azure DataLake Storage Gen2 URL    Select the Storage credential you created  Click Create</li> </ul>"},{"location":"services/azure/databricks/adb-uc-setup/#read-mores","title":"Read Mores","text":"<ul> <li> Enabling Unity Catalog on Azure Databricks: A Step-by-Step Guide</li> <li> Set up and manage Unity Catalog</li> <li> Azure Databricks administration introduction</li> <li> Create a Unity Catalog metastore</li> <li> Use Azure managed identities in Unity Catalog to access storage</li> </ul>"},{"location":"services/azure/devops/devops-for-loop/","title":"DevOps: For Loop","text":""},{"location":"services/azure/devops/devops-multi-repo/","title":"DevOps: Multi-Repositories","text":"<pre><code>pool:\n  vmImage: ubuntu-latest\n\ntrigger:\n  - none\n\nparameters:\n  - name: repo_branch\n    type: string\n    default: \"main\"\n\nresources:\n  repositories:\n  - repository: self\n    ref: $(branch)\n  - repository: repo\n    type: git\n    name: {project-name}/{repo-name}\n    ref: ${{ parameters.repo_branch }}\n\nsteps:\n  - checkout: self\n  - checkout: repo\n\n  - script: ls -al $(Build.SourcesDirectory)\n    displayName: 'List on source dir'\n\n  - task: CopyFiles@2\n    inputs:\n      SourceFolder: '$(Build.SourcesDirectory)'\n      Contents: '**'\n      TargetFolder: '$(Build.ArtifactStagingDirectory)'\n\n  - task: DeleteFiles@1\n    inputs:\n      SourceFolder: '$(Build.ArtifactStagingDirectory)'\n      Contents: |\n        **/.git\n\n  - task: PublishBuildArtifacts@1\n    inputs:\n      PathtoPublish: '$(Build.ArtifactStagingDirectory)'\n      ArtifactName: 'drop'\n      publishLocation: 'Container'\n    displayName: 'Publish Artifact: drop'\n</code></pre>"},{"location":"services/azure/devops/devops-self-hosted-win-fastapi/","title":"Self Hosted with FastAPI","text":"<p>When we want to create a Rest API application like Azure Function App on an on-premises server, it is simply to use the FastAPI package, which is a lightweight Python ASGI web application, for this our purpose.</p>"},{"location":"services/azure/devops/devops-self-hosted-win-fastapi/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/devops/devops-self-hosted-win-fastapi/#setup-application","title":"Setup Application","text":"<p>Let\u2019s start setting up your FastAPI application and a <code>.bat</code> script for run this application with dynamic input arguments.</p> app/app.py<pre><code>from fastapi import FastAPI\n\ndef create_app() -&gt; FastAPI:\n    \"\"\"Application Factory\"\"\"\n    app = FastAPI()\n\n    @app.get(\"/health/\")\n    async def health():\n        return {\"message\": \"Hello World\"}\n\n    return app\n</code></pre> main.py<pre><code>import uvicorn\nfrom app.app import create_app\n\napp = create_app()\n\nif __name__ == '__main__':\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre> scripts/runserver.bat<pre><code>@echo off\ngoto :init\n\n:usage\n    echo USAGE:\n    echo   %__bat_filename% [flags] \"release argument\"\n    echo.\n    echo.  -h, --help           shows this help\n    echo.  -p, --port value     specifies a port number value\n    echo.  --reload             enable auto-reload\n    goto :eof\n\n:missing_args\n    call :usage\n    echo.\n    echo ****\n    echo MISSING RELEASE ARGUMENT !!!\n    goto :eof\n\n:port\n    echo Port does set from argument and port changes from 8000 to %__port% ...\n    goto :eof\n\n:version\n    if \"%~1\"==\"full\" call :usage &amp; goto :eof\n    echo %__version%\n    goto :eof\n\n:init\n    set \"__name=%~n0\"\n    set \"__port=8000\"\n\n    set \"__bat_filepath=%~0\"\n    set \"__bat_path=%~dp0\"\n    set \"__bat_filename=%~nx0\"\n\n    set \"__version=0.1.0\"\n    set \"__reload=\"\n    set \"__release=\"\n\n:parse\n    if \"%~1\"==\"\"                goto :validate\n\n    if /i \"%~1\"==\"-h\"           call :usage \"%~2\" &amp; goto :end\n    if /i \"%~1\"==\"--help\"       call :usage \"%~2\" &amp; goto :end\n\n    if /i \"%~1\"==\"-v\"           call :version      &amp; goto :end\n    if /i \"%~1\"==\"--version\"    call :version full &amp; goto :end\n\n    if /i \"%~1\"==\"-p\"           set \"__port=%~2\" &amp; shift &amp; shift &amp; call :port &amp; goto :parse\n    if /i \"%~1\"==\"--port\"       set \"__port=%~2\" &amp; shift &amp; shift &amp; call :port &amp; goto :parse\n\n    if /i \"%~1\"==\"--reload\"     set \"__reload=--reload\" &amp; shift &amp; goto :parse\n\n    if not defined __release    set \"__release=%~1\" &amp; shift &amp; goto :parse\n\n    shift\n    goto :parse\n\n:validate\n    if not defined __release call :missing_args &amp; goto :end\n\n:main\n    echo INFO: Start running server with release \"%__release%\" ...\n    call .\\venv\\Scripts\\activate\n    call uvicorn main:app --port %__port% %__reload%\n\n:end\n    echo.\n    echo End and Clean Up\n    call :cleanup\n    exit /B\n\n:cleanup\n    REM The cleanup function is only really necessary if you\n    REM are _not_ using SETLOCAL.\n\n    set \"__name=\"\n    set \"__port=\"\n\n    set \"__bat_filepath=\"\n    set \"__bat_path=\"\n    set \"__bat_filename=\"\n\n    set \"__release=\"\n    set \"__version=\"\n    set \"__reload=\"\n\n    goto :eof\n</code></pre> <p>Note</p> <p>I use the <code>.bat</code> script because the on-premise server that I want to run is Windows OS.</p>"},{"location":"services/azure/devops/devops-self-hosted-win-fastapi/#deploy-to-window-service","title":"Deploy to Window Service","text":"<p>I use the NSSM software for wrap the <code>runserver.bat</code> script file and monitor whether my app is able to run continuously on the Windows service. So, I will Download NSSM and unzip the installed file to the current path.</p> <p>Note</p> <p>We cannot use the Docker container in the target on-premises server because of the Windows Server version does not support, it be Windows Server 2016 which does not support Linux container on VM and WSL.</p> <p>First, we install my application on the Windows service, which can be seen in the Services software.</p> <pre><code>.\\nssm\\win64\\nssm.exe install \"FastAPIService\" \"%cd%\\runserver.bat\"\n</code></pre> <p>Next, we can setup additional the logging component for <code>stdout</code> and <code>stderr</code> in this application.</p> <pre><code>.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppStdout \"%cd%\\logs\\FastAPIService.log\"\n.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppStderr \"%cd%\\logs\\FastAPIService.log\"\n.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppRotateFiles 1\n.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppRotateOnline 1\n.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppRotateSeconds 86400\n.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppRotateBytes 1048576\n</code></pre> <p>Finally, we start the application service by <code>sc.exe</code> command.</p> <pre><code>sc.exe start \"FastAPIService\"\n</code></pre> <p>Warning</p> <p>I cannot use the python package, <code>pywin32</code>, because I get the error message;</p> <pre><code>Error 1053: The service did not respond to the start or control request in a timely fashion\n</code></pre> <p>when start this application service on locally.</p>"},{"location":"services/azure/devops/devops-self-hosted-win-fastapi/#setup-agent-to-on-premises-server","title":"Setup Agent to On-Premises Server","text":"<p>We will create a CI/CD deployment pipeline with Azure DevOps that able to deploy the application to target server. The purpose is running this application on the Windows service in an on-premises server.</p> <p>Firstly, the on-premises server does not connect to Azure DevOps because It was not listed in the Agent Pools by Self-Hosted agent connection type in my organization setting. So, we create a new agent pool name like <code>MYSTDVM01</code> and follow the Azure document to list this new agent in the Agents menu.</p> <p>Note</p> <p>More detail about the new Agent implementation, How to install Self-hosted Windows agent for Azure DevOps.</p> <p>If you want to let everyone in your group of Azure DevOps can see and use this agent, you should add owner permission to your group by Organization Setting &gt; Agent Pools &gt; Your Agent Pool &gt; Security.</p> <p>Next, We create a folder <code>$(Agent.HomeDirectory)/app</code> for keeping source code without the DevOps pipeline process before deploying my artifact to this agent.</p> <p>Finally, we set up a Python interpreter for running the Python application in this agent.</p> <ul> <li> <p>Download the required version of Python and install it on this agent</p> </li> <li> <p>Copy all the installed python files from <code>C:/Users/{user}/AppData/Local/Programs/Python/Python39</code>   to <code>$(Agent.ToolsDirectory)/Python/3.9.13/x64/*</code></p> </li> <li> <p>Create a complete file at <code>$(Agent.ToolsDirectory)/Python/3.9.13/x64.complete</code> for trigger Azure DevOps   pipeline can use this package in the job</p> </li> </ul> <p>Note</p> <p>If your server set a proxy firwall rule, you can run self-hosted agent config by <code>./config.cmd --proxyurl http://proxy.domain.co.th --proxyusername \"CEMENTH/{user}\" --proxypassword \"*******\"</code>, it will save your password to <code>.proxycredentials</code> file for reuse this password for proxy mode configuration.</p>"},{"location":"services/azure/devops/devops-self-hosted-win-fastapi/#deploy-to-on-premises-server","title":"Deploy to On-Premises Server","text":"<p>For the CI pipeline, I test code and package dependencies on the artifact server.</p> <pre><code>jobs:\n  - job: Phase_1\n    displayName: Build and Test\n    pool:\n      vmImage: windows-latest\n    variables:\n      python.version: \"3.9, 3.10\"\n    steps:\n      - checkout: self\n        clean: true\n        fetchDepth: 1\n      - task: UsePythonVersion@0\n        displayName: Use Python $(python.version)\n        inputs:\n          versionSpec: $(python.version)\n      - task: CmdLine@2\n        displayName: Install dependencies\n        inputs:\n          script: python -m pip install --upgrade pip &amp;&amp; pip install -r requirements.txt\n          workingDirectory: fastapi\n          failOnStderr: true\n      - task: CmdLine@2\n        displayName: pytest\n        inputs:\n          script: pip install pytest &amp;&amp; pytest tests --doctest-modules --junitxml=junit/test-results.xml\n          workingDirectory: fastapi\n          failOnStderr: true\n      - task: PublishTestResults@2\n        displayName: Publish Test Results **/test-results.xml\n        inputs:\n          testResultsFiles: \"**/test-results.xml\"\n          failTaskOnFailedTests: true\n          testRunTitle: Python $(python.version)\n  - job: Phase_2\n    displayName: Publish\n    dependsOn: Phase_1\n    pool:\n      vmImage: windows-latest\n    steps:\n      - checkout: self\n        clean: true\n        fetchDepth: 1\n      - task: UsePythonVersion@0\n        displayName: Use Python 3.9.13\n        inputs:\n          versionSpec: 3.9.13\n      - task: CmdLine@2\n        displayName: Create virtual environment\n        inputs:\n          script: python -m pip install --upgrade pip &amp;&amp; python -m venv venv\n          workingDirectory: fastapi\n      - task: CmdLine@2\n        displayName: Install dependencies\n        inputs:\n          script: .\\venv\\Scripts\\activate &amp;&amp; pip install -r requirements.txt --no-cache\n          workingDirectory: fastapi\n      - task: CmdLine@2\n        displayName: Pack dependency files to wheel format\n        inputs:\n          script: .\\venv\\Scripts\\activate &amp;&amp; pip wheel -w wheels -r .\\requirements.txt &amp;&amp; echo \"Create wheel files successful.\"\n          workingDirectory: fastapi\n      - task: PublishBuildArtifacts@1\n        displayName: \"Publish Artifact: drop\"\n        inputs:\n          PathtoPublish: fastapi\n</code></pre> <p>Note</p> <p>For the Python dependencies in the requirement.txt file, I use wheel to download these dependencies from PyPI and save them to <code>\\wheels</code> path by <code>pip wheel -w wheels -r .\\requirements.txt</code></p> <p>For the CD pipeline, I deploy the application to the Windows service and the test service.</p> <pre><code>jobs:\n- job: Phase_1\n  displayName: Deploy job\n  pool:\n    vmImage: Self-hosted agent\n  variables:\n    python.version: '3.9, 3.10'\n  variables:\n    application_name: 'FastAPIServiceDev'\n    application_port: '8001'\n    application_folder: 'dev/app_dmz'\n    health_route: 'health/'\n  steps:\n  - powershell: |\n      echo \"User that this job uses to run:\"\n      whoami\n      $service = Get-Service -Name \"$(application_name)\" -ErrorAction SilentlyContinue\n      if ($service -eq $null)\n      {\n          echo \"Service does not exists.\"\n      } else {\n          echo \"Service does exists ...\"\n          sc.exe stop \"$(application_name)\"\n          nssm.exe remove \"$(application_name)\" confirm\n          echo \"Success stop and remove service.\"\n      }\n      Start-Sleep -Seconds 20\n    displayName: 'Remove existing Windows Service'\n  - task: CopyFiles@2\n    displayName: 'Copy Files to App Folder'\n    inputs:\n      SourceFolder: '$(System.DefaultWorkingDirectory)/_DATA360-DMZ/drop'\n      Contents: |\n        **\n        !.git/**/*\n        !.gitignore\n        !.gitattributes\n        !venv/**\n      TargetFolder: '$(Agent.HomeDirectory)/app/$(application_folder)'\n      CleanTargetFolder: true\n  - task: UsePythonVersion@0\n    displayName: 'Use Python 3.9.13'\n    inputs:\n      versionSpec: 3.9.13\n      disableDownloadFromRegistry: true\n  steps:\n  - powershell: |\n      echo \"Start run setup ...\"\n      .\\scripts\\setup.bat\n      $service = Get-Service -Name \"$(application_name)\" -ErrorAction SilentlyContinue\n      if ($service -eq $null)\n      {\n          echo \"Service does not exists\"\n          nssm.exe install \"$(application_name)\" \"$(pwd)\\scripts\\runserver.bat\" \"--port $(application_port) develop\"\n          nssm.exe set \"$(application_name)\" AppDirectory \"$(pwd)\"\n          echo \"Set AppStdout to $(pwd)\\logs\\$(application_name).log\"\n          nssm.exe set \"$(application_name)\" AppStdout \"$(pwd)\\logs\\$(application_name).log\"\n          echo \"Set AppStderr to $(pwd)\\logs\\$(application_name).log\"\n          nssm.exe set \"$(application_name)\" AppStderr \"$(pwd)\\logs\\$(application_name).log\"\n          nssm.exe set \"$(application_name)\" AppRotateFiles 1\n          nssm.exe set \"$(application_name)\" AppRotateOnline 1\n          nssm.exe set \"$(application_name)\" AppRotateSeconds 86400\n          nssm.exe set \"$(application_name)\" AppRotateBytes 1048576\n      } else {\n          echo \"Service does exists.\"\n      }\n      nssm.exe status \"$(application_name)\"\n      sc.exe start \"$(application_name)\"\n    workingDirectory: '$(Agent.HomeDirectory)\\app\\$(application_folder)\\'\n    displayName: 'Setup and Install Windows service'\n- job: Phase_1\n  displayName: Deploy job\n  pool:\n    vmImage: Self-hosted agent\n  variables:\n    application_port: '8001'\n    health_route: 'health/'\n  steps:\n  - powershell: |\n      $r = curl.exe \"http://localhost:$(application_port)/$(health_route)\" --silent\n      if (($r -ne '{\"detail\":\"Not Found\"}') -or ($r)) {\n        echo \"SUCCESS: The application can run normally.\"\n        exit 0;\n      } else {\n        echo \"ERROR: Failed to request to health check endpoint.\"\n        exit 1;\n      }\n    displayName: 'RestAPI to Health Check'\n</code></pre> <p>Warning</p> <p>We should install NSSM on that on-premises server before running this CI/CD pipeline. If you get a permission issue of your agent job when executing the NSSM file, you can add the self-host agent user, <code>NetworkService</code>, to the Admin group in that server.</p>"},{"location":"services/azure/devops/devops-self-hosted-win-fastapi/#setup-environments","title":"Setup Environments","text":"<p>We will set up environments for the CD pipelines by the port of application such as <code>8001</code> for development and <code>8000</code> for production because we want to partition that server for two environments.</p>"},{"location":"services/azure/eventhubs/","title":"Azure EventHubs","text":""},{"location":"services/azure/eventhubs/#getting-started","title":"Getting Started","text":"Python: AsyncPython: Batches AsyncPython: Checkpoint <pre><code>import logging\nimport asyncio\nfrom azure.eventhub.aio import EventHubConsumerClient\n\nconnection_str = '&lt;&lt; CONNECTION STRING FOR THE EVENT HUBS NAMESPACE &gt;&gt;'\nconsumer_group = '&lt;&lt; CONSUMER GROUP &gt;&gt;'\neventhub_name = '&lt;&lt; NAME OF THE EVENT HUB &gt;&gt;'\n\nlogger = logging.getLogger(\"azure.eventhub\")\nlogging.basicConfig(level=logging.INFO)\n\nasync def on_event(partition_context, event):\n    logger.info(\"Received event from partition {}\".format(partition_context.partition_id))\n    await partition_context.update_checkpoint(event)\n\nasync def receive():\n    client = EventHubConsumerClient.from_connection_string(connection_str, consumer_group, eventhub_name=eventhub_name)\n    async with client:\n        await client.receive(\n            on_event=on_event,\n            starting_position=\"-1\",  # \"-1\" is from the beginning of the partition.\n        )\n        # receive events from specified partition:\n        # await client.receive(on_event=on_event, partition_id='0')\n\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(receive())\n</code></pre> <pre><code>import logging\nimport asyncio\nfrom azure.eventhub.aio import EventHubConsumerClient\n\nconnection_str = '&lt;&lt; CONNECTION STRING FOR THE EVENT HUBS NAMESPACE &gt;&gt;'\nconsumer_group = '&lt;&lt; CONSUMER GROUP &gt;&gt;'\neventhub_name = '&lt;&lt; NAME OF THE EVENT HUB &gt;&gt;'\n\nlogger = logging.getLogger(\"azure.eventhub\")\nlogging.basicConfig(level=logging.INFO)\n\nasync def on_event_batch(partition_context, events):\n    logger.info(\"Received event from partition {}\".format(partition_context.partition_id))\n    await partition_context.update_checkpoint()\n\nasync def receive_batch():\n    client = EventHubConsumerClient.from_connection_string(connection_str, consumer_group, eventhub_name=eventhub_name)\n    async with client:\n        await client.receive_batch(\n            on_event_batch=on_event_batch,\n            starting_position=\"-1\",  # \"-1\" is from the beginning of the partition.\n        )\n        # receive events from specified partition:\n        # await client.receive_batch(on_event_batch=on_event_batch, partition_id='0')\n\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(receive_batch())\n</code></pre> <pre><code>import asyncio\nfrom azure.eventhub.aio import EventHubConsumerClient\nfrom azure.eventhub.extensions.checkpointstoreblobaio import BlobCheckpointStore\n\nconnection_str = '&lt;&lt; CONNECTION STRING FOR THE EVENT HUBS NAMESPACE &gt;&gt;'\nconsumer_group = '&lt;&lt; CONSUMER GROUP &gt;&gt;'\neventhub_name = '&lt;&lt; NAME OF THE EVENT HUB &gt;&gt;'\nstorage_connection_str = '&lt;&lt; CONNECTION STRING FOR THE STORAGE &gt;&gt;'\ncontainer_name = '&lt;&lt;NAME OF THE BLOB CONTAINER&gt;&gt;'\n\nasync def on_event(partition_context, event):\n    # do something\n    await partition_context.update_checkpoint(event)  # Or update_checkpoint every N events for better performance.\n\nasync def receive(client):\n    await client.receive(\n        on_event=on_event,\n        starting_position=\"-1\",  # \"-1\" is from the beginning of the partition.\n    )\n\nasync def main():\n    checkpoint_store = BlobCheckpointStore.from_connection_string(storage_connection_str, container_name)\n    client = EventHubConsumerClient.from_connection_string(\n        connection_str,\n        consumer_group,\n        eventhub_name=eventhub_name,\n        checkpoint_store=checkpoint_store,  # For load balancing and checkpoint. Leave None for no load balancing\n    )\n    async with client:\n        await receive(client)\n\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())\n</code></pre>"},{"location":"services/azure/eventhubs/#producer","title":"Producer","text":"<pre><code>import asyncio\nimport aiohttp\nimport json\nimport logging\nfrom azure.eventhub import EventHubClient, EventData\n\nasync def get_weather(city, state, api_key):\n    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city},{state}&amp;appid={api_key}\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            return await resp.json()\n\nasync def send_to_event_hub(event_hub_client, weather_data):\n    event = EventData(json.dumps(weather_data).encode(\"utf-8\"))\n    await event_hub_client.send(event)\n\nasync def main(api_key, event_hub_client):\n    # List of cities in the USA\n    cities = [\n        {\"city\": \"New York\", \"state\": \"NY\"},\n        {\"city\": \"Los Angeles\", \"state\": \"CA\"},\n        {\"city\": \"Chicago\", \"state\": \"IL\"},\n        {\"city\": \"Houston\", \"state\": \"TX\"},\n        {\"city\": \"Phoenix\", \"state\": \"AZ\"},\n        {\"city\": \"Philadelphia\", \"state\": \"PA\"},\n        {\"city\": \"San Antonio\", \"state\": \"TX\"},\n        {\"city\": \"San Diego\", \"state\": \"CA\"},\n        {\"city\": \"Dallas\", \"state\": \"TX\"},\n        {\"city\": \"San Jose\", \"state\": \"CA\"}\n    ]\n\n    tasks = []\n    for city_data in cities:\n        task = asyncio.create_task(get_weather(city_data[\"city\"], city_data[\"state\"], api_key))\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks)\n    for weather_data in results:\n        await send_to_event_hub(event_hub_client, weather_data)\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\n\n# Event Hub Configuration\nevent_hub_namespace = \"&lt;Your Azure Event Hub Namespace&gt;\"\nevent_hub_name = \"eh_sample_01\"\nevent_hub_key = \"&lt;Your Azure Event Hub Key&gt;\"\nevent_hub_endpoint = f\"amqps://{event_hub_namespace}.servicebus.windows.net/{event_hub_name}\"\n\napi_key = \"&lt;Your OpenWeatherMap API Key&gt;\"\ninterval = 5 * 60 # 5 minutes in seconds\nevent_hub_client = EventHubClient.from_connection_string(event_hub_endpoint, event_hub_key)\n\nwhile True:\n    asyncio.run(main(api_key, event_hub_client))\n    await asyncio.sleep(interval)\n</code></pre>"},{"location":"services/azure/eventhubs/#connect-with-kafka","title":"Connect with Kafka","text":"<p>Read More on Connector Document</p> <pre><code># Source: https://github.com/Azure/azure-event-hubs-for-kafka/tree/master/tutorials/spark#running-spark\nEH_NAME_SPACE = \"eventhubs-name-space\"\nEH_NAME = \"eventhubs-name\"\nEH_SASL = (\n    f'org.apache.kafka.common.security.plain.PlainLoginModule required'\n    f'username=\"$ConnectionString\" '\n    f'password=\"Endpoint=sb://{EH_NAME_SPACE}.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=****\";'\n)\n(\n    df.write\n        .format(\"kafka\")\n        .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n        .option(\"kafka.security.protocol\", \"SASL_SSL\")\n        .option(\"kafka.sasl.jaas.config\", EH_SASL)\n        .option(\"kafka.batch.size\", 5000)\n        .option(\"kafka.bootstrap.servers\", f\"{EH_NAME_SPACE}.servicebus.windows.net:9093\")\n        .option(\"kafka.request.timeout.ms\", 120000)\n        .option(\"topic\", EH_NAME)\n        .option(\"checkpointLocation\", \"/mnt/telemetry/cp.txt\")\n        .save()\n)\n</code></pre>"},{"location":"services/azure/eventhubs/#read-mores","title":"Read Mores","text":"<ul> <li> Azure Event Hubs Features</li> <li> PyPI: <code>azure-eventhub</code></li> <li> Set startingPosition in Event Hub on Databricks</li> <li>How to format a Pyspark connection string for Azure Eventhub with Kafka</li> </ul>"},{"location":"services/azure/fabric/","title":"Azure Fabric","text":""},{"location":"services/azure/fabric/#read-mores","title":"Read Mores","text":"<ul> <li> Building a scalable data ingestion framework for Microsoft Fabric</li> </ul>"},{"location":"services/azure/functions/az-func-to-az/","title":"Connect to Azure Services","text":""},{"location":"services/azure/functions/az-func-to-az/#authentication","title":"Authentication","text":""},{"location":"services/azure/functions/az-func-to-az/#using-system-assigned-managed-identity","title":"Using System-Assigned Managed Identity","text":""},{"location":"services/azure/functions/az-func-to-az/#enable-system-assigned-managed-identity","title":"Enable System-Assigned Managed Identity","text":"<ul> <li>Go to Azure Function App  Select Identity  Click nav System Assigned</li> <li>On Status  Enable to On    Click Save</li> </ul>"},{"location":"services/azure/functions/az-func-to-az/#services","title":"Services","text":""},{"location":"services/azure/functions/az-func-to-az/#key-vault","title":"Key Vault","text":""},{"location":"services/azure/functions/az-func-to-az/#prerequisite","title":"Prerequisite","text":"<p>Add The Azure Function MSI User to the Azure Key Vault.</p> <ul> <li>Go to Azure Key Vault  Select Access policies  Click nav Create</li> <li>On Configure from a template  Select Secret Management  Click Next</li> <li>On Principle  Search the Azure Function name    Click Create</li> </ul> <p>Add Secret to Azure Function:</p> <ul> <li>Go to Azure Key Vault  Select Secrets  Click nav Generate/Import</li> <li>Create your Secrets  Copy the Secret Identifier URI</li> <li>Go to Azure Function App  Select Configuration  Click New application setting</li> <li>Pass the name to environment variable with this value:   <code>@Microsoft.KeyVault(SecretUri=&lt;secret-identifier-uri&gt;)</code></li> </ul>"},{"location":"services/azure/functions/az-func-to-az/#connection-code","title":"Connection Code","text":"<pre><code>from azure.identity import ManagedIdentityCredential\nfrom azure.keyvault.secrets import SecretClient\n\ncredentials = ManagedIdentityCredential()\nsecret_client = SecretClient(\n    vault_url=\"https://&lt;key-vault-name&gt;.vault.azure.net\",\n    credential=credentials\n)\nsecret = secret_client.get_secret(\"secret-name\")\n</code></pre> <p>References</p> <ul> <li>Accessing Azure Key Vault from Python</li> </ul>"},{"location":"services/azure/functions/az-func-to-az/#service-bus","title":"Service Bus","text":""},{"location":"services/azure/functions/az-func-to-az/#connection-code_1","title":"Connection Code","text":"<pre><code>import os\nimport asyncio\nfrom aiohttp import ClientSession\nfrom azure.servicebus.aio import ServiceBusClient\n\nconn_str = os.environ['SERVICE_BUS_CONNECTION_STR']\ntopic_name = os.environ['SERVICE_BUS_TOPIC_NAME']\nsubscription_name = os.environ['SERVICE_BUS_SUBSCRIPTION_NAME']\n\nasync def watch(\n    topic_name,\n    subscription_name,\n):\n    async with ServiceBusClient.from_connection_string(conn_str=conn_str) as service_bus_client:\n        subscription_receiver = service_bus_client.get_subscription_receiver(\n            topic_name=topic_name,\n            subscription_name=subscription_name,\n        )\n    async with subscription_receiver:\n         message = await subscription_receiver.receive_messages(max_wait_time=1)\n\n    if message.body is not None:\n        async with ClientSession() as session:\n            await session.post('ip:port/endpoint',\n                               headers={'Content-type': 'application/x-www-form-urlencoded'},\n                               data={'data': message.body.decode()})\n\nasync def do():\n    while True:\n        for topic in ['topic1', 'topic2', 'topic3']:\n            await watch(topic, 'watcher')\n\n\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(do())\n</code></pre> <p>References</p> <ul> <li>https://stackoverflow.com/questions/63149310/azure-servicebus-using-async-await-in-python-seems-not-to-work</li> <li>https://iqan.medium.com/how-to-use-managed-identity-in-azure-functions-for-service-bus-trigger-fc61fb828b90</li> </ul>"},{"location":"services/azure/functions/az-func-to-az/#synapse","title":"Synapse","text":""},{"location":"services/azure/functions/az-func-to-az/#prerequisite_1","title":"Prerequisite","text":"<p>Enable AAD integration for Azure Synapse workspace.</p> <ul> <li>Go to Azure Synapse Workspace  Select Azure Active Directory</li> <li>Click nav Set admin  Select your user    Click Save</li> </ul> <p>Add The Azure Function MSI User to the Azure Synapse SQL Pool.</p> <ul> <li>Connect to Azure Synapse SQL Pool on target database.</li> <li> <p>Create MSI user that use the Azure Function name</p> <pre><code>CREATE USER &lt;azure-function-name&gt; FROM EXTERNAL PROVIDER\nGO\n</code></pre> </li> </ul>"},{"location":"services/azure/functions/az-func-to-az/#connection-code_2","title":"Connection Code","text":"ActiveDirectoryMsiSQL User <pre><code>import logging\nimport pyodbc\n\nserver = 'tcp:&lt;server-name&gt;.database.windows.net'\ndatabase = '&lt;database-name&gt;'\ndriver = '{ODBC Driver 17 for SQL Server}'\n\nwith pyodbc.connect(\n    (\n        f\"Driver={driver};Server={server};PORT=1433;Database={database};\"\n        f\"Authentication=ActiveDirectoryMsi;\"\n    )\n) as conn:\n    logging.info(\"Successful connection to database\")\n    with conn.cursor() as cursor:\n        cursor.execute(\"SELECT &lt;column-name&gt; FROM &lt;table-name&gt;;\")\n        row = cursor.fetchone()\n        while row:\n            logging.info(str(row[0]).strip())\n            row = cursor.fetchone()\n</code></pre> <pre><code>import logging\nimport pyodbc\n\nserver = 'tcp:&lt;server-name&gt;.database.windows.net'\ndatabase = '&lt;database-name&gt;'\ndriver = '{ODBC Driver 17 for SQL Server}'\n\nusername = \"&lt;username&gt;\"\npassword = \"&lt;password&gt;\"\n\nwith pyodbc.connect(\n    (\n        f\"Driver={driver};Server={server};PORT=1433;Database={database};\"\n        f\"UID={username};PWD={password}\"\n    )\n) as conn:\n    logging.info(\"Successful connection to database\")\n    with conn.cursor() as cursor:\n        cursor.execute(\"SELECT &lt;column-name&gt; FROM &lt;table-name&gt;;\")\n        row = cursor.fetchone()\n        while row:\n            logging.info(str(row[0]).strip())\n            row = cursor.fetchone()\n</code></pre> <p>Note</p> <p>If the Python runtime has version more than <code>3.11</code>, it will upgrade ODBC driver to version 18.</p>"},{"location":"services/azure/functions/az-func-v2/","title":"Python V2","text":"<p>https://medium.com/@saurabh.dasgupta1/developing-and-deploying-a-python-azure-function-step-by-step-83c5066a2531 https://towardsdatascience.com/tutorial-of-python-azure-functions-81949b1fd6db</p>"},{"location":"services/azure/functions/az-func-v2/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/functions/az-func-v2/#1-create-function-app","title":"1) Create Function App","text":"<ul> <li>On <code>Azure Portal</code>  Go to <code>Function App</code>    Create your function app that support Python run time with function version 2.0 -</li> </ul>"},{"location":"services/azure/functions/az-func-v2/#using-visual-studio-code","title":"Using Visual Studio Code","text":""},{"location":"services/azure/functions/az-func-v2/#prerequisite","title":"Prerequisite","text":"<ul> <li>Go to <code>VSC</code>  On Nav <code>Extensions</code>    Install <code>Azure Functions</code> and <code>Azurite</code></li> </ul>"},{"location":"services/azure/functions/az-func-v2/#create-first-app","title":"Create First App","text":"<ul> <li>On Nav <code>Azure</code>  On <code>WORKSPACE (Local)</code> drop down    Click Azure Functions </li> </ul>"},{"location":"services/azure/functions/az-func-v2/#references","title":"References","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python?pivots=python-mode-decorators</li> </ul>"},{"location":"services/azure/functions/az-func-with-docker/","title":"Dockerize","text":"<ul> <li>https://azureossd.github.io/2023/03/20/how-to-deploy-azure-functions-as-custom-container-using-azure-devops/</li> </ul>"},{"location":"services/azure/synapse/asa-auth/","title":"Auth","text":""},{"location":"services/azure/synapse/asa-auth/#users-roles","title":"Users &amp; Roles","text":""},{"location":"services/azure/synapse/asa-auth/#getting-users","title":"Getting Users","text":"All AD User AD Group SQL User SQL User without Login <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\n</code></pre> <pre><code>SELECT\n    [name]\nFROM [sys].[database_principals]\nWHERE\n    [type] = 'E'\n    AND [name] = 'username@mail.com'\n;\n</code></pre> <pre><code>SELECT\n    [name]\nFROM [sys].[database_principals]\nWHERE\n    [type] = 'X'\n    AND [name] = 'username@mail.com'\n;\n</code></pre> <pre><code>SELECT\n    [name]\nFROM [sys].[database_principals]\nWHERE\n    [type] = 'S'\n    AND [name] = 'username@mail.com'\n;\n</code></pre> <pre><code>SELECT\n    [name]\nFROM [sys].[database_principals]\nWHERE\n    [type] = 'S'\n    AND [name] = 'username@mail.com'\n;\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#create-user","title":"Create User","text":"AD User AD Group SQL User SQL User without Login <pre><code>USE [&lt;database-name&gt;];\nCREATE USER [username@mail.com] FROM EXTERNAL PROVIDER\nGO\n</code></pre> <pre><code>USE [&lt;database-name&gt;];\nCREATE USER [group-name] FROM LOGIN [group-name];\nGO\n</code></pre> <pre><code>USE [master];\nCREATE LOGIN &lt;username&gt; WITH PASSWORD = 'P@ssW0rd'\nGO\n</code></pre> <pre><code>USE [&lt;database-name&gt;];\nCREATE USER &lt;username&gt; FOR LOGIN &lt;username&gt;;\nGO\n</code></pre> <pre><code>USE [&lt;database-name&gt;];\nCREATE USER &lt;username&gt; WITHOUT LOGIN;\nGRANT IMPERSONATE ON USER::&lt;username&gt; TO [&lt;another-username&gt;];\nGO\n</code></pre> <pre><code>EXECUTE AS USER = '&lt;username&gt;';\nGO\n...\nREVERT;\nGO\n</code></pre> <p>Warning</p> <p>Azure Synapse Serverless SQL Pool does not support for create this user type.</p> <p>Getting User Example:</p> <p>Note</p> <p>If you want to drop user, you would use:</p> <pre><code>DROP USER [username@mail.com]\nGO\n</code></pre> <p>If this user have login, you would use:</p> <pre><code>DROP LOGIN [username@mail.com]\nGO\n</code></pre> <p>Generate drop statement with multi-users:</p> <pre><code>SELECT CONCAT('DROP USER [', [name], '];')  AS remove_user\nFROM\n    [sys].[database_principals]\nWHERE\n    [type] = 'E'\n    AND LOWER([name]) IN ('username@mail.com', ...)\n;\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#getting-relations","title":"Getting Relations","text":"List relate roles and users List all roles <pre><code>SELECT\n    r.[name]                                    AS [Role]\n    , m.[name]                                  AS [Member]\n    , m.Create_date                             AS [Created Date]\n    , m.modify_Date                             AS [Modified Date]\nFROM\n    [sys].[database_role_members]               AS rm\nJOIN [sys].[database_principals]                AS r\n    ON rm.[role_principal_id] = r.[principal_id]\nJOIN [sys].[database_principals]                AS m\n    ON rm.[member_principal_id] = m.[principal_id]\nWHERE\n    r.[type_desc] = 'DATABASE_ROLE'\nORDER BY\n    r.[name], m.[name];\n;\n</code></pre> <pre><code>SELECT\n    r.[name]                                    AS [Role]\n    , ISNULL(m.[name], 'No members')            AS [Member]\n    , m.create_date                             AS [Created Date]\n    , m.modify_Date                             AS [Modified Date]\nFROM\n    [sys].[database_role_members]               AS rm\nRIGHT OUTER JOIN [sys].[database_principals]    AS r\n    ON rm.[role_principal_id] = r.[principal_id]\nLEFT OUTER JOIN [sys].[database_principals]     AS m\n    ON rm.[member_principal_id] = m.[principal_id]\nWHERE\n    r.[type] = 'R'\nORDER BY\n    r.[name], ISNULL(m.[name], 'No members')\n;\n</code></pre> <p>Example Results:</p>  List relate roles and users List all roles <pre><code>Role         |Member          |Created Date           |Modified Date          |\n-------------+----------------+-----------------------+-----------------------+\nDATA ENGINEER|demo@mail.com   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_owner     |dbo             |2003-04-08 00:00:00.000|2021-09-21 00:00:00.000|\ndb_owner     |admin@mail.com  |2021-05-12 00:00:00.000|2021-05-12 00:00:00.000|\ndb_ddladmin  |DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datareader|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datawriter|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\n</code></pre> <pre><code>Role         |Member          |Created Date           |Modified Date          |\n-------------+----------------+-----------------------+-----------------------+\nDATA ENGINEER|No members      |NULL                   |NULL                   |\ndb_owner     |dbo             |2003-04-08 00:00:00.000|2021-09-21 00:00:00.000|\ndb_owner     |admin@mail.com  |2021-05-12 00:00:00.000|2021-05-12 00:00:00.000|\ndb_ddladmin  |DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datareader|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datawriter|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#roles","title":"Roles","text":"Dedicate SQL Pool Serverless SQL Pool <pre><code>CREATE ROLE [rolename]\nEXEC sp_addrolemember 'rolename', [username@mail.com]\nGO\n</code></pre> <pre><code>CREATE ROLE [rolename]\nALTER ROLE [rolename] ADD MEMBER [username@mail.com]\nGO\n</code></pre> <p>Note</p> <p>If you want to remove user from this role,</p>  Dedicate SQL Pool Serverless SQL Pool <pre><code>EXEC sp_droprolemember '&lt;role-name&gt;', 'username@mail.com';\n</code></pre> <pre><code>ALTER ROLE [role-name] DROP MEMBER [username@mail.com];\n</code></pre> <p>Generate systax,</p>  Dedicate SQL Pool <pre><code>SELECT\n  CONCAT(\n        'EXEC sp_droprolemember ''', r.name, ''', ''', m.name, ''';'\n  ) AS remove_member_command\nFROM\n    sys.database_role_members  AS rm\nRIGHT OUTER JOIN sys.database_principals AS r\n    ON rm.role_principal_id = r.principal_id\nLEFT OUTER JOIN sys.database_principals AS m\n    ON rm.member_principal_id = m.principal_id\nWHERE\n    r.type = 'R'\n    AND LOWER(m.name) IN (\n        'userbane@mail.com',\n        ...\n    )\nORDER BY\n    r.name, ISNULL(m.name, 'No members')\n;\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#permissions","title":"Permissions","text":""},{"location":"services/azure/synapse/asa-auth/#getting-relations_1","title":"Getting Relations","text":"List relate permissions List relate permissions with grantor <pre><code>SELECT DISTINCT\n     pr.principal_id                AS [ID],\n     pr.[name]                      AS [User],\n     pr.[type_desc]                 AS [Type],\n     pr.authentication_type_desc    AS [Auth_Type]\n     pe.state_desc                  AS [State]\n     pe.permission_name             AS [Permission]\n     pe.class_desc                  AS [Class]\n     coalesce(o.[name], sch.name)   AS [Object]\nFROM\n    sys.database_principals         AS pr\nJOIN sys.database_permissions       AS pe\n    ON pe.grantee_principal_id = pr.principal_id\nLEFT JOIN sys.objects               AS o\n    ON o.object_id = pe.major_id\nLEFT JOIN sys.schemas               AS sch\n    ON sch.schema_id = pe.major_id\n    AND class_desc = 'SCHEMA'\n;\n</code></pre> <pre><code>SELECT DISTINCT\n    DB_NAME()                           AS [DB],\n    p.[name]                            AS [User],\n    p.[type_desc]                       AS [Type],\n    p2.[name]                           AS [Grantor],\n    pe.[state_desc]                     AS [State],\n    pe.[permission_name]                AS [Permission],\n    o.[Name]                            AS [Object],\n    o.[type_desc]                       AS [Object Type]\nFROM [sys].[database_permissions]       AS pe\nLEFT JOIN [sys].[objects]               AS o\n    ON pe.[major_id] = o.[object_id]\nLEFT JOIN [sys].[database_principals]   AS p\n    ON pe.[grantee_principal_id] = p.[principal_id]\nLEFT JOIN [sys].[database_principals]   AS p2\n    ON pe.[grantor_principal_id] = p2.[principal_id]\n;\n</code></pre> <p>Example Results:</p>  List relate permissions List relate permissions with grantor <pre><code>ID|User              |Type          |Auth Type|State|Permission             |Class             |Object         |\n--+------------------+--------------+---------+-----+-----------------------+------------------+---------------+\n25|username@mail.com |EXTERNAL_USER |EXTERNAL |GRANT|CONNECT                |DATABASE          |               |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|ALTER                  |SCHEMA            |DATAEXTERNAL   |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|ALTER                  |SCHEMA            |MART           |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|ALTER                  |SCHEMA            |CURATED        |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|ALTER ANY DATA SOURCE  |DATABASE          |               |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|ALTER ANY FILE FORMAT  |DATABASE          |               |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|EXECUTE                |OBJECT_OR_COLUMN  |FUNC_CHK_ID    |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|EXECUTE                |OBJECT_OR_COLUMN  |FUNC_CHK_TITLE |\n</code></pre> <pre><code>DB      |User       |Type          |Grantor               |State|Permission                    |Object          |Object Type         |\n--------+-----------+--------------+----------------------+-----+------------------------------+----------------+--------------------+\nsyndpdev|adfuser    |SQL_USER      |dbo                   |GRANT|ALTER                         |                |                    |\nsyndpdev|adfuser    |SQL_USER      |dbo                   |GRANT|ALTER ANY EXTERNAL DATA SOURCE|                |                    |\nsyndpdev|adfuser    |SQL_USER      |dbo                   |GRANT|ALTER ANY EXTERNAL FILE FORMAT|                |                    |\nsyndpdev|adfuser    |SQL_USER      |dbo                   |GRANT|CONNECT                       |                |                    |\nsyndpdev|de_vendor  |DATABASE_ROLE |dbo                   |GRANT|ALTER ANY EXTERNAL FILE FORMAT|                |                    |\nsyndpdev|de_vendor  |DATABASE_ROLE |dbo                   |GRANT|EXECUTE                       |                |                    |\nsyndpdev|de_vendor  |DATABASE_ROLE |user@email.com        |GRANT|EXECUTE                       |FUNC_CHK_ID     |SQL_SCALAR_FUNCTION |\nsyndpdev|de_vendor  |DATABASE_ROLE |user@email.com        |GRANT|EXECUTE                       |FUNC_CHK_TITLE  |SQL_SCALAR_FUNCTION |\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#grant","title":"Grant","text":"data execution <pre><code>GRANT CREATE VIEW TO [role-name];\nGRANT CREATE PROCEDURE TO [role-name];\nGRANT ALTER TO [role-name];\n</code></pre> <pre><code>GRANT EXECUTE ON SCHEMA::&lt;schema-name&gt; TO [role-name];\nGRANT UPDATE ON SCHEMA::&lt;schema-name&gt; TO [role-name];\nGRANT INSERT ON SCHEMA::&lt;schema-name&gt; TO [role-name];\nGRANT DELETE ON SCHEMA::&lt;schema-name&gt; TO [role-name];\nGRANT ALTER ON SCHEMA::&lt;schema-name&gt; TO [role-name];\n</code></pre> <pre><code>GRANT IMPERSONATE ON USER::&lt;user-name&gt; TO &lt;user-name&gt;;\n</code></pre> <p>Warning</p> <p>Impersonate can not use on the Synapse Serverless SQL Pool.</p> <p>More Example for Grant Permissions</p>"},{"location":"services/azure/synapse/asa-auth/#workload","title":"Workload","text":"<pre><code>CREATE WORKLOAD GROUP &lt;group-name&gt;\nWITH (\n    MIN_PERCENTAGE_RESOURCE = 100,\n    CAP_PERCENTAGE_RESOURCE = 100,\n    REQUEST_MIN_RESOURCE_GRANT_PERCENT = 100\n);\n\n--classifies load_user with the workload group LoadData\nCREATE WORKLOAD CLASSIFIER [&lt;classifier-name&gt;]\nWITH (\n    WORKLOAD_GROUP = '&lt;group-name&gt;',\n    MEMBERNAME = '&lt;username&gt;'\n);\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft Azure Synapse Analytics SQL Authentication</li> <li> Microsoft SQL T-SQL Create User Transaction</li> </ul>"},{"location":"services/azure/synapse/asa-date-timezone/","title":"Date &amp; Timezone","text":""},{"location":"services/azure/synapse/asa-date-timezone/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/synapse/asa-date-timezone/#convert-timezone","title":"Convert Timezone","text":"<pre><code>SELECT\n    CAST(\n        CAST(\n            [DateColumn] AS DATETIMEOFFSET\n        ) AT TIME ZONE 'SE Asia Standard Time' AS DATETIME2\n    ),\n    ...\nFROM ...\n</code></pre>"},{"location":"services/azure/synapse/asa-date-timezone/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft: Data Types - Datetimeoffset</li> </ul>"},{"location":"services/azure/synapse/asa-external-data-source/","title":"External Data Source","text":""},{"location":"services/azure/synapse/asa-external-data-source/#database-scope-credential","title":"Database Scope Credential","text":"<p>A Database Credential is not mapped to a server login or database user. The credential is used by the database to access to the external location anytime the database is performing an operation that requires access.</p>"},{"location":"services/azure/synapse/asa-external-data-source/#list-credentials","title":"List Credentials","text":"<pre><code>SELECT * FROM [sys].[database_scoped_credentials];\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#create-master-key","title":"Create Master Key","text":"<p>Create a new Master Key, <code>ENCRYPTION</code> to encrypt the credentials for the external data source.</p> <pre><code>-- Optional: Create MASTER KEY if not exists in database:\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'P@ssW0rd'\nGO\n</code></pre> <p>If the master key already exists on the database, you can use:</p> <pre><code>OPEN MASTER KEY DECRYPTION BY PASSWORD = 'P@ssW0rd';\n...\nCLOSE MASTER KEY;\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#create-credential","title":"Create Credential","text":"Managed IdentityService PrincipleShared Access Signature <pre><code>CREATE DATABASE SCOPED CREDENTIAL &lt;credential-name&gt;\nWITH IDENTITY = 'Managed Identity'\nGO\n\nCREATE EXTERNAL DATA SOURCE &lt;external-data-source&gt;\nWITH (\n    LOCATION   = 'https://&lt;storage_account&gt;.dfs.core.windows.net/&lt;container&gt;/&lt;path&gt;',\n    CREDENTIAL = &lt;credential-name&gt;\n)\n</code></pre> <pre><code>-- authority-url: `https://login.microsoftonline.com/&lt;tenant-id&gt;/oauth2/token`\nCREATE DATABASE SCOPED CREDENTIAL &lt;credential-name&gt;\nWITH IDENTITY = '&lt;client-id&gt;@&lt;authority-url&gt;',\n    SECRET = '&lt;client-secret&gt;'\nGO\n\nCREATE EXTERNAL DATA SOURCE &lt;external-data-source&gt;\nWITH (\n    LOCATION   = 'https://&lt;storage_account&gt;.dfs.core.windows.net/&lt;container&gt;/&lt;path&gt;',\n    CREDENTIAL = &lt;credential-name&gt;\n)\n</code></pre> <pre><code>-- The secret value must remove the leading '?'\nCREATE DATABASE SCOPED CREDENTIAL &lt;credential-name&gt;\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\n    SECRET = 'sv=2018-03-28&amp;ss=bfqt&amp;...&amp;sig=lQHczN...'\nGO\n\nCREATE EXTERNAL DATA SOURCE &lt;external-data-source&gt;\nWITH (\n    LOCATION   = 'https://&lt;storage_account&gt;.dfs.core.windows.net/&lt;container&gt;/&lt;path&gt;',\n    CREDENTIAL = &lt;credential-name&gt;\n)\n</code></pre> <p>And the permission of User, Managed Identity, or Service Principle that want to access data on target external data source should be any role in <code>Storage Blob Data Owner/Contributor/Reader</code> roles in order for the application to access the data via RBAC in Azure Portal.</p> <p>Example</p> <pre><code>USE [master];\nCREATE LOGIN [username] WITH PASSWORD = 'P@ssW0rd';\nGO\n\nUSE [database];\nCREATE USER [username] FROM LOGIN [username];\nGRANT REFERENCES ON DATABASE SCOPED CREDENTIAL::[credential-name] TO [username];\nGO\n</code></pre> <pre><code>IF NOT EXISTS (\n    SELECT *\n    FROM [sys].[external_data_sources]\n    WHERE [name] = '&lt;external-data-source-name&gt;'\n)\n    CREATE EXTERNAL DATA SOURCE &lt;external-data-source-name&gt;\n    WITH (\n        CREDENTIAL = &lt;credential-name&gt;,\n        LOCATION = 'abfss://&lt;container&gt;@&lt;storage-account&gt;.dfs.core.windows.net'\n    )\nGO\n</code></pre> <pre><code>CREATE OR ALTER VIEW [CURATED].[&lt;view-name&gt;]\nAS\n    SELECT *\n    FROM OPENROWSET(\n        BULK '/delta_silver/&lt;delta-table-name&gt;',\n        DATA_SOURCE = '&lt;external-data-source-name&gt;',\n        FORMAT = 'DELTA'\n) AS [r]\nGO\n\nGRANT SELECT ON OBJECT::[DEVDWHCURATED].[VW_DELTA_DIM_SALE_ADB] TO adbuser\nGO\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#external-data-source_1","title":"External Data Source","text":""},{"location":"services/azure/synapse/asa-external-data-source/#list-data-source","title":"List Data Source","text":"<pre><code>SELECT * FROM [sys].[external_data_sources];\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#create-data-source","title":"Create Data Source","text":"Dedicate SQL PoolServerless SQL Pool <pre><code>CREATE EXTERNAL DATA SOURCE [&lt;external-data-source&gt;]\nWITH(\n    LOCATION = 'abfss://&lt;container&gt;@&lt;storage-account&gt;.dfs.core.windows.net',\n    CREDENTIAL = &lt;credential-name&gt;,\n    PUSHDOWN = ON,\n    TYPE = HADOOP\n);\n</code></pre> <p>Note</p> <p>PolyBase data virtualization is used when the <code>EXTERNAL DATA SOURCE</code> is created with <code>TYPE=HADOOP</code>.</p> <p><code>PUSHDOWN = ON | OFF</code> is set to <code>ON</code> by default, meaning the ODBC Driver can leverage server-side processing for complex queries.</p> <pre><code>CREATE EXTERNAL DATA SOURCE [&lt;external-data-source&gt;]\nWITH(\n    LOCATION = 'abfss://&lt;container&gt;@&lt;storage-account&gt;.dfs.core.windows.net',\n    CREDENTIAL = &lt;credential-name&gt;\n);\n</code></pre> <p>For the <code>LOCATION</code>, it provide the connectivity protocol and path to the external data source. See More Supported Protocol</p> <p>Note</p> <p>If you want to use Azure AD for access an external data source you can use:</p> <pre><code>-- The Permission from this solution will up to user that want to access\n-- target external data source.\nCREATE EXTERNAL DATA SOURCE [&lt;external-data-source&gt;]\nWITH (\n    LOCATION  = 'https://&lt;storage_account&gt;.dfs.core.windows.net/&lt;container&gt;/&lt;path&gt;'\n)\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#external-file-format","title":"External File Format","text":""},{"location":"services/azure/synapse/asa-external-data-source/#create-file-format","title":"Create File Format","text":"<pre><code>CREATE EXTERNAL FILE FORMAT &lt;parquet_snappy&gt;\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n);\n</code></pre> <pre><code>CREATE EXTERNAL FILE FORMAT &lt;skip_header_csv&gt;\nWITH (\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS(\n        FIELD_TERMINATOR    = ',',\n        STRING_DELIMITER    = '\"',\n        FIRST_ROW           = 2,\n        USE_TYPE_DEFAULT    = True\n    )\n);\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#read-mores","title":"Read Mores","text":"<ul> <li>Microsoft: Develop Storage Files Access Control</li> <li>Microsoft: TSQL - Create External Data Source</li> <li>Microsoft: TSQL - Create External File Format</li> <li>Medium: Query Azure Data Lake via Synapse Serverless Security</li> </ul>"},{"location":"services/azure/synapse/asa-low-level-security/","title":"Low-Level Security","text":""},{"location":"services/azure/synapse/asa-low-level-security/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/synapse/asa-low-level-security/#read-mores","title":"Read Mores","text":"<ul> <li>SQLShack: Implementing Row Level Security</li> </ul>"},{"location":"services/azure/synapse/asa-monitoring/","title":"Monitoring","text":""},{"location":"services/azure/synapse/asa-monitoring/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/synapse/asa-monitoring/#table-size","title":"Table Size","text":"<pre><code>CREATE VIEW dbo.vTableSizes\nAS\nWITH base\nAS\n(\nSELECT\n GETDATE()                                                             AS  [execution_time]\n, DB_NAME()                                                            AS  [database_name]\n, s.name                                                               AS  [schema_name]\n, t.name                                                               AS  [table_name]\n, QUOTENAME(s.name)+'.'+QUOTENAME(t.name)                              AS  [two_part_name]\n, nt.[name]                                                            AS  [node_table_name]\n, ROW_NUMBER() OVER(PARTITION BY nt.[name] ORDER BY (SELECT NULL))     AS  [node_table_name_seq]\n, tp.[distribution_policy_desc]                                        AS  [distribution_policy_name]\n, c.[name]                                                             AS  [distribution_column]\n, nt.[distribution_id]                                                 AS  [distribution_id]\n, i.[type]                                                             AS  [index_type]\n, i.[type_desc]                                                        AS  [index_type_desc]\n, nt.[pdw_node_id]                                                     AS  [pdw_node_id]\n, pn.[type]                                                            AS  [pdw_node_type]\n, pn.[name]                                                            AS  [pdw_node_name]\n, di.name                                                              AS  [dist_name]\n, di.position                                                          AS  [dist_position]\n, nps.[partition_number]                                               AS  [partition_nmbr]\n, nps.[reserved_page_count]                                            AS  [reserved_space_page_count]\n, nps.[reserved_page_count] - nps.[used_page_count]                    AS  [unused_space_page_count]\n, nps.[in_row_data_page_count]\n    + nps.[row_overflow_used_page_count]\n    + nps.[lob_used_page_count]                                        AS  [data_space_page_count]\n, nps.[reserved_page_count]\n - (nps.[reserved_page_count] - nps.[used_page_count])\n - ([in_row_data_page_count]\n         + [row_overflow_used_page_count]+[lob_used_page_count])       AS  [index_space_page_count]\n, nps.[row_count]                                                      AS  [row_count]\nfrom\n    sys.schemas s\nINNER JOIN sys.tables t\n    ON s.[schema_id] = t.[schema_id]\nINNER JOIN sys.indexes i\n    ON  t.[object_id] = i.[object_id]\n    AND i.[index_id] &lt;= 1\nINNER JOIN sys.pdw_table_distribution_properties tp\n    ON t.[object_id] = tp.[object_id]\nINNER JOIN sys.pdw_table_mappings tm\n    ON t.[object_id] = tm.[object_id]\nINNER JOIN sys.pdw_nodes_tables nt\n    ON tm.[physical_name] = nt.[name]\nINNER JOIN sys.dm_pdw_nodes pn\n    ON  nt.[pdw_node_id] = pn.[pdw_node_id]\nINNER JOIN sys.pdw_distributions di\n    ON  nt.[distribution_id] = di.[distribution_id]\nINNER JOIN sys.dm_pdw_nodes_db_partition_stats nps\n    ON nt.[object_id] = nps.[object_id]\n    AND nt.[pdw_node_id] = nps.[pdw_node_id]\n    AND nt.[distribution_id] = nps.[distribution_id]\n    AND i.[index_id] = nps.[index_id]\nLEFT OUTER JOIN (select * from sys.pdw_column_distribution_properties where distribution_ordinal = 1) cdp\n    ON t.[object_id] = cdp.[object_id]\nLEFT OUTER JOIN sys.columns c\n    ON cdp.[object_id] = c.[object_id]\n    AND cdp.[column_id] = c.[column_id]\nWHERE pn.[type] = 'COMPUTE'\n)\n, size\nAS\n(\nSELECT\n   [execution_time]\n,  [database_name]\n,  [schema_name]\n,  [table_name]\n,  [two_part_name]\n,  [node_table_name]\n,  [node_table_name_seq]\n,  [distribution_policy_name]\n,  [distribution_column]\n,  [distribution_id]\n,  [index_type]\n,  [index_type_desc]\n,  [pdw_node_id]\n,  [pdw_node_type]\n,  [pdw_node_name]\n,  [dist_name]\n,  [dist_position]\n,  [partition_nmbr]\n,  [reserved_space_page_count]\n,  [unused_space_page_count]\n,  [data_space_page_count]\n,  [index_space_page_count]\n,  [row_count]\n,  ([reserved_space_page_count] * 8.0)                                 AS [reserved_space_KB]\n,  ([reserved_space_page_count] * 8.0)/1000                            AS [reserved_space_MB]\n,  ([reserved_space_page_count] * 8.0)/1000000                         AS [reserved_space_GB]\n,  ([reserved_space_page_count] * 8.0)/1000000000                      AS [reserved_space_TB]\n,  ([unused_space_page_count]   * 8.0)                                 AS [unused_space_KB]\n,  ([unused_space_page_count]   * 8.0)/1000                            AS [unused_space_MB]\n,  ([unused_space_page_count]   * 8.0)/1000000                         AS [unused_space_GB]\n,  ([unused_space_page_count]   * 8.0)/1000000000                      AS [unused_space_TB]\n,  ([data_space_page_count]     * 8.0)                                 AS [data_space_KB]\n,  ([data_space_page_count]     * 8.0)/1000                            AS [data_space_MB]\n,  ([data_space_page_count]     * 8.0)/1000000                         AS [data_space_GB]\n,  ([data_space_page_count]     * 8.0)/1000000000                      AS [data_space_TB]\n,  ([index_space_page_count]  * 8.0)                                   AS [index_space_KB]\n,  ([index_space_page_count]  * 8.0)/1000                              AS [index_space_MB]\n,  ([index_space_page_count]  * 8.0)/1000000                           AS [index_space_GB]\n,  ([index_space_page_count]  * 8.0)/1000000000                        AS [index_space_TB]\nFROM base\n)\nSELECT *\nFROM size\n;\n</code></pre> <p>Design tables using dedicated SQL pool in Azure Synapse Analytics</p>"},{"location":"services/azure/synapse/asa-monitoring/#operation-status","title":"Operation Status","text":"<pre><code>-- This query returns the latest operations in the server\n-- it needs to be executed in the master database\n-- the information in this table is removed automatically after 2 or 3 hours\nSELECT [session_activity_id]\n      ,[resource_type]\n      ,[resource_type_desc]\n      ,[major_resource_id]\n      ,[minor_resource_id]\n      ,[operation]\n      ,[state]\n      ,[state_desc]\n      ,[percent_complete]\n      ,[error_code]\n      ,[error_desc]\n      ,[error_severity]\n      ,[error_state]\n      ,[start_time]\n      ,[last_modify_time]\n    FROM sys.dm_operation_status\n</code></pre>"},{"location":"services/azure/synapse/asa-monitoring/#data-skew-outdated-state","title":"Data Skew &amp; Outdated State","text":"<pre><code>-- data skew -&gt; cmp_rows&gt;1mil, skew &gt;= 10%\n-- missing stats -&gt; cmp_rows&gt;1mil, ctl_rows=1000\n-- outdated stats -&gt; cmp_rows&gt;1mil, cmp_rows &lt;&gt; ctl_rows (for (cmp_rows-ctl_rows) &gt; 20%)\n\nDECLARE @minRows INT = 1000000;\nDECLARE @minSkewPercent decimal=10.0;\nDECLARE @missingStatCtlRowCount int=1000;\nDECLARE @CtlCmpRowDifferencePercentageForOutdatedStats decimal=20.0;\n\nWITH cmp_details AS\n(\n       select tm.object_id, ps.index_id, ps.distribution_id, count(ps.partition_number) [partitions], sum(ps.row_count) cmp_row_count\n       from sys.dm_pdw_nodes_db_partition_stats ps\n              join sys.pdw_nodes_tables nt on nt.object_id=ps.object_id and ps.distribution_id=nt.distribution_id\n              join sys.pdw_table_mappings tm on tm.physical_name=nt.name\n       where ps.index_id&lt;2\n       group by tm.object_id, ps.index_id, ps.distribution_id\n)\n, cmp_summary as\n(\n       select object_id, index_id, sum(cmp_row_count) cmp_row_count\n              , (max(cmp_row_count)-min(cmp_row_count)) highest_skew_rows_difference\n              , convert(decimal(10,2),((max(cmp_row_count) - min(cmp_row_count))*100.0 / nullif(sum(cmp_row_count),0))) skew_percent\n       from cmp_details\n       group by object_id, index_id\n)\n, ctl_summary as\n(\n       select t.object_id, i.index_id, s.name sch_name, t.name table_name, i.type_desc table_type, dp.distribution_policy_desc distribution_type, count(p.partition_number) [partitions], sum(p.rows) ctl_row_count\n       from sys.schemas s\n              join sys.tables t on t.schema_id=s.schema_id\n              join sys.pdw_table_distribution_properties dp on dp.object_id=t.object_id\n              join sys.indexes i on i.object_id=t.object_id and i.index_id&lt;2\n              join sys.partitions p on p.object_id=t.object_id and p.index_id=i.index_id\n       group by t.object_id, i.index_id, s.name, t.name, i.type_desc, dp.distribution_policy_desc\n)\n, [all_results] as\n(\n       select ctl.object_id, ctl.index_id, ctl.sch_name, ctl.table_name, ctl.table_type, ctl.distribution_type, ctl.[partitions]\n              , ctl.ctl_row_count, cmp.cmp_row_count, convert(decimal(10,2),(abs(ctl.ctl_row_count - cmp.cmp_row_count)*100.0 / nullif(cmp.cmp_row_count,0))) ctl_cmp_difference_percent\n              , cmp.highest_skew_rows_difference, cmp.skew_percent\n              , case\n                     when (ctl.ctl_row_count = @missingStatCtlRowCount) then 'missing stats'\n                     when ((ctl.ctl_row_count &lt;&gt; cmp.cmp_row_count) and ((abs(ctl.ctl_row_count - cmp.cmp_row_count)*100.0 / nullif(cmp.cmp_row_count,0)) &gt; @CtlCmpRowDifferencePercentageForOutdatedStats)) then 'outdated stats'\n                     else null\n                end stat_info\n              , case when (cmp.skew_percent &gt;= @minSkewPercent) then 'data skew' else null end skew_info\n       from ctl_summary ctl\n              join cmp_summary cmp on ctl.object_id=cmp.object_id and ctl.index_id=cmp.index_id\n)\nselect *\nfrom [all_results]\nwhere cmp_row_count&gt;@minRows and (stat_info is not null or skew_info is not null)\norder by sch_name, table_name\n</code></pre>"},{"location":"services/azure/synapse/asa-monitoring/#read-mores","title":"Read Mores","text":"<ul> <li> Azure Synapse Last Operations in Server</li> <li> GitHub: ProdataSQL - SynapseTools</li> <li> GitHub: ProdataSQL - SynapseTools vTableStats</li> </ul>"},{"location":"services/azure/synapse/asa-partition-view/","title":"Partition View","text":""},{"location":"services/azure/synapse/asa-partition-view/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/synapse/asa-partition-view/#manage-permission","title":"Manage Permission","text":"<ol> <li> <p>Remove bulk operations on master</p> <pre><code>USE [master];\nDENY ADMINISTER BULK OPERATIONS TO [username];\nDENY ADMINISTER BULK OPERATIONS TO [public];\nGO\n</code></pre> </li> <li> <p>Grant bulk operations on the database level</p> <pre><code>USE [database];\nGRANT ADMINISTER DATABASE BULK OPERATIONS TO [username];\nGRANT ADMINISTER DATABASE BULK OPERATIONS TO [public];\nGRANT REFERENCES ON DATABASE SCOPED CREDENTIAL::[&lt;credential-name&gt;] TO [&lt;user-name&gt;];\nGO\n</code></pre> </li> </ol>"},{"location":"services/azure/synapse/asa-partition-view/#partition-pruning","title":"Partition Pruning","text":"Parquet Delta CSV Json <pre><code>CREATE VIEW [&lt;schema-name&gt;].[&lt;view-name&gt;]\nAS\nSELECT\n    *,\n    CAST(temp.filepath(1) AS INT) AS [year],\n    CAST(temp.filepath(2) AS TINYINT) AS [month],\n    CAST(temp.filepath(3) AS TINYINT) AS [day]\nFROM\n    OPENROWSET(\n        BULK 'data/table/year=*/month=*/day=*/**',\n        DATA_SOURCE = '&lt;external-storage-name&gt;',\n        FORMAT = 'PARQUET'\n    )\nWITH (\n    [timestamp]       [datetime],\n    [edge_id]         [varchar](max),\n    [temperature]     [float],\n    [humidity]        [float],\n    [lqi]             [float],\n    [pm1.0]           [float],\n    [pm2.5]           [float],\n    [pm10.0]          [float],\n    [date]            [date]\n) AS temp\nGO\n</code></pre> <pre><code>CREATE VIEW [&lt;schema-name&gt;].[&lt;view-name&gt;]\nAS\nSELECT\n    *\nFROM\n    OPENROWSET(\n        BULK 'data/delta_table/',\n        DATA_SOURCE = '&lt;external-storage-name&gt;',\n        FORMAT = 'DELTA'\n    )\nWITH (\n    [timestamp]       [datetime],\n    [edge_id]         [varchar](max),\n    [temperature]     [float],\n    [humidity]        [float],\n    [lqi]             [float],\n    [pm1.0]           [float],\n    [pm2.5]           [float],\n    [pm10.0]          [float],\n    [date]            [date]\n) AS temp\nGO\n</code></pre> <pre><code>CREATE VIEW [&lt;schema-name&gt;].[&lt;view-name&gt;]\nAS\nSELECT\n    *\nFROM\n    OPENROWSET(\n        BULK 'data/delta_table/',\n        DATA_SOURCE = '&lt;external-storage-name&gt;',\n        FORMAT = 'CSV',\n        PARSER_VERSION ='2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '\\n',\n        HEADER_ROW = TRUE,\n        FIRSTROW = 1,\n        FIELDQUOTE = '\"',\n        ESCAPECHAR = '\\\\',\n        ROWSET_OPTIONS = '{\"READ_OPTIONS\":[\"ALLOW_INCONSISTENT_READS\"]}'\n    )\nWITH (\n    [timestamp]       [datetime],\n    [edge_id]         [varchar](max),\n    [temperature]     [float],\n    [humidity]        [float],\n    [lqi]             [float],\n    [pm1.0]           [float],\n    [pm2.5]           [float],\n    [pm10.0]          [float],\n    [date]            [date]\n) AS temp\nGO\n</code></pre> <pre><code>CREATE VIEW [&lt;schema-name&gt;].[&lt;view-name&gt;]\nAS\nSELECT\n    *\nFROM\n    ...\n</code></pre>"},{"location":"services/azure/synapse/asa-partition-view/#read-mores","title":"Read Mores","text":"<ul> <li> User Permission in Serverless SQL Pools</li> <li> Serverless SQL: Partition Pruning Delta Tables in Azure Synapse Analytics</li> </ul>"},{"location":"services/databricks/","title":"Databricks","text":"<p>Databricks is a cloud-based big data and machine learning platform based on Apache Spark. It was created by the creators of Apache Spark and provides a convenient interface for working with large volumes of data, as well as tools for analytics and machine learning.</p> <p>Note</p> <p>It is widely used in clouds such as AWS, Google, and Microsoft Azure</p> <p>Read more any information on the Databricks Blog</p>"},{"location":"services/databricks/databricks-app/","title":"Databricks: App","text":"<p>https://levelup.gitconnected.com/packaging-databricks-apps-b5e4cc88cde9</p>"},{"location":"services/databricks/databricks-aws-orchestration/","title":"Databricks: AWS - Orchestration","text":"<p>https://www.databricks.com/blog/2022/01/27/orchestrating-databricks-workloads-on-aws-with-managed-workflows-for-apache-airflow.html</p>"},{"location":"services/databricks/databricks-custom-policy/","title":"Databricks: Custom Policy","text":""},{"location":"services/databricks/databricks-custom-policy/#references","title":"References","text":"<ul> <li>https://medium.com/codex/tutorial-how-to-create-and-assign-a-custom-policy-on-databricks-cluster-7239b974c6e2</li> </ul>"},{"location":"services/databricks/databricks-custom-python-docker/","title":"Databricks: Running Python Wheel Tasks in Custom Docker Containers in Databricks","text":""},{"location":"services/databricks/databricks-custom-python-docker/#references","title":"References","text":"<ul> <li>https://towardsdatascience.com/running-python-wheel-tasks-in-custom-docker-containers-in-databricks-de3ff20f5c79</li> </ul>"},{"location":"services/databricks/databricks-data-quality/","title":"Databricks: Data Quality","text":"<p>https://medium.com/@josemanuelgarciagimenez/implementing-data-quality-with-databricks-2b15d89d3fa5</p>"},{"location":"services/databricks/databricks-delta-live-table/","title":"Delta Live Table","text":"<p>The data engineering domain has evolved recently to contain many data transformation frameworks. In the past years, frameworks like DBT and Dataform have gained momentum and proven their benefit across analytics environments. I had the chance to work with those frameworks and build data architectures that used them as a core for running data transformations and ensuring code quality.</p> <p></p>"},{"location":"services/databricks/databricks-delta-live-table/#getting-started","title":"Getting Started","text":""},{"location":"services/databricks/databricks-delta-live-table/#project-file-structure","title":"Project File Structure","text":"<pre><code>workflows/\n\u251c\u2500\u2500 datasource/\n\u2502  \u2514\u2500\u2500 &lt;env&gt;_deployment.yml\n\ndatasource/\n\u2502\n\u251c\u2500\u2500 raw/\n\u2502 \u251c\u2500\u2500 docs/\n\u2502 \u2502 \u2514\u2500\u2500 docs.yml\n\u2502 \u2502\n\u2502 \u251c\u2500\u2500 configs/\n\u2502 \u2502 \u2514\u2500\u2500 tables.yml\n\u2502 \u2502\n\u2502 \u251c\u2500\u2500 tests/\n\u2502 \u2502 \u2514\u2500\u2500 tests.yml\n\u2502 \u2502\n\u2502 \u2514\u2500\u2500 scripts/\n\u2502 \u251c\u2500\u2500 raw_customers.sql\n\u2502 \u2514\u2500\u2500 raw_orders.sql\n\u2502\n\u251c\u2500\u2500 silver/\n\u2502 \u2514\u2500\u2500 scripts/\n\u2502 \u251c\u2500\u2500 silver_customers.sql\n\u2502 \u2514\u2500\u2500 silver_orders.sql\n\u2514\u2500\u2500 ...\n\u2502\n\u2514\u2500\u2500 gold/\n\u2514\u2500\u2500 ...\n\u251c\u2500\u2500 datasource_transformation_notebook.py\n</code></pre>"},{"location":"services/databricks/databricks-delta-live-table/#references","title":"References","text":"<ul> <li>Production Ready Project File Structure for Databricks Delta Live Tables</li> </ul>"},{"location":"services/databricks/databricks-deploy-with-aws/","title":"Databricks: Deploy with AWS","text":"<p>https://www.databricks.com/blog/2017/07/13/serverless-continuous-delivery-with-databricks-and-aws-codepipeline.html https://www.linkedin.com/pulse/deploying-databricks-aws-deepak-rajak/</p>"},{"location":"services/databricks/databricks-dynamically-workflow/","title":"Databricks: Workflow Dynamically","text":""},{"location":"services/databricks/databricks-dynamically-workflow/#references","title":"References","text":"<ul> <li>https://medium.com/@rishianand.nits/create-update-databricks-workflow-dynamically-abba4b0916b8</li> </ul>"},{"location":"services/databricks/databricks-fastapi-to-serverless/","title":"Databricks: FastAPI to Serverless","text":""},{"location":"services/databricks/databricks-fastapi-to-serverless/#references","title":"References","text":"<ul> <li>https://betterprogramming.pub/build-a-fastapi-on-the-lakehouse-94e4052cc3c9</li> </ul>"},{"location":"services/databricks/databricks-fine-grained-access-control/","title":"Fine-grained Access Control","text":""},{"location":"services/databricks/databricks-fine-grained-access-control/#references","title":"References","text":"<ul> <li>Fine-grained Access Control with Permission Table in Databricks SQL</li> </ul>"},{"location":"services/databricks/databricks-func-workspace-organize/","title":"Databricks: Functional Workspace Organization","text":"<ul> <li>https://www.databricks.com/blog/2022/03/10/functional-workspace-organization-on-databricks.html</li> </ul>"},{"location":"services/databricks/databricks-lakehouse-monitoring/","title":"Monitoring","text":""},{"location":"services/databricks/databricks-lakehouse-monitoring/#references","title":"References","text":"<ul> <li>Introduction to Databricks Lakehouse monitoring</li> </ul>"},{"location":"services/databricks/databricks-lakehouse/","title":"Databricks: Lakehouse","text":"<p>https://medium.com/@tsiciliani/databricks-lakehouse-federation-1b149b123a4c</p>"},{"location":"services/databricks/databricks-liquibase/","title":"Databricks: Liquibase","text":"<p>https://medium.com/dbsql-sme-engineering/advanced-schema-management-on-databricks-with-liquibase-1900e9f7b9c0</p>"},{"location":"services/databricks/databricks-migration-workspace/","title":"Databricks: Migration Workspace","text":""},{"location":"services/databricks/databricks-migration-workspace/#references","title":"References","text":"<ul> <li>https://medium.com/d-one/databricks-workspace-migration-ce450e3931da</li> </ul>"},{"location":"services/databricks/databricks-realtime-etl/","title":"Databricks: Real-time ETL","text":"<p>https://medium.com/dbsql-sme-engineering/real-time-etl-on-databricks-sql-with-streaming-tables-and-materialized-views-e8930cdf4f1f</p>"},{"location":"services/databricks/databricks-row-and-column-level-filter/","title":"Databricks: Row Level Filtering and Column Level Masking in Databricks","text":""},{"location":"services/databricks/databricks-row-and-column-level-filter/#references","title":"References","text":"<ul> <li>https://medium.com/@rahulsoni4/row-level-filtering-and-column-level-masking-in-databricks-f561d3b81cc6</li> </ul>"},{"location":"services/databricks/databricks-tune-query-performance/","title":"Databricks: Tune Query Performance","text":"<p>https://medium.com/dbsql-sme-engineering/tune-query-performance-in-databricks-sql-with-the-query-profile-439196b18f47</p>"},{"location":"services/docker/","title":"Docker","text":"<p>Docker container use only cloud native application which the code can run on container software. The large package software such as CRM, ERP, or SAP can not run on container yet, so it still run on VM.</p> <p>Docker is not compatible manage the large resource such as 128 Core with 1 memory 1TB.</p> <p>Note</p> <p>The Production database should not run on Docker or any container services. You able to go this way, but it has many problems, like maintain solution.</p>"},{"location":"services/docker/#docker-concepts","title":"Docker Concepts","text":"<p>Docker has concept Build, Ship, and Run. This concept make Docker be the most popular container software in this world.</p> <p>Components for concepts:</p> <ul> <li>Image</li> </ul> <p>The basic of a Docker container.</p> <ul> <li>Container</li> <li>Engine</li> <li>Registry</li> </ul>"},{"location":"services/docker/docker-cmd-management/","title":"Docker System Management","text":""},{"location":"services/docker/docker-cmd-management/#usage","title":"Usage","text":"<pre><code>docker system df\n</code></pre>"},{"location":"services/docker/docker-cmd-management/#clear-cache","title":"Clear cache","text":"<pre><code>docker system prune -a\n</code></pre> <pre><code>docker builder prune\n</code></pre>"},{"location":"services/docker/docker-compose-postgresql/","title":"Docker: Docker Compose - Postgres","text":"<p>https://medium.com/towards-data-engineering/running-a-postgresql-and-pgadmin4-instance-using-docker-compose-c6dd6e6e03bb</p>"},{"location":"services/docker/docker-file/","title":"Docker: Dockerfile","text":""},{"location":"services/docker/docker-file/#docker-init","title":"Docker Init","text":"<p>Quote</p> <p>The docker init makes dockerization a piece of cake, especially for the Docker newbies. It eliminates the manual task of writing Dockerfiles and other configuration files, saving time and minimizing errors.<sup>1</sup></p> <p><code>docker init</code> is a command-line utility that helps in the initialization of Docker resources within a project. It creates Dockerfiles, Compose files, and <code>.dockerignore</code> files based on the project\u2019s requirements.</p> <p>Note</p> <p>Latest version of <code>docker init</code> supports Go, Python, Node.js, Rust, ASP.NET, PHP, and Java. It is available with Docker Desktop.</p>"},{"location":"services/docker/docker-file/#multi-stage-builds","title":"Multi-Stage Builds","text":"Python <pre><code># temp stage\nFROM python:3.9-slim as builder\n\nWORKDIR /app\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends gcc\n\nCOPY requirements.txt .\nRUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt\n\n\n# final stage\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY --from=builder /app/wheels /wheels\nCOPY --from=builder /app/requirements.txt .\n\nRUN pip install --no-cache /wheels/*\n</code></pre>"},{"location":"services/docker/docker-file/#references","title":"References","text":"<ul> <li>TestDriven: Tips - Docker multi-stage builds</li> </ul> <ol> <li> <p>You should stop writing Dockerfiles today \u2014 Do this instead \u21a9</p> </li> </ol>"},{"location":"services/google/","title":"Google","text":""},{"location":"services/google/#usecases","title":"Usecases","text":"<ul> <li>PySpark in Google Cloud Platform</li> </ul>"},{"location":"services/google/google-api-authen/","title":"Google: API Authentication","text":""},{"location":"services/google/google-api-authen/#getting-started","title":"Getting Started","text":""},{"location":"services/google/google-api-authen/#1-create-credential","title":"1) Create Credential","text":"<ul> <li>Go to APIs &amp; Services  On Enabled APIs &amp; Services  Click ENABLE APIS AND SERVICES</li> <li>Find your API that want to authorize  Click ENABLE</li> <li>On Credentials  Click CREATE CREDENTIALS and   select OAuth client ID</li> <li>Drop down and select Web Application  Pass a credential   name like <code>app-bigquery-server</code></li> <li>On Authorized redirect URIs  Pass <code>http://localhost:8080</code>  Click ADD URI and then CREATE</li> <li>Copy Client ID and Client Secret ID from this creation process</li> </ul>"},{"location":"services/google/google-api-authen/#2-client-authenticate","title":"2) Client Authenticate","text":"<ul> <li> <p>Get Authorization Code</p> <pre><code>GET /o/oauth2/v2/auth HTTP/1.1\nHost: accounts.google.com\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={client-id}&amp;\nredirect_uri={redirect-uri}&amp;\nresponse_type=code&amp;\nscope={scope}&amp;\naccess_type=offline&amp;\ninclude_granted_scopes=true&amp;\nprompt=consent\n</code></pre> </li> <li> <p>Request Access and Refresh tokens</p> <pre><code>POST /token HTTP/1.1\nHost: oauth2.googleapis.com\nContent-Type: application/x-www-form-urlencoded\n\ncode={authorization-code}&amp;\nclient_id={client-id}.apps.googleusercontent.com&amp;\nclient_secret={client-secret}&amp;\nredirect_uri=https://oauth2-login.appclient.com/code&amp;\ngrant_type=authorization_code\n</code></pre> <p>Abstract</p> <p>When does a refresh token expire?:</p> <p>Refresh tokens do not expire, unless there are few special conditions:</p> <ul> <li>The user has removed your Google application.</li> <li>The refresh token has not been used for six months.</li> <li>The user changed password and the refresh token contained Gmail scopes.     This means that the refresh token will be invalidated only when he had previously     given the permisions for managing his Gmail, and then later changed his password.     For the rest of Google services like Youtube, Calendar etc, a changed password     will not invalidate the refresh token.</li> <li>The application generated a new refresh token for the user for more than 50 times.</li> </ul> </li> <li> <p>Re-generate Access Token</p> <pre><code>POST /token HTTP/1.1\nHost: oauth2.googleapis.com\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={client-id}.apps.googleusercontent.com&amp;\nclient_secret={client-secret}&amp;\nrefresh_token={refresh-token}&amp;\ngrant_type=refresh_token\n</code></pre> </li> <li> <p>Varify Access Token</p> <pre><code>GET /oauth2/v3/tokeninfo HTTP/1.1\nHost: googleapis.com\nContent-Type: application/x-www-form-urlencoded\n\naccess_token={access-token}\n</code></pre> </li> </ul> <p> How Can I Verify a Google Authentication API Access Token?</p> <p>Note</p> <p>Revoke:</p> <pre><code>POST /o/oauth2/revoke HTTP/1.1\nHost: accounts.google.com\nContent-Type: application/x-www-form-urlencoded\n\ntoken={refresh-token}\n</code></pre>"},{"location":"services/google/google-api-authen/#oauth-playground","title":"OAuth Playground","text":"<p>Developer OAuth Playground</p>"},{"location":"services/google/google-api-authen/#references","title":"References","text":"<ul> <li> Google Developer: Identity - OAuth2 Web Server</li> <li> Getting Google OAuth Access Token using Google APIs</li> <li> How to get Google Authorization Code using PostMan</li> </ul>"},{"location":"services/google/bigquery/getting-started/","title":"BigQuery: Getting Started","text":"<p>https://medium.com/@vutrinh274/i-spent-6-hours-understanding-the-design-principles-of-bigquery-heres-what-i-found-6050cb7880fa</p>"},{"location":"services/google/functions/gcf-to-managing-secrets/","title":"Cloud Functions: To Managing Secrets","text":""},{"location":"services/google/functions/gcf-to-managing-secrets/#references","title":"References","text":"<ul> <li>Medium: Google Cloud - Manage Secrets for GCP Cloud Functions</li> </ul>"},{"location":"services/infisical/","title":"Infisical","text":""},{"location":"services/k8s/","title":"Kubernetes","text":"<p>Kubernetes (K8s)</p>"},{"location":"services/k8s/#installation","title":"Installation","text":"MacOS <pre><code>$ brew install kubectl\n$ brew install helm\n$ brew install k9s\n</code></pre>"},{"location":"services/k8s/#core-concepts-of-kubernetes","title":"Core Concepts of Kubernetes","text":"<ul> <li>Control Plane \u2014 The \u2018brain\u2019 of the cluster, responsible for managing the nodes and applications that are running on them. It consists of several components, such as API server, etcd, scheduler, and controller manager.</li> <li>API Server \u2014 It exposes a RESTful API that allows users to interact with the cluster.</li> <li>etcd \u2014 Database that stores the configuration data for the cluster.</li> <li>Scheduler \u2014 Responsible for placing applications onto nodes, depending on the availability and requirements of the resources.</li> <li>Controller Manager \u2014 monitoring the state of the cluster.</li> </ul>"},{"location":"services/k8s/#data-plane-components-of-kubernetes","title":"Data Plane components of Kubernetes","text":"<ul> <li> <p>Containers: Kubernetes is built around containerization. A container packages an application with all its dependencies and runtime environment. This ensures consistency across various development, testing, and production environments.</p> </li> <li> <p>Pods: The smallest deployable units in Kubernetes. A pod can contain one or more containers that share storage, network, and specifications on how to run the containers. Pods are ephemeral and disposable.</p> </li> <li> <p>Deployments: This is a higher-level concept that manages declarative updates for Pods and ReplicaSets. Deployments use a template for a pod and control parameters to scale the number of pods, rolling update strategies, and desired state. used for managing the rollout and scaling of applications.</p> </li> </ul> <p></p> <ul> <li>Services: Exposing an application running on a POD to the network (other PODS).   A Kubernetes Service is an abstraction layer that defines a logical set of Pods   and a policy by which to access them. This is often used to provide network access   to a set of pods.</li> </ul> <p></p> <ul> <li>Ingress \u2014 Ingresses provide external access to services within the cluster.   They manage external HTTP and HTTPS routing, allowing us to define rules for   handling incoming traffic.</li> </ul> <p></p> <ul> <li>Statefulsets \u2014 A StatefulSet is a set of pods with a unique, persistent hostname   and ID. StatefulSets are designed to run stateful applications in Kubernetes   with dedicated persistent storage. When pods run as part of a StatefulSet,   Kubernetes keeps state data in the persistent storage volumes of the StatefulSet,   even if the pods shut down. StatefulSets provide predictable order and state for   stateful workloads such as databases. They are scaled up or down sequentially   in a predictable order. This is very important for databases. StatefulSets have   a unique DNS name based on this order. This is important for workloads such as   Apache Kafka which distribute the data amongst their brokers; hence, one broker   is not the same as another.</li> </ul> <p></p> <ul> <li> <p>Config map \u2014 Config Maps store configuration data separately from application code, allowing for easier configuration changes without modifying the container image.</p> </li> <li> <p>Secret \u2014 Secrets store sensitive information such as passwords, API keys, and tokens. They are base64-encoded and can be mounted into pods or used as environment variables.</p> </li> </ul> <p></p> <ul> <li>Daemonsets \u2014 DaemonSets ensure that a copy of a pod runs on every node in the cluster. This is particularly useful for system-level tasks such as log collection, monitoring, or node-specific functionality.</li> </ul> <p></p> <ul> <li>Job \u2014 Jobs manage the execution of short-lived tasks to completion. They are useful for batch processing, data migration, or any task that needs to run to completion but doesn\u2019t require continuous execution.</li> <li>Cron Jobs \u2014 Cron Jobs enables the scheduling of recurring tasks in a Kubernetes cluster, similar to the cron jobs in traditional Unix/Linux systems. They are valuable for automating periodic processes.</li> </ul> <p></p> <ul> <li> <p>Namespace \u2014 Namespaces provide a way to divide cluster resources into virtual   clusters, enabling multi-tenancy and resource isolation. They help organize and   manage objects within a cluster.</p> </li> <li> <p>Role \u2014 Roles define a set of permissions within a namespace. They are used in conjunction with role bindings to grant access to resources within the cluster.</p> </li> <li>Role Binding \u2014 A role binding grants the permissions defined in a role to a user or set of users. It holds a list of subjects (users, groups, or service accounts), and a reference to the role being granted</li> <li>Service Account \u2014 A service account provides an identity for processes that run in a Pod, and maps to a ServiceAccount object. When you authenticate to the API server, you identify yourself as a particular user.</li> </ul> <p></p> <ul> <li>Nodes: These are worker machines in Kubernetes, which can be either physical or virtual. Each node runs pods and is managed by the master node. Nodes have the necessary services to run pods and are managed by the control plane.</li> <li> <p>Label, Annotation &amp; Selectors \u2014 Labels are key-value pairs attached to objects (such as pods, nodes, services, and more) to help organize and categorize them. Labels can be used with selectors to filter and select specific resources. Annotations in Kubernetes are a mechanism for adding arbitrary metadata to objects (such as pods, services, nodes, and more) that are not used for identification or selection purposes.</p> </li> <li> <p>Affinity \u2014 Affinities are used to express Pod scheduling constraints that can match the characteristics of candidate Nodes and the Pods that are already running on those Nodes. A Pod that has an \u201caffinity\u201d to a given Node is more likely to be scheduled to it; conversely, an \u201canti-affinity\u201d makes it less probable it\u2019ll be scheduled. Affinity can be either an attracting affinity or a repelling anti-affinity.</p> </li> <li>Taints &amp; toleration \u2014 Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.</li> <li>Cert Manager \u2014 cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates.</li> <li>Kubectl \u2014 Kubectl is the command-line interface (CLI) for interacting with Kubernetes clusters. It allows users to perform various operations on Kubernetes resources, such as deploying applications, inspecting and managing cluster resources, and troubleshooting issues. Kubectl communicates with the Kubernetes API server to execute these commands and obtain information about the cluster\u2019s state.</li> </ul> <p>??? note \"Kubectl commands\"</p> <pre><code>    ```shell\n    kubectl version --client\n    kubectl cluster-info\n    kubectl config view\n    kubectl config current-context\n    kubectl config use-context &lt;cluster name&gt;\n    kubectl get nodes\n    kubectl label node &lt;node-name&gt; node-role.kubernetes.io/worker=worker\n    kubectl drain &lt;node-name&gt;\n    kubectl cordon &lt;node-name&gt;\n    kubectl uncordon &lt;node-name&gt;\n    kubectl apply -f &lt;K8s-manifest.yaml&gt;\n    kubectl delete -f &lt;K8s-manifest.yaml&gt;\n\n    # Pod\n    kubectl get pods -n &lt;namespace&gt; -o wide\n    kubectl get pods -n &lt;namespace&gt; -w\n    kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n    kubectl logs -f pod &lt;pod-name&gt; -n &lt;namespace&gt;\n    kubectl logs --since=1h &lt;pod-name&gt; -n &lt;namespace&gt;\n    kubectl logs --tail=200 &lt;pod-name&gt; -n &lt;namespace&gt;\n    kubectl exec -it &lt;pod-name&gt; -c &lt;container_name&gt; -n &lt;namespace&gt; -- bash\n    kubectl get events --field-selector involvedObject.name=&lt;pod-name&gt; --field-selector involvedObject.kind=Pod\n    kubectl port-forward &lt;pod-name&gt; -n &lt;namespace&gt; &lt;local-port&gt;:&lt;pod-port&gt;\n\n    # Deployment\n    kubectl get deployments -n &lt;namespace&gt;\n    kubectl edit deployments &lt;deployment_name&gt; -n &lt;namespace&gt;\n    kubectl set image deployment/nginx-deployment nginx=nginx:2.0\n    kubectl scale deployment &lt;deployment-name&gt; -n &lt;namespace&gt; --replicas=&lt;desired-replica-count&gt;\n    kubectl rollout restart deployment/&lt;deployment-name&gt;\n    kubectl rollout status deployment &lt;deployment-name&gt; -n &lt;namespace&gt;\n    kubectl delete deployment &lt;deployment_name&gt; -n &lt;namespace&gt;\n\n    # Services\n    kubectl get services -n &lt;namespace&gt;\n    kubectl describe service &lt;service-name&gt; -n &lt;namespace&gt;\n    kubectl expose deployment &lt;deployment-name&gt; -n &lt;namespace&gt; --type=NodePort --port=&lt;port&gt;\n\n    # Config Map &amp; Secret\n    kubectl create configmap &lt;configmap-name&gt; --from-file=&lt;path-to-file&gt; -n &lt;namespace&gt;\n    kubectl create secret generic &lt;secret-name&gt; --from-file=file_name -n &lt;namespace&gt;\n\n    # Role &amp; Role binding\n    kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods\n    kubectl create rolebinding bob-admin-binding --clusterrole=pod-reader --user=bob --namespace=&lt;namespace&gt;\n\n    # Job / CronJob\n    kubectl create cronjob my-cron --image=busybox --schedule=\"*/5 * * * *\" -- echo hello -n &lt;namespace&gt;\n\n    # Patch\n    kubectl patch svc my-apache -n apache-http-server -p '{\"spec\":{\"externalIPs\":[\"172.20.82.86\"]}}'\n\n    # Copy from local to pod\n    kubectl cp -n &lt;namespace&gt; &lt;local_file_path&gt; &lt;pod_name&gt;:/path/on/container/file_name\n\n    # Auth Check\n    kubectl --as=system:serviceaccount:NAMESPACE:SERVICE_ACCOUNT auth can-i get secret/SECRET_NAME\n    ```\n</code></pre> <ul> <li>Helm \u2014 is a package manager used to manage and deploy Kubernetes applications. Helm was developed to make complex application deployments and configurations in Kubernetes environments more manageable and reusable. Examples are apache-airflow/airflow and datahub/datahub.</li> </ul> <p>??? note \"Helm commands\"</p> <pre><code>    ```shell\n    # Helm Chart\n    helm create mychart\n    helm install myrelease ./mychart\n    helm uninstall myrelease\n    helm upgrade myrelease ./mychart\n    helm upgrade myrelease ./mychart --set key1=value1,key2=value2\n    helm rollback myrelease 1\n    helm repo list\n    helm repo add apache-airflow https://airflow.apache.org\n    helm repo update\n    helm install airflow apache-airflow/airflow --namespace airflow --create-namespace -f values.yaml\n    kubectl port-forward svc/airflow-webserver 8080:8080 --namespace airflow\n\n    # Examples from DE charts\n    helm repo add kafka https://charts.bitnami.com/bitnami\n    helm install kafka bitnami/kafka\n    helm repo add pinot https://raw.githubusercontent.com/apache/pinot/master/kubernetes/helm\n    helm install pinot pinot/pinot\n    helm repo add superset https://apache.github.io/superset\n    helm install superset superset/superset -f superset-values.yaml\n    ```\n</code></pre> <ul> <li>Operators \u2014 Kubernetes\u2019 operator pattern concept lets you extend the cluster\u2019s behavior without modifying the code of Kubernetes itself by linking controllers to one or more custom resources. Operators are clients of the Kubernetes API that act as controllers for a Custom Resource. Examples are sparkoperator, and druid-operator.</li> </ul>"},{"location":"services/k8s/#common-kubernetes-errors","title":"Common Kubernetes Errors","text":"<ul> <li>CrashLoopBackOff \u2014 pod repeatedly crashes and is restarted by the kubelet, faulty container image, resource constraints, or problems with the application itself.</li> <li>ImagePullBackOff \u2014 kubelet is unable to pull the container image from the specified registry, incorrect image names or tags, network problems, or authentication issues.</li> <li>OutOfMemory \u2014 pod consumes more memory than its resource limits allow.</li> <li>OutOfMemoryKilled \u2014 The pod consumes more memory than its limits allow, and the kubelet kills the container to prevent the pod from consuming even more memory.</li> <li>OutOfCPU \u2014 The pod consumes more CPU resources than its limits allow.</li> <li>ImagePullError \u2014The kubelet is unable to pull the container image from the specified registry due to a non-HTTP error, network connectivity problems, or registry availability issues.</li> <li>EvictionWarning \u2014 The pod is at risk of being evicted from a node due to resource constraints.</li> <li>FailedScheduling \u2014 The scheduler is unable to find a node that meets the resource and placement requirements of a pod.</li> <li>DiskPressure \u2014 The node runs out of available disk space.</li> <li>NodeUnreachable \u2014 The pod is unable to communicate with its assigned node.</li> <li>RunContainerError \u2014 The issue is usually due to misconfiguration such as: Mounting a not-existent volume such as ConfigMap or Secrets. Mounting a read-only volume as read-write.</li> <li>Pod in pending state \u2014 The cluster doesn\u2019t have enough resources such as CPU and memory to run the Pod. The current Namespace has a ResourceQuota object and creating the Pod will make the Namespace go over the quota. The Pod is bound to a Pending PersistentVolumeClaim.</li> </ul>"},{"location":"services/k8s/#practices","title":"Practices","text":"<ul> <li>Most common mistakes to avoid when using Kubernetes: Anti-Patterns</li> </ul>"},{"location":"services/k8s/#references","title":"References","text":"<ul> <li>LearnK8s - Troubleshooting Deployments</li> <li>Intro to K8s for Data Engineers</li> <li>https://blog.cloudnatician.com/%E0%B8%AD%E0%B8%98%E0%B8%B4%E0%B8%9A%E0%B8%B2%E0%B8%A2-kubernetes-%E0%B8%9E%E0%B8%B7%E0%B9%89%E0%B8%99%E0%B8%90%E0%B8%B2%E0%B8%99-%E0%B9%83%E0%B8%99-5-%E0%B8%99%E0%B8%B2%E0%B8%97%E0%B8%B5-823cb6190c65</li> <li>Kubernetes for Data Engineering</li> </ul>"},{"location":"services/k8s/k8s-networking/","title":"Networking","text":"<p>The Kubernetes network model makes it possible for different components of a Kubernetes cluster, like Nodes, Pods, Services, and external traffic, to talk to each other. Kubernetes is a distributed system, the network plane spans across our cluster\u2019s physical Nodes. It uses a virtual overlay network that provides a flat structure for our cluster resources to connect to.</p>"},{"location":"services/k8s/k8s-networking/#references","title":"References","text":"<ul> <li>K8s Networking</li> </ul>"},{"location":"services/k8s/k8s-pod-scheduling/","title":"Pod Scheduling","text":"<p>Scheduling in Kubernetes is a core component as it determines where a pod will be launched. For every pod scheduler finds the best Node for launching it. There are several ways available through which one can determine on what node, pods can be placed.</p> <p>Among these features, Node selector, affinity and anti-affinity, taints &amp; toleration, stand out as essential tools for fine-tuning pod placement according to specific requirements.</p> <p>Let\u2019s take a look at some of them.</p>"},{"location":"services/k8s/k8s-pod-scheduling/#node-selector","title":"Node Selector","text":"<p>Node Selector is the simplest yet effective way to control pod scheduling on specific nodes in a Kubernetes cluster based on labels. Node Selector allows us to specify a label query (key-value pairs defined in Pod spec) that must be satisfied by a node for a pod to be scheduled on that node. This feature provides a straightforward mechanism for pod placement according to simple matching attributes like node labels.</p> <p>We can label the nodes where we want specific pods to be scheduled:</p> <pre><code>kubectl label nodes &lt;node-name&gt; disktype=ssd\n</code></pre> <p>Specifying Node Selector in Pod Spec:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: spark-executors\nspec:\n  containers:\n  - name: spark-executors\n    image: apache-spark:v3.4.1\n  nodeSelector:\n    disktype: ssd\n</code></pre> <p>This pod will only be scheduled on nodes labeled with <code>disktype=ssd</code>.</p>"},{"location":"services/k8s/k8s-pod-scheduling/#references","title":"References","text":"<ul> <li>K8s for Data Engineers \u2014 Pod Scheduling</li> </ul>"},{"location":"services/k8s/k8s-rbac/","title":"RBAC","text":"<p>Quote</p> <p>Users are mapped to roles, roles are mapped to set of permissions that allow access to resource(s).</p> <p>When K8s receives a new request K8s API server performs the following steps:</p> <ul> <li>Authenticate the user, if validation fails return <code>401 unauthorised</code></li> <li>Authorise the request, if it fails return <code>403 Forbidden</code></li> </ul>"},{"location":"services/k8s/k8s-rbac/#references","title":"References","text":"<ul> <li>K8s for Data Engineers \u2014 RBAC</li> </ul>"},{"location":"services/k8s/k8s-state-phase/","title":"State &amp; Phase","text":""},{"location":"services/k8s/k8s-state-phase/#pod-phases","title":"Pod phases","text":"<p>Pods have a predetermined life cycle. We can use the Kubernetes API to check its status. Pod statuses are crucial in understanding the health and state of a pod at any given time. There are five primary phases or statuses that a pod can be in.</p> <p></p>"},{"location":"services/k8s/k8s-state-phase/#pending","title":"Pending","text":"<p>The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.</p> <ul> <li>Insufficient resources: When the cluster does not have enough resources to accommodate the pod\u2019s resource requests, the pod cannot be scheduled.</li> <li>Image issues: If there is a wrong image name specified in the pod spec, or issues are pulling the image from the repository, the pod will not start.</li> <li>Node selector and node labels mismatches: Node selectors are used to target specific nodes for pod placement. If the labels on the nodes do not match the selectors defined in the pod specification, the pod will remain unscheduled.</li> <li>Taints and Tolerations: If a node is \u201ctainted\u201d, it\u2019s marked to repel certain Pods. A Pod with a matching \u201ctoleration\u201d can be scheduled on such a node. If no node with a suitable toleration for the Pod\u2019s taints exists, the Pod will stay in Pending status.</li> <li>Unschedulable nodes: Nodes may be marked as unschedulable for various reasons, such as maintenance, which prevents pods from being scheduled on them.</li> <li>Rolling update surge</li> <li>Inter pod affinity &amp; anti affinity</li> <li>PVC linked issues</li> </ul>"},{"location":"services/k8s/k8s-state-phase/#running","title":"Running","text":"<p>The Pod has been bound to a node, and all the containers have been created. At least one container is still running, or is in the process of starting or restarting.</p>"},{"location":"services/k8s/k8s-state-phase/#succeeded","title":"Succeeded","text":"<p>All containers in the Pod have terminated in success, and will not be restarted.</p>"},{"location":"services/k8s/k8s-state-phase/#failed","title":"Failed","text":"<p>All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system.</p>"},{"location":"services/k8s/k8s-state-phase/#unknown","title":"Unknown","text":"<p>For some reason the state of the Pod could not be obtained, typically due to an error in communicating with the host of the Pod.</p>"},{"location":"services/k8s/k8s-state-phase/#references","title":"References","text":"<ul> <li>K8s for Data Engineers \u2014 Pod &amp; Container States/Phase</li> </ul>"},{"location":"services/postgres/","title":"Postgres","text":"<p>PostgreSQL isn't just a simple relational database; it\u2019s a data management framework with the potential to engulf the entire database realm. The trend of \u201cUsing Postgres for Everything\u201d is no longer limited to a few elite teams but is becoming a mainstream best practice.</p> <ul> <li>why-use-postgresql-over-sql-server</li> <li>PostgreSQL as a Cache</li> <li>Postgres is eating the database world</li> </ul>"},{"location":"services/postgres/#practices","title":"Practices","text":"<p>Data Engineer: PostgreSQL Best Practice</p>"},{"location":"services/postgres/postgres-optimize-tricks/","title":"Postgres: Optimize Tricks","text":"<p>https://blog.stackademic.com/postgresql-optimization-tricks-how-to-load-data-fast-part-1-6cba8029faf7</p>"},{"location":"services/postgres/postgres-with-k8s/","title":"PostgreSQL in Kubernetes","text":"<p>Recommended Approach for PostgreSQL in Kubernetes</p>"},{"location":"services/server/","title":"Edge Server","text":""},{"location":"services/server/edge-sftp/","title":"SFTP Server","text":""},{"location":"services/server/edge-sftp/#gen-ssh","title":"Gen SSH","text":"<pre><code>$ ssh-keygen\n</code></pre> <pre><code>$ ls -l /home/user/.ssh\n-rw------- 1 user user 2610 Feb  7 15:11 id_rsa\n-rw-r--r-- 1 user user  573 Feb  7 15:11 id_rsa.pub\n</code></pre> <p>Take note of the permissions of the private key ( id_rsa ). SSH Private Key files should ALWAYS HAVE 600 PERMISSIONS! If not, change its permission to the said value using the chmod command:</p> <pre><code>$ chmod 600 /home/user/.ssh/id_rsa\n</code></pre> <pre><code>$ ssh-copy-id USER@IP\n</code></pre> <p>If you do not have <code>ssh-copy-id</code> available, but you have password-based SSH access to an account on your server, you can upload your keys using a conventional SSH method.</p> <pre><code>$ cat ~/.ssh/id_rsa.pub | ssh username@remote_host \"mkdir -p ~/.ssh &amp;&amp; cat &gt;&gt; ~/.ssh/authorized_keys\"\n</code></pre> <pre><code>$ ssh USER@IP\n</code></pre>"},{"location":"services/server/edge-sftp/#connect-with-python","title":"Connect with Python","text":"<p>...</p>"},{"location":"services/server/edge-ssh/","title":"SSH","text":""},{"location":"services/server/edge-ssh/#certificate","title":"Certificate","text":"<p>There are 4 different ways to present certificates and their components:</p> PEM <p>Governed by RFCs, used preferentially by open-source software because it is text-based and therefore less prone to translation/transmission errors. It can have a variety of extensions (<code>.pem</code>, <code>.key</code>, <code>.cer</code>, <code>.cert</code>, more)</p> PKCS7 <p>An open standard used by Java and supported by Windows. Does not contain private key material.</p> PKCS12 <p>A Microsoft private standard that was later defined in an RFC that provides enhanced security versus the plain-text PEM format. This can contain private key and certificate chain material. Its used preferentially by Windows systems, and can be freely converted to PEM format through use of openssl.</p> DER <p>The parent format of PEM. It's useful to think of it as a binary version of the base64-encoded PEM file. Not routinely used very much outside of Windows.</p>"},{"location":"services/server/edge-ssh/#authenticate-with-password-and-public-key","title":"Authenticate with Password and Public Key","text":"<p>First, we set the SSHD configuration file for allow support public key and password authentication methods together.</p> ~/etc/ssh/sshd_config<pre><code>AuthenticationMethods \"publickey,password\"\n</code></pre> <p>Add a command to get the public key matching process after the default step for easy maintenance in the future.</p> ~/etc/ssh/sshd_config<pre><code>AuthorizedKeysCommand /etc/ssh/authorized.sh %u\nAuthorizedKeysCommandUser root\n</code></pre> <p>Note</p> <p>AuthorizedKeysCommand will run after SSH daemon read public key from <code>authorized_keys</code> and does not found any matching key.</p> <p>Create bash script with root user,</p> /etc/ssh/authorized.sh<pre><code>#!/bin/bash\nfor file in /home/$1/.ssh/*.pub; do\n   cat $file;\ndone\n</code></pre> <p>Should grant permission with <code>sudo chmod 755 /etc/ssh/authorized.sh</code> for execute by SSH daemon.</p> <p>Add your client public key to <code>~/.ssh/</code> path, for example, I will add my public key with <code>username.pub</code>.</p> <pre><code>~/.ssh/\n    .\n    ..\n    authorized_keys\n    username.pub\n</code></pre> <p>Finally, refresh ssh service</p> Ubuntu/Debian <pre><code>sudo service ssh reload\n</code></pre> <p>Testing,</p> DefaultFix Private Key Path <pre><code>ssh username@hostname\n</code></pre> <pre><code>ssh -i /path/of/private-key/username username@hostname\n</code></pre>"},{"location":"services/server/edge-ssh/#multiple-public-keys","title":"Multiple Public Keys","text":"<p>Generate multi pair of private and public key,</p> <pre><code>ssh-keygen -t rsa -f ~/.ssh/id_rsa.home\nssh-keygen -t rsa -f ~/.ssh/id_rsa.work\n</code></pre> ~/.ssh/config<pre><code>Host home\n    Hostname home.example.com\n    IdentityFile ~/.ssh/id_rsa.home\n    User &lt;your home acct&gt;\n    IdentitiesOnly yes\n\nHost work\n    Hostname work.example.com\n    IdentityFile ~/.ssh/id_rsa.work\n    User &lt;your work acct&gt;\n    IdentitiesOnly yes\n</code></pre>"},{"location":"services/server/edge-ssh/#references","title":"References","text":"<ul> <li> Key based SSH login that requires both key AND password</li> </ul>"},{"location":"services/snowflake/","title":"Snowflake","text":""},{"location":"services/snowflake/snf-data-wash/","title":"Data Wash","text":""},{"location":"services/snowflake/snf-data-wash/#references","title":"References","text":"<ul> <li>DataWash: An Advanced Snowflake Data Quality Tool Powered by Snowpark \u2014 Part 1</li> </ul>"},{"location":"services/terraform/","title":"Terraform","text":"<p>Terraform for Data Engineer</p> <p>https://blog.skooldio.com/terraform-vs-ansible/#:~:text=%E0%B8%AA%E0%B8%A3%E0%B8%B8%E0%B8%9B%E0%B9%81%E0%B8%A5%E0%B9%89%E0%B8%A7%20%E0%B8%AA%E0%B8%B3%E0%B8%AB%E0%B8%A3%E0%B8%B1%E0%B8%9A%20Terraform%20%E0%B8%A1%E0%B8%B5,%E0%B8%97%E0%B8%B3%20Provisioning%20infra%20%E0%B9%84%E0%B8%94%E0%B9%89%E0%B9%80%E0%B8%8A%E0%B9%88%E0%B8%99 https://nopnithi.medium.com/%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B9%81%E0%B8%95%E0%B8%81%E0%B8%95%E0%B9%88%E0%B8%B2%E0%B8%87%E0%B8%A3%E0%B8%B0%E0%B8%AB%E0%B8%A7%E0%B9%88%E0%B8%B2%E0%B8%87-terraform-vs-ansible-c912bdee04dd</p>"},{"location":"services/terraform/tf-azure-databricks/","title":"To Azure Databricks","text":"<p>```terraform titles=\"variables.tf\"</p>"},{"location":"services/terraform/tf-azure-databricks/#-","title":"------------------------","text":""},{"location":"services/terraform/tf-azure-databricks/#authentication-variables","title":"Authentication Variables","text":""},{"location":"services/terraform/tf-azure-databricks/#-_1","title":"------------------------","text":"<p>variable \"aad_tenant_id\" {   type        = string   description = \"The id of the Azure Tenant to which all subscriptions belong\" }</p> <p>variable \"aad_subscription_id\" {   type        = string   description = \"The id of the Azure Subscription\" }</p> <p>variable \"aad_client_id\" {   type        = string   description = \"The client id of the Service Principal for interacting with Azure resources\" }</p> <p>variable \"aad_client_secret\" {   type        = string   description = \"The client secret of the Service Principal for interacting with Azure resources\"   sensitive   = true }</p>"},{"location":"services/terraform/tf-azure-databricks/#-_2","title":"-------------------","text":""},{"location":"services/terraform/tf-azure-databricks/#terraform-variables","title":"Terraform Variables","text":""},{"location":"services/terraform/tf-azure-databricks/#-_3","title":"-------------------","text":"<p>variable \"azure_service_principal_display_name\" {   description = \"A display name for the Azure Active Directory (Azure AD) Service Principal.\"   type        = string   default     = \"Terraform Databricks\" } ```</p>"},{"location":"services/terraform/tf-azure-databricks/#references","title":"References","text":"<ul> <li>https://medium.com/@alonso.md/deploy-azure-databricks-using-terraform-6e8a39aa7287</li> <li>https://gmusumeci.medium.com/how-to-deploy-databricks-in-azure-with-terraform-step-by-step-e1262e456be9</li> </ul>"},{"location":"services/terraform/tf-databricks/","title":"Terraform: Databricks","text":"<p>https://github.com/databricks/terraform-databricks-examples https://github.com/databricks/terraform-databricks-examples/blob/main/modules/aws-databricks-workspace/variables.tf</p>"},{"location":"services/terraform/tf-manage-secret/","title":"Terraform: Manage Secret","text":"<p>One of the most common questions we get about using Terraform to manage infrastructure as code (IaC) is how to handle secrets such as passwords, API keys, and other sensitive data.</p> <p>For example, here\u2019s a snippet of Terraform code that can be used to deploy MySQL using Amazon RDS:</p> <pre><code>resource \"aws_db_instance\" \"example\" {\n  engine               = \"mysql\"\n  engine_version       = \"5.7\"\n  instance_class       = \"db.t2.micro\"\n  name                 = \"example\"\n\n  # How should you manage the credentials for the master user?\n  username             = \"???\"\n  password             = \"???\"\n}\n</code></pre>"},{"location":"services/terraform/tf-manage-secret/#technique-environment-variables","title":"Technique: Environment Variables","text":"<pre><code>resource \"aws_db_instance\" \"example\" {\n  engine               = \"mysql\"\n  engine_version       = \"5.7\"\n  instance_class       = \"db.t2.micro\"\n  name                 = \"example\"\n\n  # Set the secrets from variables\n  username             = var.username\n  password             = var.password\n}\n</code></pre>"},{"location":"services/terraform/tf-manage-secret/#references","title":"References","text":"<ul> <li>GruntWork: A Comprehensive Guide to Manage Secrets in Terraform</li> </ul>"},{"location":"tools/","title":"Tools","text":"<p>Warning</p> <p>I will filter Data Engineering Tools on this session that do not dynamically and flexibility for the most Data Architect and Modern Data Strack.</p> <p> Modern Data Stack (MDS) is a collection of Software Tools and Cloud Services that used to Collect, Process, Store, and Analyze data.</p> <p>Note</p> <p>This session groups any Open-Soure Tools base on Modern Data Stack concept. Some topic I found the tools from the ReStack</p> DBT Ecosystem Overview <p> Modern Data Stack advocates a lot of changes, but if not done right they can be painful, expensive, and risky. Most often when I talk to data stack owners in the enterprise, the question is not so much how one tool compares to another but whether they can live with the status quo or Have a strong enough desire to change.</p> <p> The Key Concepts:</p> <ul> <li>Easy to Try &amp; Deploy</li> <li>Massive Scalability; Data, Users, and Use-Cases</li> <li>Composable Data Stack</li> <li>Flexible Pricing</li> </ul> The Modern Open-Source Data Stack <p> Modern Data Stack allows you to build on top of the software of giants instead of adopting the \"not invented here syndrome\". Consider whether the business value from your analytics solution is coming from how well you manage your data pipelines and data infrastructure, versus how well you build analytics and AI products on top of your infrastructure.</p>"},{"location":"tools/#tools-comparison","title":"Tools Comparison","text":""},{"location":"tools/#open-table","title":"Open Table","text":"<ul> <li>Apache Iceberg vs. Delta Lake: A Comprehensive Guide for Modern Data Processing</li> </ul>"},{"location":"tools/#file-format","title":"File Format","text":"<ul> <li>Comparing Performance of Big Data File Formats: A Practical Guide</li> <li>https://medium.com/@turkelturk/data-file-formats-in-data-engineering-5ba0db8c2c16</li> <li>Compressing Your Data: A Comparison of Popular Algorithms</li> </ul>"},{"location":"tools/#data-ingestion","title":"Data Ingestion","text":"<p> Modern Data Stack: Reverse ETL</p>"},{"location":"tools/#computing","title":"Computing","text":"<ul> <li>Trino vs StarRocks</li> </ul>"},{"location":"tools/#dataframe-api","title":"Dataframe API","text":"<ul> <li>Polars, DuckDB, Pandas, Modin, Ponder, Fugue, Daft</li> </ul>"},{"location":"tools/#duckdb-vs-polars","title":"DuckDB vs Polars","text":"<p>Read More: Benchmarking Python Processing Engines: Who\u2019s the Fastest?</p>"},{"location":"tools/#data-quality","title":"Data Quality","text":"<p>https://medium.com/@brunouy/a-guide-to-open-source-data-quality-tools-in-late-2023-f9dbadbc7948</p>"},{"location":"tools/#data-orchestration","title":"Data Orchestration","text":"<p>Apache Airflow vs Mage.ai in Data Engineering</p>"},{"location":"tools/analytic/mlflow/","title":"MLflow","text":"<p>MLflow is an open-source platform for end-to-end lifecycle management of Machine Learning developed by Databricks.</p> <p>MLflow offers a variety of features, such as monitoring models in training, using an artefact store, serving models, and more. Today we will look at how to use MLflow as an orchestrator of a Machine Learning pipeline. This is because especially in the world of AI where there are various steps and experimentation it is critical to have clean, understandable and easily reproducible code.</p>"},{"location":"tools/analytic/mlflow/#references","title":"References","text":"<ul> <li>MLOps \u2014 A Gentle Introduction to Mlflow Pipelines</li> </ul>"},{"location":"tools/analytic/pinot/","title":"Apache Pinot","text":"<p>Pinot is a distributed low latency OLAP data store. It is optimized for analytical workload on immutable append-only data. It has the following features:</p> <ul> <li>column oriented modern OLAP solution</li> <li>Supports variety of indexes \u2014 inverted index, star tree index, text index, range index, forward index</li> <li>SQL like query interface</li> <li>Support for Upserts</li> <li>Near Realtime ingestion</li> </ul>"},{"location":"tools/analytic/pinot/#references","title":"References","text":"<ul> <li>Apache Pinot</li> </ul>"},{"location":"tools/collection/airbyte/","title":"Airbyte","text":""},{"location":"tools/collection/airbyte/airbyte-with-terraform/","title":"Airbyte: With Terraform","text":"<p>https://medium.com/@jeremysrgt/manage-airbyte-with-code-a-guide-to-using-the-terraform-provider-347f73e132eb</p>"},{"location":"tools/collection/dlt/","title":"DLT","text":"<p>In the vast ocean of data management, the processes of extracting data from various sources and loading it into target systems are fundamental yet critical steps for businesses aiming to leverage data for insightful decision-making. The \u201cExtract, Load, Transform\u201d (ELT or ELT) approach has been the staple approach, emphasizing the importance of efficiently moving data before applying any transformations. This blog post delves into a new and exciting Python library called Data Load Tool. It streamlines the EL operations, thereby enhancing our data integration strategy. This is a perfect fit for the Data Build Tool (dbt) as it relies on other tools for EL process.</p>"},{"location":"tools/collection/dlt/#what-is-dlt","title":"What is DLT?","text":"<p>Data Load Tool (DLT) is an open-source Python library that aims to simplify the creation and maintenance of data pipeline Extract and Load process. We can add it to Python scripts to load data from various sources into well-structured datasets.</p> <p>Following are the key advantages of dlt:</p> Python Based <p>Leverages Python to build data pipelines. Run it where Python runs.</p> Dependency Free <p>No need to use any backends or containers. Simply import dlt in a Python file.</p> Automatic Schema Management <p>With schema inference, evolution, alerts, and with short declarative code, maintenance becomes simple.</p> Declarative Syntax <p>User-friendly, declarative interface that removes knowledge obstacles. Makes code maintenance easy.</p> Compatible <p>Integrate with existing tools and services ranging from Airflow, serverless functions, notebooks and Python scripts.</p>"},{"location":"tools/collection/dlt/#references","title":"References","text":"<ul> <li>Data Load Tool (dlt): A Python library for Exract and Load (EL). Perfect fit for dbt</li> </ul>"},{"location":"tools/collection/sqlalchemy/","title":"SQLAlchemy","text":""},{"location":"tools/deploy/jenkins/","title":"Jenkins","text":""},{"location":"tools/deploy/jenkins/#examples","title":"Examples","text":"<ul> <li>The Top 10 Jenkins Pipelines Examples</li> </ul>"},{"location":"tools/deploy/jira/","title":"Jira","text":""},{"location":"tools/deploy/nginx/","title":"Nginx","text":""},{"location":"tools/etl/arrow/","title":"Apache Arrow","text":"<ul> <li>https://medium.com/gooddata-developers/how-to-build-analytics-with-apache-arrow-a773bb8eec3c</li> <li>Leveraging PyArrow for Efficient Data Processing</li> </ul>"},{"location":"tools/etl/arrow/arrow-modern-data-service/","title":"Modern Data Service","text":""},{"location":"tools/etl/arrow/arrow-modern-data-service/#references","title":"References","text":"<ul> <li>Building a Modern Data Service Layer with Apache Arrow</li> </ul>"},{"location":"tools/etl/dbt/","title":"DBT","text":"<p>DBT (Data Build Tool) is an open-source tool (Core version) used for transforming workflows, using SQL scripts (.sql)and YAML scripts (.yml).</p> <ul> <li>https://datarockie.com/blog/dbt/</li> </ul> <ul> <li>DBT for Analytic Engineer</li> <li>Dev Genius: DBT in a NutShell</li> <li>Mastering DBT from Zero to Hero</li> <li>https://medium.com/@turkelturk/best-practices-for-workflows-a-guide-to-effective-dbt-use-fa925127647c</li> </ul>"},{"location":"tools/etl/dbt/#cicd","title":"CICD","text":"<p>https://paulfry999.medium.com/v0-4-pre-chatgpt-how-to-create-ci-cd-pipelines-for-dbt-core-88e68ab506dd</p>"},{"location":"tools/etl/dbt/dbt-kimbal-data-modeling/","title":"DBT: Kimbal Data Modeling","text":""},{"location":"tools/etl/dbt/dbt-kimbal-data-modeling/#references","title":"References","text":"<ul> <li>https://docs.getdbt.com/blog/kimball-dimensional-model</li> </ul>"},{"location":"tools/etl/dbt/dbt-loom/","title":"DBT: DBT Loom","text":"<p>https://medium.astrafy.io/dbt-loom-bringing-multi-project-integration-to-dbt-core-c3c8c16d023d</p>"},{"location":"tools/etl/dbt/dbt-mesh/","title":"DBT: Mesh","text":"<p>https://medium.com/@rhelenius/exploring-dbt-mesh-data-model-contracts-6e1cd1cfcfeb</p>"},{"location":"tools/etl/dbt/dbt-migrate-to-databricks-with-lakehouse/","title":"DBT: Migrate to Databricks with Lakehouse Utils","text":"<p>https://medium.com/dbsql-sme-engineering/migrate-to-databricks-with-lakehouse-utils-dbt-package-v2-streamlined-6d52874075c9</p>"},{"location":"tools/etl/dbt/dbt-on-lake-house/","title":"DBT: On Lake House","text":"<p>https://medium.com/dbsql-sme-engineering/optimal-dbt-on-lakehouse-design-patterns-11efe702f509 https://medium.com/dbsql-sme-engineering/dbt-on-lakehouse-part-2-scd-2-with-snapshots-40134d3606bf</p>"},{"location":"tools/etl/dbt/dbt-recce/","title":"Recce","text":"<p>Recce (short for reconnaissance) is a data modeling validation toolkit with a focus on environment diffing. Take two dbt environments, such as dev and prod, and compare them using the suite of diff tools in Recce.</p> <ul> <li>(https://medium.com/inthepipeline/data-modeling-validation-toolkit-better-pr-review-open-source-recce-b6e207b6c1f2)</li> </ul>"},{"location":"tools/etl/dbt/connection/dbt-to-athena/","title":"To AWS Athena","text":"<p>DBT Athena</p> <pre><code>$ pip install dbt-athena-community\n</code></pre>"},{"location":"tools/etl/dbt/connection/dbt-to-athena/#noted","title":"Noted","text":"<ul> <li>DBT Athena - Iceberg: How to use bucketing</li> <li>DBT Athena - Iceberg: Timestamp precision (3) not supported. Use \"timestamp(6)\" instead</li> </ul>"},{"location":"tools/etl/dbt/connection/dbt-to-spark/","title":"To Spark","text":"<ul> <li>\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 dbt \u0e01\u0e31\u0e1a Apache Spark</li> </ul>"},{"location":"tools/etl/dbt/connection/dbt-to-synapse/","title":"DBT: To Synapse SQL Pool","text":"<p>https://medium.com/@sayed.cse01/integrating-azure-synapse-with-dbt-46cbfeb0dcf3</p>"},{"location":"tools/etl/dbt/connection/dbt-to-synapse/#prerequisite","title":"Prerequisite","text":"<pre><code>$ python --version\nPython 3.9.6\n$ pip install --upgrade pip\n$ pip install dbt dbt-synapse\n</code></pre> <p>Note</p> <p>For Synapse Serverless SQL Pool,</p> <p>https://github.com/dbt-labs/dbt-external-tables/blob/main/sample_sources/synapse.yml</p>"},{"location":"tools/etl/dbt/connection/dbt-to-trino/","title":"Trino","text":"<p><code>dbt-trino</code> adapter uses Trino as a underlying query engine to perform query federation across disperse data sources. Trino connects to multiple and diverse data sources (available connectors) via one dbt connection and process SQL queries at scale. Transformations defined in dbt are passed to Trino which handles these SQL transformation queries and translates them to queries specific to the systems it connects to create tables or views and manipulate data.</p>"},{"location":"tools/etl/dbt/connection/dbt-to-trino/#prerequisite","title":"Prerequisite","text":""},{"location":"tools/etl/dbt/connection/dbt-to-trino/#configuring-postgresql-and-trino","title":"Configuring postgresql and trino","text":"<p>```yaml titles=\"docker-compose.yml\" version: '3.7' services:     sales:         image: postgres:11         container_name: sales         restart: always         environment:             POSTGRES_DB: postgres             POSTGRES_USER: postgres             POSTGRES_PASSWORD: postgres         ports:             - \"15432:5432\"         volumes:             - ./postgres-data:/var/lib/postgresql/data             - ./sql/create_tables.sql:/docker-entrypoint-initdb.d/create_tables.sql             - ./sql/fill_tables.sql:/docker-entrypoint-initdb.d/fill_tables.sql     trino:         hostname: trino         container_name: trino         image: 'trinodb/trino:latest'         ports:             - '8080:8080'         volumes:             - ./catalog:/etc/trino/catalog networks:     trino-network:         driver: bridge <pre><code>Notice that we have a persisted volume inside a `catalog` folder.\nInside this folder, we need to set some configuration files to allow `trino` to\nconnect to our databases. In our case, we need to file, one for `postgres` and\nthe other for `bigquery`.\n\n=== \"Postgres\"\n\n    ```text titles=\"postgres.properties\"\n    connector.name=postgresql\n    connection-url=jdbc:postgresql://sales:5432/postgres\n    connection-user=postgres\n    connection-password=postgres\n    ```\n\n=== \"BigQuery\"\n\n    ```text titles=\"bigquery.properties\"\n    connector.name=bigquery\n    bigquery.project-id=dbt-sales\n    bigquery.views-enabled=false\n    bigquery.credentials-file=/etc/trino/catalog/dbt-sales-svc.json\n    ```\n\n```shell\npip install dbt-trino dbt-bigquery\n</code></pre></p>"},{"location":"tools/etl/dbt/connection/dbt-to-trino/#references","title":"References","text":"<ul> <li>Using dbt-trino as a data ingestion tool: Part I</li> </ul>"},{"location":"tools/etl/flink/","title":"Apache Flink","text":"<p>https://blog.devgenius.io/apache-flink-an-intro-3e2b1d0ca28a</p>"},{"location":"tools/etl/kafka/","title":"Kafka","text":"<ul> <li>Setting up Apache Kafka for local development with Docker</li> </ul>"},{"location":"tools/etl/kafka/#connection-code","title":"Connection Code","text":"<pre><code>$ pip install kafka-python\n</code></pre> ProducerConsumer <pre><code>from kafka import KafkaProducer\nimport json\n\nproducer = KafkaProducer(\n    bootstrap_servers='localhost:29092',\n    security_protocol=\"PLAINTEXT\",\n    value_serializer=lambda v: json.dumps(v).encode('ascii')\n)\n\nproducer.send(\n 'hotel-booking-request',\n value={\n     \"name\": \"Evy Lina\",\n     \"hotel\": \"Cheap Hotel\",\n     \"dateFrom\": \"14-07-2024\",\n     \"dateTo\": \"01-08-2021\",\n     \"details\": \"Wish coffee ready \ud83d\ude00\"\n     }\n)\nproducer.flush()\n</code></pre> <pre><code>from kafka import KafkaConsumer\nimport json\n\nconsumer = KafkaConsumer(\n    bootstrap_servers='localhost:29092',\n    security_protocol=\"PLAINTEXT\",\n    value_deserializer=lambda v: json.loads(v.decode('ascii')),\n    # auto_offset_reset='earliest'\n)\n\nconsumer.subscribe(topics='hotel-booking-request')\n\nfor message in consumer:\n    print(f\"{message.partition}:{message.offset} v={message.value}\")\n</code></pre> <ul> <li>Write your first Kafka producer and Kafka consumer in Python</li> </ul>"},{"location":"tools/etl/kafka/kafka-agoda/","title":"Kafka: Agoda","text":"<p>https://medium.com/agoda-engineering/how-agoda-manages-1-8-trillion-events-per-day-on-kafka-1d6c3f4a7ad1</p>"},{"location":"tools/etl/pandas/","title":"Pandas","text":"<p>Pandas for Data Engineers</p> <pre><code>def etl(cursor):\n    cursor.execute(\"&lt;query&gt;\")\n    for row in cursor.fetchall():\n        yield row\n\ndef df_generator(cursor):\n    print('Creating pandas DF using generator...')\n    column_names = ['id',\n                'merchant',\n                'status',\n                'transaction_date',\n                'amount_usd']\n\n    df = pd.DataFrame(data = etl(cursor), columns=column_names)\n    print('DF successfully created!\\n')\n    return df\n\ndef df_create_from_batch(cursor, batch_size):\n    print('Creating pandas DF using generator...')\n    colnames = ['id',\n               'merchant',\n               'status',\n               'transaction_date',\n               'amount_usd']\n\n    df = pd.DataFrame(columns=colnames)\n    # execute a database query to extract data\n    cursor.execute(query)\n    while True:\n        rows = cursor.fetchmany(batch_size)\n        if not rows:\n            break\n        # some ETL on a chunk of data of batch_size\n        batch_df = pd.DataFrame(data = rows, columns=colnames)\n        df = pd.concat([df, batch_df], ignore_index=True)\n    print('DF successfully created!\\n')\n    return df\n</code></pre> <pre><code>import boto3\nimport json\nfrom datetime import datetime\nimport pytz\ns3 = boto3.client('s3')\n\n\ndef upload_chunks(chunk_gen, s3_bucket, s3_file_prefix):\n    \"\"\"Perform Multi-part upload to AWS S3 datalake\"\"\"\n    try:\n        cnt = 0\n        logs = []\n        for chunk in chunk_gen:\n            part = bytes(json.dumps(chunk), encoding='utf8')\n            key = s3_file_prefix + file_key()\n            s3.put_object(Body=part, Bucket=s3_bucket, Key=key)\n            logs.append(f'aws s3 cp s3://{s3_bucket}/{key} ./ ')\n            cnt += 1\n\n        print(f'upload_chunks: Uploaded {cnt} chunks.')\n        print('\\n'.join(str(i) for i in logs))\n    except Exception as e:\n        print(e)\n\ndef file_key():\n    \"\"\"Get a file suffix, i.e. /data_pipe_1/2023/12/11/09/5234023930\"\"\"\n    suffix = datetime.utcnow().replace(tzinfo=pytz.utc).strftime('%Y/%m/%d/%H/%M%S%f')\n    return f'{suffix}'\n\ndef df_create_from_batch(cursor, batch_size):\n     print('Creating pandas DF using generator...')\n     colnames = ['id',\n                'merchant',\n                'status',\n                'transaction_date',\n                'amount_usd']\n\n     df = pd.DataFrame(columns=colnames)\n     # execute a database query to extract data\n     cursor.execute(query)\n     while True:\n         rows = cursor.fetchmany(batch_size)\n         if not rows:\n             break\n         # some ETL on a chunk of data of batch_size\n         batch_df = pd.DataFrame(data = rows, columns=colnames)\n         yield batch_df\n     print('DF successfully created!\\n')\n     return df\n\ns3_upload_scope = df_create_from_batch(cursor, 10000)\nupload_chunks(s3_upload_scope, config.get('s3_bucket'), pipe['name'])\n</code></pre> <pre><code>def chunk_gen(itemList, chunks):\n    '''Read data in chunks and yield each chunk'''\n    for i in range(0, len(itemList), chunks):\n        yield itemList[i:i + chunks]\n\ndef sb_batch_extract(idList_gen):\n    '''Reads data generator, i.e. list of ids, and works with each batch\n    to extract data from database\n    '''\n    try:\n        step = 1\n        while True:\n            ids = next(idList_gen)\n            logging.debug(f'&gt; Step {step} processing ids:  {ids})')\n            ids_str = ','.join(str(i) for i in ids)\n            out = get_sb_data(sql, ids_str)\n            logging.debug(f'&gt; Step {step} ids used to produce : {out}')\n            step += 1\n            yield out\n    except Exception as e:\n        print(e)\n    except StopIteration:\n        pass\n    finally:\n        del idList_gen\n\n#  Step 1: Generate user id list as generator object.\nidList_gen = chunk_gen([col[0] for col in get_ids()], 250)\n# Step 2: Extract in chunks from database:\nextract = sb_batch_extract(idList_gen)\n\nactual = [i for i in batch_extract_demo(idList_gen)]\n</code></pre>"},{"location":"tools/etl/pandas/#practices","title":"Practices","text":"<ul> <li> Don't use <code>loc</code>/<code>iloc</code> with Loops, Instead, Use This!</li> </ul>"},{"location":"tools/etl/pandas/py_pd_convert_from_sql/","title":"Pandas: Convert from SQL","text":""},{"location":"tools/etl/pandas/py_pd_convert_from_sql/#references","title":"References","text":"<ul> <li> TowardDS - Pandas equivalent of 10 useful SQL queries</li> </ul>"},{"location":"tools/etl/polars/","title":"Polars","text":"<p>https://brilliantprogrammer.medium.com/everything-you-should-know-to-get-started-with-polars-as-a-data-engineer-b6ffa999da39 https://python.plainenglish.io/say-goodbye-to-pandas-introducing-polars-fabbd4a28b6a</p>"},{"location":"tools/etl/polars/#use-cases","title":"Use-Cases","text":"<ul> <li> Process Hundreds of GB of Data in the Cloud</li> </ul>"},{"location":"tools/etl/polars/polars-data-pipeline/","title":"Polars: Data Pipeline","text":"<p>Polars way of working with data lends itself quite nicely to building scalable data pipelines. First of all, the fact that we can chain the methods so easily allows for some fairly complicated pipelines to be written quite elegantly.</p> <p>Notice that these functions take a Polars DataFrame as input (together with some other arguments) and output already altered Polars Data Frame. Chaining these methods together is a piece of cake with <code>.pipe()</code>.</p> <pre><code>import poloars as pl\n\ndef process_date(df, date_column, format):\n    result = df.with_columns(pl.col(date_column).str.to_date(format))\n    return result\n\n\ndef filter_year(df, date_column, year):\n    result = df.filter(pl.col(date_column).dt.year() == year)\n    return result\n\n\ndef get_first_by_month(df, date_column, metric):\n    result = df.with_columns(\n        pl.col(metric)\n        .rank(method=\"ordinal\", descending=True)\n        .over(pl.col(date_column).dt.month())\n        .alias(\"rank\")\n    ).filter(pl.col(\"rank\") == 1)\n\n    return result\n\ndef select_data_to_write(df, columns):\n    result = df.select([pl.col(c) for c in columns])\n    return result\n</code></pre> NormalLazy <pre><code>(\n    pl.read_csv('folder/file.csv')\n        .pipe(process_date, date_column=\"trending_date\", format=\"%y.%d.%m\")\n        .pipe(filter_year, date_column=\"trending_date\", year=2018)\n        .pipe(get_first_by_month, date_column=\"trending_date\", metric=\"views\")\n        .pipe(\n            select_data_to_write,\n            columns=[\"trending_date\", \"title\", \"channel_title\", \"views\"],\n        )\n).write_parquet(\"top_monthly_videos.parquet\")\n</code></pre> <pre><code>(\n    pl.scan_csv('folder/file.csv')\n        .pipe(process_date, date_column=\"trending_date\", format=\"%y.%d.%m\")\n        .pipe(filter_year, date_column=\"trending_date\", year=2018)\n        .pipe(get_first_by_month, date_column=\"trending_date\", metric=\"views\")\n        .pipe(\n            select_data_to_write,\n            columns=[\"trending_date\", \"title\", \"channel_title\", \"views\"],\n        )\n).collect().write_parquet(\"top_monthly_videos.parquet\")\n</code></pre>"},{"location":"tools/etl/polars/polars-data-pipeline/#example-machine-learning-features","title":"Example: Machine Learning Features","text":"ReadingData CleaningBasic FeatureData TransformAdvanced AggPeriod Agg <pre><code>data_path: \"./youtube/videos.csv\"\ncategory_map_path: \"./youtube/category_id.json\"\n</code></pre> <pre><code>def read_category_mappings(path: str) -&gt; Dict[int, str]:\n    with open(path, \"r\") as f:\n        categories = json.load(f)\n\n    id_to_category = {}\n    for c in categories[\"items\"]:\n        id_to_category[int(c[\"id\"])] = c[\"snippet\"][\"title\"]\n\n    return id_to_category\n</code></pre> <pre><code># Pre-processing config\ndate_column_format:\n  trending_date: \"%y.%d.%m\"\n  publish_time: \"%Y-%m-%dT%H:%M:%S%.fZ\"\n</code></pre> <pre><code>def parse_dates(date_cols: Dict[str, str]) -&gt; List[pl.Expr]:\n    expressions = []\n    for date_col, fmt in date_cols.items():\n        expressions.append(pl.col(date_col).str.to_date(format=fmt))\n\n    return expressions\n\ndef map_dict_columns(\n    mapping_cols: Dict[str, Dict[str | int, str | int]]\n) -&gt; List[pl.Expr]:\n    expressions = []\n    for col, mapping in mapping_cols.items():\n        expressions.append(pl.col(col).map_dict(mapping))\n    return expressions\n\ndef clean_data(\n    df: pl.DataFrame,\n    date_cols_config: Dict[str, str],\n    mapping_cols_config: Dict[str, Dict[str | int, str | int]],\n) -&gt; pl.DataFrame:\n    parse_dates_expressions = parse_dates(date_cols=date_cols_config)\n    mapping_expressions = map_dict_columns(mapping_cols_config)\n\n    df = df.with_columns(parse_dates_expressions + mapping_expressions)\n    return df\n</code></pre> <pre><code># Feature engineering config\nratio_features:\n  # feature name\n  likes_to_dislikes:\n    # features used in calculation\n    - likes\n    - dislikes\n  likes_to_views:\n    - likes\n    - views\n  comments_to_views:\n    - comment_count\n    - views\n\ndifference_features:\n  days_to_trending:\n    - trending_date\n    - publish_time\n\ndate_features:\n  trending_date:\n    - weekday\n</code></pre> <pre><code>def ratio_features(features_config: Dict[str, List[str]]) -&gt; List[pl.Expr]:\n    expressions = []\n    for name, cols in features_config.items():\n        expressions.append((pl.col(cols[0]) / pl.col(cols[1])).alias(name))\n\n    return expressions\n\ndef diff_features(features_config: Dict[str, List[str]]) -&gt; List[pl.Expr]:\n    expressions = []\n    for name, cols in features_config.items():\n        expressions.append((pl.col(cols[0]) - pl.col(cols[1])).alias(name))\n\n    return expressions\n\ndef date_features(features_config: Dict[str, List[str]]) -&gt; List[pl.Expr]:\n    expressions = []\n    for col, features in features_config.items():\n        if \"weekday\" in features:\n            expressions.append(pl.col(col).dt.weekday().alias(f\"{col}_weekday\"))\n        if \"month\" in features:\n            expressions.append(pl.col(col).dt.month().alias(f\"{col}_month\"))\n        if \"year\" in features:\n            expressions.append(pl.col(col).dt.year().alias(f\"{col}_year\"))\n\n    return expressions\n\ndef basic_feature_engineering(\n    data: pl.DataFrame,\n    ratios_config: Dict[str, List[str]],\n    diffs_config: Dict[str, List[str]],\n    dates_config: Dict[str, List[str]],\n) -&gt; pl.DataFrame:\n    ratio_expressions = ratio_features(ratios_config)\n    date_diff_expressions = diff_features(diffs_config)\n    date_expressions = date_features(dates_config)\n\n    data = data.with_columns(\n        ratio_expressions + date_diff_expressions + date_expressions\n    )\n    return data\n</code></pre> <pre><code># Filter videos\nmax_time_to_trending: 60\n\n# Features to join to the transformed data\nbase_columns:\n  - views\n  - likes\n  - dislikes\n  - comment_count\n  - comments_disabled\n  - ratings_disabled\n  - video_error_or_removed\n  - likes_to_dislikes\n  - likes_to_views\n  - comments_to_views\n  - trending_date_weekday\n  - channel_title\n  - tags\n  - description\n  - category_id\n\n# Use these columns to join transformed data with original\njoin_columns:\n  - video_id\n  - trending_date\n</code></pre> <pre><code>def join_original_features(\n    main: pl.DataFrame,\n    original: pl.DataFrame,\n    main_join_cols: List[str],\n    original_join_cols: List[str],\n    other_cols: List[str],\n) -&gt; pl.DataFrame:\n    original_features = original.select(original_join_cols + other_cols).unique(\n        original_join_cols\n    )  # unique ensures one row per video + date\n    main = main.join(\n        original_features,\n        left_on=main_join_cols,\n        right_on=original_join_cols,\n        how=\"left\",\n    )\n    return main\n\ndef create_target_df(\n    df: pl.DataFrame,\n    time_to_trending_thr: int,\n    original_join_cols: List[str],\n    other_cols: List[str],\n) -&gt; pl.DataFrame:\n    # Create a DF with video ID per row and corresponding days to trending and days in trending (target)\n    target = (\n        df.groupby([\"video_id\"])\n        .agg(\n            pl.col(\"days_to_trending\").min().dt.days(),\n            pl.col(\"trending_date\").min().dt.date().alias(\"first_day_in_trending\"),\n            pl.col(\"trending_date\").max().dt.date().alias(\"last_day_in_trending\"),\n            # our TARGET\n            (pl.col(\"trending_date\").max() - pl.col(\"trending_date\").min()).dt.days().alias(\"days_in_trending\"),\n        )\n        .filter(pl.col(\"days_to_trending\") &lt;= time_to_trending_thr)\n    )\n\n    # Join features to the aggregates\n    target = join_original_features(\n        main=target,\n        original=df,\n        main_join_cols=[\"video_id\", \"first_day_in_trending\"],\n        original_join_cols=original_join_cols,\n        other_cols=other_cols,\n    )\n    return target\n</code></pre> <pre><code>aggregate_windows:\n  - 7\n  - 30\n  - 180\n</code></pre> <pre><code>def build_channel_rolling(df: pl.DataFrame, date_col: str, period: int) -&gt; pl.DataFrame:\n    channel_aggs = (\n        df.sort(date_col)\n        .groupby_rolling(\n            index_column=date_col,\n            period=f\"{period}d\",\n            by=\"channel_title\",\n            closed=\"left\",  # only left to not include the actual day\n        )\n        .agg(\n            pl.col(\"video_id\").n_unique().alias(f\"channel_num_trending_videos_last_{period}_days\"),\n            pl.col(\"days_in_trending\").max().alias(f\"channel_max_days_in_trending_{period}_days\"),\n            pl.col(\"days_in_trending\").mean().alias(f\"channel_avg_days_in_trending_{period}_days\"),\n        )\n        .fill_null(0)\n    )\n    return channel_aggs\n\ndef add_rolling_features(\n    df: pl.DataFrame, date_col: str, periods: List[int]\n) -&gt; pl.DataFrame:\n    for period in periods:\n        rolling_features = build_channel_rolling(df, date_col, period)\n        df = df.join(rolling_features, on=[\"channel_title\", \"first_day_in_trending\"])\n    return df\n</code></pre> <pre><code>aggregate_windows:\n  - 7\n  - 30\n  - 180\n</code></pre> <pre><code>def build_period_features(df: pl.DataFrame, date_col: str, period: int) -&gt; pl.DataFrame:\n    general_aggs = (\n        df.sort(date_col)\n        .groupby_dynamic(\n            index_column=date_col,\n            every=\"1d\",\n            period=f\"{period}d\",\n            closed=\"left\",\n        )\n        .agg(\n            pl.col(\"video_id\").n_unique().alias(f\"general_num_trending_videos_last_{period}_days\"),\n            pl.col(\"days_in_trending\").max().alias(f\"general_max_days_in_trending_{period}_days\"),\n            pl.col(\"days_in_trending\").mean().alias(f\"general_avg_days_in_trending_{period}_days\"),\n        )\n        .with_columns(\n            # shift match values with previous period\n            pl.col(f\"general_num_trending_videos_last_{period}_days\").shift(period),\n            pl.col(f\"general_max_days_in_trending_{period}_days\").shift(period),\n            pl.col(f\"general_avg_days_in_trending_{period}_days\").shift(period),\n        )\n        .fill_null(0)\n    )\n    return general_aggs\n\ndef add_period_features(\n    df: pl.DataFrame, date_col: str, periods: List[int]\n) -&gt; pl.DataFrame:\n    for period in periods:\n        rolling_features = build_period_features(df, date_col, period)\n        df = df.join(rolling_features, on=[\"first_day_in_trending\"])\n    return df\n</code></pre> Full-examples pipe_config.yaml<pre><code>data_path: \"./youtube/videos.csv\"\ncategory_map_path: \"./youtube/category_id.json\"\n\ndate_column_format:\n  trending_date: \"%y.%d.%m\"\n  publish_time: \"%Y-%m-%dT%H:%M:%S%.fZ\"\n\n# Feature engineering config\nratio_features:\n  # feature name\n  likes_to_dislikes:\n    # features used in calculation\n    - likes\n    - dislikes\n  likes_to_views:\n    - likes\n    - views\n  comments_to_views:\n    - comment_count\n    - views\n\ndifference_features:\n  days_to_trending:\n    - trending_date\n    - publish_time\n\ndate_features:\n  trending_date:\n    - weekday\n\n# Filter videos\nmax_time_to_trending: 60\n\n# Features to join to the transformed data\nbase_columns:\n  - views\n  - likes\n  - dislikes\n  - comment_count\n  - comments_disabled\n  - ratings_disabled\n  - video_error_or_removed\n  - likes_to_dislikes\n  - likes_to_views\n  - comments_to_views\n  - trending_date_weekday\n  - channel_title\n  - tags\n  - description\n  - category_id\n\n# Use these columns to join transformed data with original\njoin_columns:\n  - video_id\n  - trending_date\n\naggregate_windows:\n  - 7\n  - 30\n  - 180\n</code></pre> <pre><code>import time\nimport json\nfrom typing import Dict, List\n\nimport yaml\nimport polars as pl\n\n\ndef read_category_mappings(path: str) -&gt; Dict[int, str]:\n    with open(path, \"r\") as f:\n        categories = json.load(f)\n\n    id_to_category = {}\n    for c in categories[\"items\"]:\n        id_to_category[int(c[\"id\"])] = c[\"snippet\"][\"title\"]\n\n    return id_to_category\n\n\ndef parse_dates(date_cols: Dict[str, str]) -&gt; List[pl.Expr]:\n    expressions = []\n    for date_col, fmt in date_cols.items():\n        expressions.append(pl.col(date_col).str.to_date(format=fmt))\n\n    return expressions\n\n\ndef map_dict_columns(\n    mapping_cols: Dict[str, Dict[str | int, str | int]]\n) -&gt; List[pl.Expr]:\n    expressions = []\n    for col, mapping in mapping_cols.items():\n        expressions.append(pl.col(col).map_dict(mapping))\n    return expressions\n\n\ndef clean_data(\n    df: pl.DataFrame,\n    date_cols_config: Dict[str, str],\n    mapping_cols_config: Dict[str, Dict[str | int, str | int]],\n) -&gt; pl.DataFrame:\n    parse_dates_expressions = parse_dates(date_cols=date_cols_config)\n    mapping_expressions = map_dict_columns(mapping_cols_config)\n\n    df = df.with_columns(parse_dates_expressions + mapping_expressions)\n    return df\n\ndef ratio_features(features_config: Dict[str, List[str]]) -&gt; List[pl.Expr]:\n    expressions = []\n    for name, cols in features_config.items():\n        expressions.append((pl.col(cols[0]) / pl.col(cols[1])).alias(name))\n\n    return expressions\n\n\ndef diff_features(features_config: Dict[str, List[str]]) -&gt; List[pl.Expr]:\n    expressions = []\n    for name, cols in features_config.items():\n        expressions.append((pl.col(cols[0]) - pl.col(cols[1])).alias(name))\n\n    return expressions\n\ndef date_features(features_config: Dict[str, List[str]]) -&gt; List[pl.Expr]:\n    expressions = []\n    for col, features in features_config.items():\n        if \"weekday\" in features:\n            expressions.append(pl.col(col).dt.weekday().alias(f\"{col}_weekday\"))\n        if \"month\" in features:\n            expressions.append(pl.col(col).dt.month().alias(f\"{col}_month\"))\n        if \"year\" in features:\n            expressions.append(pl.col(col).dt.year().alias(f\"{col}_year\"))\n\n    return expressions\n\ndef basic_feature_engineering(\n    data: pl.DataFrame,\n    ratios_config: Dict[str, List[str]],\n    diffs_config: Dict[str, List[str]],\n    dates_config: Dict[str, List[str]],\n) -&gt; pl.DataFrame:\n    ratio_expressions = ratio_features(ratios_config)\n    date_diff_expressions = diff_features(diffs_config)\n    date_expressions = date_features(dates_config)\n\n    data = data.with_columns(\n        ratio_expressions + date_diff_expressions + date_expressions\n    )\n    return data\n\n\ndef join_original_features(\n    main: pl.DataFrame,\n    original: pl.DataFrame,\n    main_join_cols: List[str],\n    original_join_cols: List[str],\n    other_cols: List[str],\n) -&gt; pl.DataFrame:\n    original_features = original.select(original_join_cols + other_cols).unique(\n        original_join_cols\n    )  # unique ensures one row per video + date\n    main = main.join(\n        original_features,\n        left_on=main_join_cols,\n        right_on=original_join_cols,\n        how=\"left\",\n    )\n\n    return main\n\n\ndef create_target_df(\n    df: pl.DataFrame,\n    time_to_trending_thr: int,\n    original_join_cols: List[str],\n    other_cols: List[str],\n) -&gt; pl.DataFrame:\n    # Create a DF with video ID per row and corresponding days to trending and days in trending (target)\n    target = (\n        df.groupby([\"video_id\"])\n        .agg(\n            pl.col(\"days_to_trending\").min().dt.days(),\n            pl.col(\"trending_date\").min().dt.date().alias(\"first_day_in_trending\"),\n            pl.col(\"trending_date\").max().dt.date().alias(\"last_day_in_trending\"),\n            # our TARGET\n            (pl.col(\"trending_date\").max() - pl.col(\"trending_date\").min()).dt.days().alias(\"days_in_trending\"),\n        )\n        .filter(pl.col(\"days_to_trending\") &lt;= time_to_trending_thr)\n    )\n\n    # Join features to the aggregates\n    target = join_original_features(\n        main=target,\n        original=df,\n        main_join_cols=[\"video_id\", \"first_day_in_trending\"],\n        original_join_cols=original_join_cols,\n        other_cols=other_cols,\n    )\n\n    return target\n\n\ndef build_channel_rolling(df: pl.DataFrame, date_col: str, period: int) -&gt; pl.DataFrame:\n    channel_aggs = (\n        df.sort(date_col)\n        .groupby_rolling(\n            index_column=date_col,\n            period=f\"{period}d\",\n            by=\"channel_title\",\n            closed=\"left\",  # only left to not include the actual day\n        )\n        .agg(\n            pl.col(\"video_id\").n_unique().alias(f\"channel_num_trending_videos_last_{period}_days\"),\n            pl.col(\"days_in_trending\").max().alias(f\"channel_max_days_in_trending_{period}_days\"),\n            pl.col(\"days_in_trending\").mean().alias(f\"channel_avg_days_in_trending_{period}_days\"),\n        )\n        .fill_null(0)\n    )\n\n    return channel_aggs\n\n\ndef add_rolling_features(\n    df: pl.DataFrame, date_col: str, periods: List[int]\n) -&gt; pl.DataFrame:\n    for period in periods:\n        rolling_features = build_channel_rolling(df, date_col, period)\n        df = df.join(rolling_features, on=[\"channel_title\", \"first_day_in_trending\"])\n\n    return df\n\n\ndef build_period_features(df: pl.DataFrame, date_col: str, period: int) -&gt; pl.DataFrame:\n    general_aggs = (\n        df.sort(date_col)\n        .groupby_dynamic(\n            index_column=date_col,\n            every=\"1d\",\n            period=f\"{period}d\",\n            closed=\"left\",\n        )\n        .agg(\n            pl.col(\"video_id\").n_unique().alias(f\"general_num_trending_videos_last_{period}_days\"),\n            pl.col(\"days_in_trending\").max().alias(f\"general_max_days_in_trending_{period}_days\"),\n            pl.col(\"days_in_trending\").mean().alias(f\"general_avg_days_in_trending_{period}_days\"),\n        )\n        .with_columns(\n            # shift match values with previous period\n            pl.col(f\"general_num_trending_videos_last_{period}_days\").shift(period),\n            pl.col(f\"general_max_days_in_trending_{period}_days\").shift(period),\n            pl.col(f\"general_avg_days_in_trending_{period}_days\").shift(period),\n        )\n        .fill_null(0)\n    )\n\n    return general_aggs\n\n\ndef add_period_features(\n    df: pl.DataFrame, date_col: str, periods: List[int]\n) -&gt; pl.DataFrame:\n    for period in periods:\n        rolling_features = build_period_features(df, date_col, period)\n        df = df.join(rolling_features, on=[\"first_day_in_trending\"])\n\n    return df\n</code></pre> <pre><code>def pipeline():\n    \"\"\"Pipeline that reads, cleans, and transofrms data into\n    the format we need for modelling\n    \"\"\"\n    # Read and unwrap the config\n    with open(\"pipe_config.yaml\", \"r\") as file:\n        pipe_config = yaml.safe_load(file)\n\n    date_column_format = pipe_config[\"date_column_format\"]\n    ratios_config = pipe_config[\"ratio_features\"]\n    diffs_config = pipe_config[\"difference_features\"]\n    dates_config = pipe_config[\"date_features\"]\n\n    id_to_category = read_category_mappings(pipe_config[\"category_map_path\"])\n    col_mappings = {\"category_id\": id_to_category}\n\n    output_data = (\n        pl.scan_csv(pipe_config[\"data_path\"])\n        .pipe(clean_data, date_column_format, col_mappings)\n        .pipe(basic_feature_engineering, ratios_config, diffs_config, dates_config)\n        .pipe(\n            create_target_df,\n            time_to_trending_thr=pipe_config[\"max_time_to_trending\"],\n            original_join_cols=pipe_config[\"join_columns\"],\n            other_cols=pipe_config[\"base_columns\"],\n        )\n        .pipe(\n            add_rolling_features,\n            \"first_day_in_trending\",\n            pipe_config[\"aggregate_windows\"],\n        )\n        .pipe(\n            add_period_features,\n            \"first_day_in_trending\",\n            pipe_config[\"aggregate_windows\"],\n        )\n    ).collect()\n\n    return output_data\n\n\nif __name__ == \"__main__\":\n    t0 = time.time()\n    output = pipeline()\n    t1 = time.time()\n    print(\"Pipeline took\", t1 - t0, \"seconds\")\n    print(\"Output shape\", output.shape)\n    print(\"Output columns:\", output.columns)\n    output.write_parquet(\"./data/modelling_data.parquet\")\n</code></pre>"},{"location":"tools/etl/polars/polars-data-pipeline/#noted","title":"Noted","text":"<p>Make sure to apply these learnings to your own data. I recommend starting small (2\u20133 steps) and then expanding the pipeline as your needs grow. Make sure to keep it modular, lazy, and group as many operations into <code>.with_columns()</code> as possible to ensure proper parallelization.</p>"},{"location":"tools/etl/polars/polars-data-pipeline/#references","title":"References","text":"<ul> <li> TowardDS - Data Pipelines with Polars: Step-by-Step Guide</li> </ul>"},{"location":"tools/etl/polars/polars-scd2-on-delta-lake/","title":"Polars: SCD2 on Delta Lake","text":"<p>https://medium.com/@jimmy-jensen/implementing-a-type-2-slowly-changing-dimension-with-polars-and-delta-lake-4a633fb28195</p>"},{"location":"tools/etl/polars/polars-with-delta-lake/","title":"Polars: With Delta Lake","text":"<ul> <li>Polars, Partitions and Performance with Delta Lake</li> </ul>"},{"location":"tools/etl/polars/connection/polars-to-synapse/","title":"To Azure Synapse Dedicate SQL Pool","text":""},{"location":"tools/etl/spark/","title":"Apache Spark","text":"<p>Quote</p> <p>Apache Spark is a unified computing engine and a set of libraries for parallel data processing on computer cluster</p> <p>The Essential PySpark Cheat Sheet for Success!</p>"},{"location":"tools/etl/spark/#architecture","title":"Architecture","text":"<ul> <li>Understanding Apache Spark Architecture</li> <li>A Deep Dive into Apache Spark Architecture</li> <li>https://medium.com/@think-data/this-level-of-detail-in-spark-is-tackled-only-by-experts-2-975cfb41af50</li> <li>Partitioning &amp; Bucketing</li> <li> <p>How does Adaptive Query Execution fix your Spark performance issues</p> </li> <li> <p>Apache Spark for Dummies: Part 4 \u2014 Advanced Spark Features</p> </li> </ul>"},{"location":"tools/etl/spark/#spark-context-vs-spark-session","title":"Spark Context vs Spark Session","text":"<p>Quote</p> <p>Spark Session is a unified entry point of a spark application from Spark 2.0</p> Spark SessionSpark Context <p>Stop current spark session:</p> <pre><code>spark.sparkContext.stop()\n</code></pre> <pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession\n        .builder\n        .appName(\"YourAppName\")\n        .master(\"local\")\n        .config(\"park.executor.memory\", \"2g\")\n        .config(\"spark.executor.cores\", 4)\n        .enableHiveSupport()  # Default be True\n        .getOrCreate()\n)\n</code></pre> <p>Get default config:</p> <pre><code>spark.sparkContext._conf.getAll()\n</code></pre> <pre><code>from pyspark.context import SparkContext\nfrom pyspark import SparkConf\n\nSparkContext.stop(sc)\n\nsc = (\n    SparkContext(\n        \"local\",\n        \"YourAppName\",\n        conf=(\n            SparkConf()\n                .set(\"spark.executor.memory\", \"2g\")\n                .set(\"spark.executor.cores\", \"4\")\n        )\n    )\n    .getOrCreate()\n)\n</code></pre>"},{"location":"tools/etl/spark/#spark-api","title":"Spark API","text":"<p>Spark has two API types</p> <ul> <li>Low-level API (Unstructured)</li> <li>High-level API (Spark\u2019s Structured API)</li> </ul> <p></p>"},{"location":"tools/etl/spark/#low-level-api","title":"Low-level API","text":"<p>Resilient Distributed Datasets (RDDs) is collection of elements partitioned that distributes to each node in cluster and work parallel.</p> <pre><code>rdd = spark.read.csv(\"file.csv\", header=True).rdd\n</code></pre> <p>RDDs support 2 operations:</p> <ul> <li>Transformations \u2014 create new rdd but lazy meaning they don't execute until you   call an action on RDD.</li> </ul> <p>Some transformations on RDDs are <code>flatMap()</code>, <code>map()</code>, <code>reduceByKey()</code>, <code>filter()</code>, <code>sortByKey()</code>   and all these return a new RDD instead of updating the current.</p> <ul> <li>Actions \u2014 return value to driver program after compute dataset finish.</li> </ul> <p>Some actions on RDDs are <code>count()</code>, <code>collect()</code>, <code>first()</code>, <code>max()</code>, <code>reduce()</code> and more.</p>"},{"location":"tools/etl/spark/#high-level-api","title":"High-level API","text":"<p>Structured API is tool for manipulate all sorts of data such as Unstructured, Semi-Structured, Structured data</p> <p>Structured API able to use with batch and streaming computation that is Spark SQL, Dataframes, Datasets API but for streaming, it be Spark Structured Streaming.</p> <p></p>"},{"location":"tools/etl/spark/#execution","title":"Execution","text":"<p>https://blog.stackademic.com/apache-spark-101-understanding-spark-code-execution-cbff49cb85ac</p>"},{"location":"tools/etl/spark/#practices","title":"Practices","text":"<ul> <li>https://medium.com/@uzzaman.ahmed/list/pyspark-sql-basics101-d80bd574842d</li> </ul>"},{"location":"tools/etl/spark/#most-common-use-cases","title":"Most Common Use Cases","text":"<ul> <li>https://towardsdatascience.com/fetch-failed-exception-in-apache-spark-decrypting-the-most-common-causes-b8dff21075c</li> <li>https://medium.com/art-of-data-engineering/distinct-and-dropduplicates-in-spark-real-project-example-9007954b49af</li> <li>https://medium.com/@vishalbarvaliya/coalesce-vs-repartition-58b12a0f0a3d</li> <li>https://medium.com/@vishalbarvaliya/apache-sparks-reducebykey-and-reduce-transformations-42b3bd80e32e</li> </ul>"},{"location":"tools/etl/spark/#interview-questions","title":"Interview Questions","text":"<ul> <li>https://blog.devgenius.io/spark-interview-questions-ii-120e1621be9a</li> <li>https://blog.devgenius.io/spark-interview-questions-x-843a24cb703a</li> <li>https://gsanjeewa1111.medium.com/pyspark-facts-b83366842ddf</li> <li>Top 25 PySpark Interview Questions and Answers (2023)</li> </ul>"},{"location":"tools/etl/spark/#optimization","title":"Optimization","text":"<ul> <li>https://medium.com/plumbersofdatascience/7-key-strategies-to-optimize-your-spark-applications-948e7df607b</li> <li>PySpark Tips</li> <li>4 Examples to Take Your PySpark Skills to Next Level</li> </ul>"},{"location":"tools/etl/spark/spark-aggregate/","title":"Aggregate","text":"<p>Apache Spark provides two primary methods for performing aggregations: Sort-based aggregation and Hash-based aggregation. These methods are optimized for different scenarios and have distinct performance characteristics.</p> <p></p>"},{"location":"tools/etl/spark/spark-aggregate/#references","title":"References","text":"<ul> <li>Apache Spark Aggregation Methods: Hash-based Vs. Sort-based</li> </ul>"},{"location":"tools/etl/spark/spark-bucketing/","title":"Spark: Bucketing","text":""},{"location":"tools/etl/spark/spark-bucketing/#what-is-bucketing","title":"What is bucketing?","text":"<p>Let\u2019s start with this simple question. Bucketing in Spark is a way how to organize data in the storage system in a particular way so it can be leveraged in subsequent queries which can become more efficient. This efficiency improvement is specifically related to avoiding the shuffle in queries with joins and aggregations if the bucketing is designed well.</p> <p>Queries with sort-merge join or shuffle-hash join and aggregations or window functions require the data to be repartitioned by the joining/grouping keys. More specifically, all rows that have the same value of the joining/grouping key must be in the same partition. To satisfy this requirement Spark has to repartition the data, and to achieve that, Spark has to physically move the data from one executor to another \u2014 Spark has to do a so-called shuffle (for more details about the logic that Spark uses to determine if a shuffle is necessary, see also my other article closely related to this topic).</p> <p>With bucketing, we can shuffle the data in advance and save it in this pre-shuffled state. After reading the data back from the storage system, Spark will be aware of this distribution and will not have to shuffle it again.</p>"},{"location":"tools/etl/spark/spark-bucketing/#how-to-make-the-data-bucketed","title":"How to make the data bucketed","text":"<p>In Spark API there is a function <code>bucketBy</code> that can be used for this purpose:</p> <pre><code>(\n    df.write\n        .mode(\"append/overwrite\")\n        .bucketBy(\"&lt;number&gt;\", \"&lt;field1&gt;\", \"&lt;field2&gt;\", ...)\n        .sortBy(\"&lt;field1&gt;\", \"&lt;field2&gt;\", ...)\n        .option(\"path\", \"&lt;output-path&gt;\")\n        .saveAsTable(\"&lt;table-name&gt;\")\n)\n</code></pre> <p>There are four points worth mentioning here:</p> <ul> <li> <p>We need to save the data as a table (a simple save function is not sufficient)   because the information about the bucketing needs to be saved somewhere. Calling   saveAsTable will make sure the metadata is saved in the metastore   (if the Hive metastore is correctly set up) and Spark can pick the information   from there when the table is accessed.</p> </li> <li> <p>Together with bucketBy, we can call also sortBy, this will sort each bucket by the specified fields. Calling sortBy is optional, bucketing will work also without the sorting. The other way around is not working though \u2014 you can not call sortBy if you don\u2019t call bucketBy as well.</p> </li> <li> <p>The first argument of the bucketBy is the number of buckets that should be created. Choosing the correct number might be tricky and it is good to consider the overall size of the dataset and the number and size of the created files (see more detailed discussion below).</p> </li> <li> <p>Careless usage of the bucketBy function can lead to the creation of too many files and custom repartition of the DataFrame might be needed before the actual write \u2014 see more info about this issue below (in the Bucketing from a data engineer perspective section).</p> </li> </ul>"},{"location":"tools/etl/spark/spark-bucketing/#how-is-the-data-distributed-among-buckets","title":"How is the data distributed among buckets?","text":"<p>So we know that bucketing will distribute the data into some buckets/groups. You might be now wondering how are these buckets determined. Having a specific row, do we know in which bucket it will end up? Well, yes! Roughly speaking, Spark is using a hash function that is applied on the bucketing field and then computes this hash value modulo number of buckets that should be created (hash(x) mod n). This modulo operation ensures that no more than the specified number of buckets are created. For the sake of simplicity, let\u2019s first assume that after applying the hash function we get these values: (1, 2, 3, 4, 5, 6 ) and we want to create 4 buckets, so we will compute modulo 4. Modulo function returns the remainder after integer division:</p> <pre><code>1 mod 4 = 1  # remainder after the integer division\n2 mod 4 = 2\n3 mod 4 = 3\n4 mod 4 = 0\n5 mod 4 = 1\n6 mod 4 = 2\n</code></pre> <p>The computed number is the final bucket. As you can see, we just distributed these six values into four buckets</p> <pre><code>(1, 2, 3, 4, 5, 6 ) -&gt; (1, 2, 3, 0, 1, 2)\n</code></pre> <p>To be more exact, Spark is not using a simple modulo function, but a so-called positive modulo which makes sure the final bucket value is a positive number and is defined as follows:</p> <pre><code>b = value mod n\nif b &lt; 0:\n    b = (b + n) mod n\n</code></pre> <p>So if the bucket value is negative, we will add n (number of buckets) and compute the modulo again which will no longer be negative. Let's assume this example in which the hash function returns negative number -9 and we want to compute to which bucket it belongs (still assuming we use four buckets):</p> <pre><code>n = 4\nvalue = -9\nb = value mod n = -9 mod 4 = -1\n# be is negative so we continue:\nb = (b + n) mod n = (-1 + 4) mod 4 = 3 mod 4 = 3\n</code></pre> <p>So the value -9 will belong to bucket number 3.</p> <p>The hash function that Spark is using is implemented with the MurMur3 hash algorithm and the function is actually exposed in the DataFrame API (see in docs) so we can use it to compute the corresponding bucket if we want:</p> <pre><code>from pyspark.sql.functions import hash, col, expr\n(\n    spark.range(100) # this will create a DataFrame with one column id\n        .withColumn(\"hash\", hash(col(\"id\")))\n        .withColumn(\"bucket\", expr(\"pmod(hash, 8)\"))\n)\n</code></pre> <p>Here we can see how the data would be distributed into buckets if we use bucketing by the column id with 8 buckets. Notice that the pmod function is called inside expr because the function is not directly available in the PySpark API, but it is available in SQL (to see more information about how the <code>expr</code> function can be used with SQL functions, you can check my recent article about DataFrame transformations).</p>"},{"location":"tools/etl/spark/spark-bucketing/#advantages-of-bucketing","title":"Advantages of bucketing","text":"<p>The main goal of bucketing is to speed up queries and gain performance improvements. There are two main areas where bucketing can help, the first one is to avoid shuffle in queries with joins and aggregations, the second one is to reduce the I/O with a feature called bucket pruning. Let\u2019s see both these optimization opportunities more in detail in the following subsections.</p>"},{"location":"tools/etl/spark/spark-bucketing/#shuffle-free-joins","title":"Shuffle-free joins","text":"<p>If you are joining two tables and neither of them is particularly small, Spark will have to make sure both tables are distributed on the cluster in the same way (according to the joining key) and will therefore shuffle the data (both tables will be shuffled). In the query plan, you will see an <code>Exchange</code> operator in both branches of the join. Let\u2019s see an example:</p> <pre><code>tableA.join(tableB, 'user_id')\n</code></pre> <p>If the join is planed with sort-merge join, the execution plan will look like this:</p> <p></p> <p>As you can see, each branch of the join contains an <code>Exchange</code> operator that represents the shuffle (notice that Spark will not always use sort-merge join for joining two tables \u2014 to see more details about the logic that Spark is using for choosing a joining algorithm, see my other article About Joins in Spark 3.0 where we discuss it in detail).</p> <p>However, if both tables are bucketed by the joining key into the same number of buckets, Spark will read the data on the cluster with this specific distribution, so it doesn't require additional repartitioning and shuffle \u2014 the Exchange operator will no longer be present in the plan:</p> <p></p>"},{"location":"tools/etl/spark/spark-bucketing/#one-side-shuffle-free-join","title":"One-side shuffle-free join","text":"<p>An interesting question is what happens if only one table is bucketed and the other is not. The answer actually depends on the number of buckets and the number of shuffle partitions. If the number of buckets is greater or equal to the number of shuffle partitions, Spark will shuffle only one side of the join \u2014 the table that was not bucketed. However, if the number of buckets is less than the number of shuffle partitions, Spark will shuffle both tables and will not leverage the fact that one of the tables is already well distributed. The default number of shuffle partitions is 200 and it can be controlled with this configuration setting:</p> <pre><code>spark.conf.set(\"spark.sql.shuffle.partitions\", n)\n</code></pre> <p>So if we use the default setting (200 partitions) and one of the tables (let\u2019s say <code>tableA</code>) is bucketed into, for example, 50 buckets and the other table (<code>tableB</code>) is not bucketed at all, Spark will shuffle both tables and will repartition the tables into 200 partitions.</p> <p>To make sure that bucketing of <code>tableA</code> is leveraged, we have two options, either we set the number of shuffle partitions to the number of buckets (or smaller), in our example 50,</p> <pre><code># if tableA is bucketed into 50 buckets and tableB is not bucketed\nspark.conf.set(\"spark.sql.shuffle.partitions\", 50)\ntableA.join(tableB, joining_key)\n</code></pre> <p>or we repartition the tableB into 50 partitions by explicitly calling repartition as follows:</p> <pre><code>(\n    tableA\n        .join(tableB.repartition(50, joining_key), joining_key)\n)\n</code></pre> <p>Both these techniques will lead to a one-side shuffle-free join, which can be seen also from the query plan because the Exchange operator will be in only one branch of the join and so only one table will be shuffled.</p>"},{"location":"tools/etl/spark/spark-bucketing/#tables-with-different-bucket-numbers","title":"Tables with different bucket numbers","text":"<p>There is one more situation that we can consider here. What if both tables are bucketed, but into a different number of buckets? What will happen depends on the Spark version because an enhancement was implemented for this case in 3.1.1.</p> <p>Before 3.1 the situation was actually similar to the previous case where only one table is bucketed and the other is not, in other words, both tables will be shuffled unless a specific condition with shuffle partitions and the number of buckets is met in which case only one table will be shuffled and we will get a one-side shuffle-free join. Here the condition is similar as before \u2014 the number of shuffle partitions must be equal to or less than the number of buckets of the bigger table. Let\u2019s see this more clearly on a simple example: if tableA has 50 buckets, tableB has 100, and the number of shuffle partitions is 200 (default), in that case, both tables will be shuffled into 200 partitions. However, if the number of shuffle partitions is set to 100 or less, only the tableA will be shuffled into 100 partitions. Similarly, we can also repartition one of the tables to the number of buckets of the other table in which case also only one shuffle would happen during the execution.</p> <p>In Spark 3.1.1 a new feature was implemented which can coalesce the larger number of buckets into the smaller one if it buckets numbers are multiples of each other. This feature is by default turned off and can be controlled with this configuration setting <code>spark.sql.bucketing.coalesceBucketsInJoin.enabled</code>. So if we turn it on and have again tableA bucketed into 50 buckets, tableB into 100, the join will be shuffle-free because Spark will coalesce tableB into 50 buckets so both tables will have the same number and this will happen regardless of the number of shuffle partitions.</p>"},{"location":"tools/etl/spark/spark-bucketing/#what-about-the-sort","title":"What about the sort?","text":"<p>We have seen that with bucketing we can eliminate <code>Exchange</code> from the plan of sort-merge join. The plan contains also <code>Sort</code> operators, just after the <code>Exchange</code> because the data has to be sorted to be merged correctly. Can we eliminate the Sort as well? You might be tempted to say yes because the bucketing supports also sorting, we can call <code>sortBy</code> after <code>bucketBy</code> and have each bucket sorted, so it should be possible to leverage that during the join. However, the situation with sort is more complex.</p> <p>Before Spark 3.0, it was possible to eliminate the <code>Sort</code> operator from the join plan if each bucket was formed by exactly one file. In that case, Spark was sure that the data is sorted after reading it on the cluster and indeed the final plan was Sort-free. However, if there were more files per bucket, Spark couldn\u2019t guarantee that the data is globally sorted and thus kept the Sort operator in the plan \u2014 the data had to be sorted during the join execution. (See the section <code>Bucketing from a Data Engineer perspective</code> below to learn how to achieve exactly one file per bucket.)</p> <p>In Spark 3.0 the situation changed and by default, the <code>Sort</code> is present even if there is only one file per bucket. The reason for this change was that listing all the files to check if there is only one per bucket was too expensive (if there were too many files) so it was decided to turn off this check and have the Sort in the plan all the time (for the sort-merge join). As you can see it is a trade-off, one optimization for another. There was also introduced a new configuration setting <code>spark.sql.legacy.bucketedTableScan.outputOrdering</code> that you can set to <code>True</code> to enforce the behavior before 3.0 and still leverage the sorted buckets with one file.</p>"},{"location":"tools/etl/spark/spark-bucketing/#shuffle-free-aggregations","title":"Shuffle-free aggregations","text":"<p>Similarly to joins, aggregations also require correct distribution of the data on the cluster and in general Spark will have to shuffle the data for the following queries:</p> <pre><code># this requires partial shuffle if tableA is not bucketed:\n(\n    tableA\n        .groupBy('user_id')\n        .agg(count('*'))\n)\n# this requires full shuffle if tableA is not bucketed :\n(\n    tableA\n        .withColumn('n', count('*').over(Window().partitionBy('user_id')))\n)\n</code></pre> <p>If, however, the tableA is bucketed by the field user_id, both queries will be shuffle-free.</p>"},{"location":"tools/etl/spark/spark-bucketing/#bucket-pruning","title":"Bucket pruning","text":"<p>Bucket pruning is a feature that was released in Spark 2.4 and its purpose is to reduce I/O if we use a filter on the field by which the table is bucketed. Let\u2019s assume the following query:</p> <pre><code>spark.table('tableA').filter(col('user_id') == 123)\n</code></pre> <p>If the table is not bucketed, Spark will have to scan the entire table to find this record and if the table is large, it can take many tasks that will have to be launched and executed. On the other hand, if the table is bucketed, Spark will know immediately to which bucket this row belongs (Spark computes the hash function with the modulo to see directly the bucket number) and will scan files only from the corresponding bucket. And how does Spark know which files belong to which bucket? Well, each file name has a specific structure and contains information not only about the bucket to which it belongs, but also which task produced the file as you can see from this picture:</p> <p></p> <p>The bucket pruning can lead to a huge speed-up if the table is very large.</p>"},{"location":"tools/etl/spark/spark-bucketing/#disadvantages-of-bucketing","title":"Disadvantages of bucketing","text":"<p>We just described the advantages that bucketing can offer. You might be wondering whether there are also some disadvantages or simply some situations in which it is better to avoid it. There is actually one consequence of bucketing that is good to keep in mind, and it is parallelization during execution. If a table is bucketed into n buckets, and you will query it, the first stage of the resulting job will have exactly <code>n</code> tasks. On the other hand, if the table is not bucketed or the bucketing is turned off, a number of tasks can be very different because Spark will try to split the data into partitions to have approximately 128 MB per partition (this is controlled by configuration setting <code>spark.sql.files.maxPartitionBytes</code>) so the tasks have reasonable size and don\u2019t get into troubles with memory.</p> <p>If a table is bucketed and over time it grew in size and the buckets become large it could be more efficient to turn the bucketing off to allow Spark to create more partitions and avoid problems with data spill. This is useful especially if the query doesn\u2019t do any operations that could directly leverage the distribution provided by bucketing.</p> <p>In Spark 3.1.1 a new feature was implemented that can recognize a situation in which bucketing is not useful based on the query plan (no joins or aggregations) and will turn the bucketing off in the sense that it will discard the distribution and scan the data in the same way as if it wasn\u2019t bucketed. This feature is by default turned on and can be controlled by <code>spark.sql.sources.bucketing.autoBucketedScan.enabled</code> configuration setting.</p>"},{"location":"tools/etl/spark/spark-bucketing/#bucketing-from-a-data-analyst-perspective","title":"Bucketing from a data analyst perspective","text":"<p>Data Analyst wants to query the data and in an ideal world, he/she doesn\u2019t want to care about the details of how the table is stored in the data lake. Well, we don\u2019t live in an ideal world and sometimes it is still useful to know some details about the table to leverage a faster execution and achieve better performance. It will be important to be able at least check if the bucketing is leveraged in the query or if it can be leveraged, in other words, if there is a way to easily achieve some performance improvement for the query.</p>"},{"location":"tools/etl/spark/spark-bucketing/#is-the-table-bucketed","title":"Is the table bucketed?","text":"<p>To see if and how a table is bucketed we can simply check the details about the table by calling a SQL statement</p> <pre><code>spark.sql(\"DESCRIBE EXTENDED table_name\").show(n=100)\n</code></pre> <p></p> <p>From this, you can see if the table is bucketed, what fields were used for the bucketing and how many buckets the table has. Notice that we called here <code>show(n=100)</code> because the show function displays by default only 20 rows, but if the schema of the table is large, the information about bucketing will not appear in the first 20 rows, so just be aware that depending on the table it might be needed to show more rows to see the bucketing info.</p>"},{"location":"tools/etl/spark/spark-bucketing/#is-the-bucketing-leveraged-in-my-query","title":"Is the bucketing leveraged in my query?","text":"<p>First of all, bucketing has to be enabled, which is by default, but if you are not sure, you can check it as follows</p> <pre><code>spark.conf.get(\"spark.sql.sources.bucketing.enabled\")\n</code></pre> <p>and it should return <code>True</code>. This configuration setting can be used to control if bucketing is on or off.</p> <p>If a table is bucketed the information about it is saved in metastore. If we want Spark to use it we need to access the data as a table (this will make sure that Spark gets the information from the metastore):</p> <pre><code># Spark will use the information about bucketing from metastore:\ndf = spark.table(table_name)\n# Spark will not use the information about bucketing:\ndf = spark.read.parquet(path_to_data)\n</code></pre> <p>Notice that in the second case where we approach the data directly from the path, Spark will not communicate with the Hive metastore and will not get the information about bucketing \u2014 bucketing will not be used.</p> <p>Last but not least we can check the query plan and see if there are <code>Exchange</code> operators in the plan in places where we want to avoid them.</p>"},{"location":"tools/etl/spark/spark-bucketing/#can-i-help-spark","title":"Can I help Spark?","text":"<p>Usually, if the tables are bucketed in the same number of buckets, the bucketing will work out of the box. But there are some cases in which Spark will fail to leverage bucketing and we can actually help to make it work. To get an idea, let\u2019s see some of these situations.</p> <p>Before Spark 3.0, if the bucketing column has a different name in two tables that we want to join, and we rename the column in the DataFrame to have the same name, the bucketing will stop working. For example, <code>tableA</code> is bucketed by <code>user_id</code>, and <code>tableB</code> is bucketed by <code>userId</code>, the column has the same meaning (we can join on it), but the name is different (<code>user_id</code> vs <code>userId</code>). The bucketing will not be fully leveraged in the following query:</p> <pre><code># The bucketing information is discarded because we rename the\n# bucketed column and we will get extra shuffle:\n(\n    tableA\n        .withColumnRenamed('user_id', 'userId')\n        .join(tableB, 'userId')\n)\n</code></pre> <p>To make it work, we need to keep the original names:</p> <pre><code># Here bucketing will work:\n(\n    tableA\n        .join(tableB, tableA['user_id'] == tableB['userId'])\n)\n</code></pre> <p>This issue was fixed in Spark 3.0 so renaming the column is no longer a problem.</p> <p>Another thing that is good to watch out for is the data types of the joining columns \u2014 they need to be the same. Let\u2019s assume this example: <code>tableA</code> is bucketed by <code>user_id</code> which is of integer type, <code>tableB</code> is also bucketed by <code>user_id</code>, but it is of long type and both tables are bucketed into 50 buckets. In this situation the data types of the joining column are different in each table, so Spark will have to cast it, will discard the bucketing information and both tables will be shuffled:</p> <pre><code># both tables will be shuffled if user_id has different data type\n# in both tables:\ntableA.join(tableB, user_id)\n</code></pre> <p>It is quite unfortunate that both tables were created with a different data type for a column that has the same meaning. Nevertheless, we can help Spark to achieve at least one-side shuffle-free join as follows:</p> <pre><code>(\n    tableA\n        .withColumn('user_id', col('user_id').cast('long'))\n        .repartition(50, 'user_id')\n        .join(tableB, 'user_id')\n)\n</code></pre> <p>As you can see we explicitly convert the data type to be the same in both tables and then repartition the changed table into the same number of partitions as is the other table. The shuffle will happen only on this side where we repartition it, the other table will be shuffle-free. This basically becomes equivalent to the situation in which only one table is bucketed and the other is not.</p> <p>The last example in this section is related to using a user-defined function (UDF) in a query with a join. We need to keep in mind, that UDF will discard information about bucketing, so if we call the UDF before the join, it will lead to the same situation as if only one table is bucketed. Either both tables will be shuffled, or we will have one-side shuffle-free join if we repartition the table or if we set the number of shuffle partitions to the number of buckets:</p> <pre><code># Spark will shuffle both tables because of the UDF\n(\n    tableA.withColumn('x', my_udf('some_col'))\n        .join(tableB, 'user_id')\n)\n# One-side shuffle-free join:\n(\n    tableA.withColumn('x', my_udf('some_col'))\n        .repartition(50, 'user_id') # assuming we have 50 buckets\n        .join(tableB, 'user_id')\n)\n# One-side shuffle-free join:\n# set number of shuffle partitions to number of buckets (or less):\nspark.conf.set('spark.sql.shuffle.partitions', 50)\n(\n    tableA.withColumn('x', my_udf('some_col'))\n        .join(tableB, 'user_id')\n)\n</code></pre> <p>And if we want to totally avoid the shuffle, we can simply call the UDF after the join</p> <pre><code>(\n    tableA\n      .join(tableB, 'user_id')\n      .withColumn('x', my_udf('some_col'))\n)\n</code></pre>"},{"location":"tools/etl/spark/spark-bucketing/#bucketing-from-a-data-engineer-perspective","title":"Bucketing from a data engineer perspective","text":"<p>The tables in the data lake are usually prepared by data engineers. They need to consider how the data will be used and prepare it so it serves typical use-cases of the data users, which are usually data analysts and scientists. Bucketing is one of the techniques that need to be considered similarly to partitioning which is another way how to organize data in the file system. Let\u2019s now see some issues that the data engineer usually has to face.</p>"},{"location":"tools/etl/spark/spark-bucketing/#how-to-create-a-bucketed-table","title":"How to create a bucketed table","text":"<p>We have already seen the query above using the function <code>bucketBy</code>. The problem in practice becomes to control the number of created files. We need to keep in mind that each task in the last stage of the Spark job will create one file for each bucket for which it carries data. Let\u2019s assume this example in which we process a 20 GB dataset and we have the data distributed into 200 tasks in the last stage (each task processing approximately 100 MB) and we want to create a table with 200 buckets. If the data on the cluster is distributed randomly (which is the general case) each of these 200 tasks will carry data for each of these 200 buckets, so each task will create 200 files, leading to 200 x 200 = 40 000 files, where all final files will be very small. You can see that number of resulting files is the multiplication of the number of tasks with the requested number of final buckets.</p> <p>We can fix this problem by achieving already on the cluster the same distribution that we want to have in the file system (in the storage). If each task has data only for one bucket, in that case, each task will write only one file. That can be achieved by custom repartition before the writing.</p> <pre><code>(\n    df.repartition(expr(\"pmod(hash(user_id), 200)\"))\n        .write\n        .mode(saving_mode)  # append/overwrite\n        .bucketBy(200, 'user_id')\n        .option(\"path\", output_path)\n        .saveAsTable(table_name)\n)\n</code></pre> <p>This will create exactly one file per bucket. As you can see we repartition the data exactly by the same expression that is Spark using under the hood to distribute the data among the buckets (see the relevant section above for more details on how this works). You could actually use here more simple <code>df.repartition(200, \u2018user_id\u2019)</code> with the same result, but the advantage of the above approach is that it works also if you want to at the same time partition the data in the filesystem by another field as follows</p> <pre><code>(\n    df\n        .repartition(200, \"created_year\",expr(\"pmod(hash(user_id), 200)\"))\n        .write\n        .mode(saving_mode)\n        .partitionBy(\"created_year\")\n        .bucketBy(200, \"user_id\")\n        .option(\"path\", output_path)\n        .saveAsTable(table_name)\n)\n</code></pre> <p>Here each file system partition will have exactly 200 files (one file per bucket), so the total number of files will be the number of buckets multiplied by the number of file system partitions. Notice that this wouldn\u2019t work out if you just call <code>df.repartition(200, \u201ccreated_year\u201d, \u201cuser_id\u201d)</code>.</p>"},{"location":"tools/etl/spark/spark-bucketing/#how-to-determine-a-reasonable-number-of-buckets","title":"How to determine a reasonable number of buckets","text":"<p>This can be tricky and depends on more circumstances. It is important to consider the size of the final buckets \u2014 remember that when you read the data back, one bucket will be processed by one task, and if the bucket is large in size, the task will get into troubles with memory and Spark will have to spill data on the disk during the execution which will lead to a performance hit. Depending on the queries that you will run on the data, 150-200 MB per bucket might be a reasonable choice and if you know the total size of the dataset, you can compute from this how many buckets to create. In practice, the situation is more complex and one has to face the following challenges:</p> <ul> <li> <p>the table is continuously appended and its size grows in time and so does the   size of the buckets. In some cases, this may still be fine if the dataset is also   partitioned by some date dimension (year and month for example) and the buckets   are distributed uniformly across these partitions. If the typical query asks always   only for recent data, for example, the last 6 months, we can design the buckets   so the reasonable size corresponds to the 6 months of data. The total size of   the bucket will grow, but it doesn\u2019t matter, because we will never ask for the   entire bucket.</p> </li> <li> <p>the data is skewed \u2014 this happens if there is a specific value of the bucketing   key for which there are many more records than for other values of the key. For   example, if the table is bucketed by user_id, there might be a specific user that   has many more interactions/activities/purchases or whatever the dataset represents   and this will lead to data skew \u2014 the task that will process this bigger bucket   will take longer than the other.</p> </li> </ul>"},{"location":"tools/etl/spark/spark-bucketing/#evolution-of-the-bucketing-feature","title":"Evolution of the bucketing feature","text":"<p>Spark itself is being continuously evolved and improved with each new release. Also, the bucketing feature underwent some improvements in the last few releases, so let's mention some of these here:</p>"},{"location":"tools/etl/spark/spark-bucketing/#improvements-in-spark-24","title":"Improvements in Spark 2.4","text":"<ul> <li>Bucket pruning (see Jira)\u2014 reduce I/O with a filter on the bucketed field.</li> </ul>"},{"location":"tools/etl/spark/spark-bucketing/#improvements-in-spark-30","title":"Improvements in Spark 3.0","text":"<ul> <li> <p>Discard information about sorting (see Jira) \u2014 this is not really an improvement of bucketing, but rather the opposite. After this change, the sort-merge join always requires sorting, no matter if the buckets are already sorted. This was done in favor to have a faster explain command which needed to do file listing to verify if each bucket has only one file. There is a configuration setting that can bring the original behavior back (spark.sql.legacy.bucketedTableScan.outputOrdering, by default it is False so you need to set it to True if you want to leverage sorted buckets during the join). Also, see the discussion about the sort in the relevant section above.</p> </li> <li> <p>Respect aliases in output partitioning (see Jira) \u2014 it makes sure that sort-merge join will be shuffle-free for bucketed tables even if we rename the bucketed column.</p> </li> </ul>"},{"location":"tools/etl/spark/spark-bucketing/#improvements-in-spark-31","title":"Improvements in Spark 3.1","text":"<ul> <li> <p>Coalescing bucketed tables for join (see Jira) \u2014 enable shuffle-free join if both tables have a different number of buckets. See the discussion about the feature in the relevant section above.</p> </li> <li> <p>Enable/disable bucketing by a rule (see Jira) \u2014 a rule that will turn off bucketing if it cannot be leveraged in the query.</p> </li> </ul>"},{"location":"tools/etl/spark/spark-bucketing/#future-improvements","title":"Future improvements","text":"<p>Here is listed a couple of features that are not implemented at the time of writing this article (April 2021):</p> <ul> <li> <p>Add bucket scan info to explain (see Jira) \u2014 see the information if the bucketing is used in the query plan</p> </li> <li> <p>Read multiple sorted bucket files (see Jira) \u2014 leverage the sorted buckets for the sort-merge join even if there are more files per bucket</p> </li> <li> <p>Hive bucketing write support (see Jira) \u2014 enable compatibility with Hive bucketing (so it could be leveraged also by Presto)</p> </li> </ul>"},{"location":"tools/etl/spark/spark-bucketing/#configuration-settings-related-to-bucketing","title":"Configuration settings related to bucketing","text":"<p>We have already seen some of them throughout the article, but let\u2019s list them here to have them in one place:</p> <ul> <li> <p>spark.sql.sources.bucketing.enabled \u2014 control if bucketing is on/off, default is True.</p> </li> <li> <p>spark.sql.sources.bucketing.maxBuckets \u2014 maximum number of buckets that can be used for a table. By default, it is 100 000.</p> </li> <li> <p>spark.sql.sources.bucketing.autoBucketedScan.enabled \u2014 it will discard bucketing information if it is not useful (based on the query plan). By default it is True.</p> </li> <li> <p>spark.sql.bucketing.coalesceBucketsInJoin.enabled \u2014 if both tables have a different number of buckets, it will coalesce buckets of the table with the bigger number to have the same as the other table. It works only if both numbers are multiples of each other. It is also constrained by the next configuration setting. By default it is False.</p> </li> <li> <p>spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio \u2014 the maximum ratio of the two bucket numbers to have the coalescing work. By default, it is 4. In other words, if one table has more than 4 times the number of buckets than the other table, the coalescing will not take place.</p> </li> <li> <p>spark.sql.legacy.bucketedTableScan.outputOrdering \u2014 use the behavior before Spark 3.0 to leverage the sorting information from bucketing (it might be useful if we have one file per bucket). By default it is False.</p> </li> <li> <p>spark.sql.shuffle.partitions \u2014 control number of shuffle partitions, by default it is 200.</p> </li> </ul>"},{"location":"tools/etl/spark/spark-bucketing/#final-discussion","title":"Final discussion","text":"<p>In this report, we described bucketing from different perspectives. We have seen some of the issues that a data engineer needs to deal with when creating a bucketed table, like choosing a reasonable number of buckets and controlling the number of created files. We have also discussed the data analyst view \u2014 having bucketed tables provides optimization opportunities. In many cases, these opportunities are utilized by Spark out of the box, in some situations, however, extra care needs to be taken to leverage the bucketing potential. This happens with joins of tables where the bucketing details differ, for example, the tables have a different number of buckets, the bucketing column has a different name or data type, here we have seen that at least one-side shuffle-free join can be achieved with simple tricks using an explicit repartition of the DataFrame, or changing the number of shuffle partitions to meet the number of buckets.</p>"},{"location":"tools/etl/spark/spark-bucketing/#references","title":"References","text":"<ul> <li>https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53</li> </ul>"},{"location":"tools/etl/spark/spark-dataframe/","title":"Spark: DataFrame","text":"<p>DataFrames are a higher-level abstraction in PySpark compared to RDDs, making it easier to work with structured data. They offer a tabular structure, akin to a spreadsheet or SQL table, with rows and columns.</p>"},{"location":"tools/etl/spark/spark-dataframe/#create-dataframe","title":"Create DataFrame","text":"CSV data <p>id,name,age,salary,city,gender,marital_status,education_level,joining_date,performance_rating,attrition 1,John Doe,28,60000,New York,Male,Married,Bachelor's,2022-01-01,90,No 2,Jane Smith,35,75000,San Francisco,Female,Single,Master's,2021-12-15,75,Yes 3,Bob Johnson,42,90000,Chicago,Male,Married,PhD,2022-02-10,88,No 4,Alice Brown,31,65000,Los Angeles,Female,Single,Bachelor's,2021-11-20,77,Yes 5,Charlie Wilson,45,80000,Houston,Male,Married,Master's,2021-10-05,92,No 6,Eva Davis,29,70000,Miami,Female,Single,Bachelor's,2022-03-15,80,Yes 7,Michael Lee,38,85000,Seattle,Male,Married,Master's,2021-09-30,85,No 8,Sophia Chen,33,72000,Boston,Female,Single,Bachelor's,2022-04-20,79,Yes 9,David White,40,95000,Atlanta,Male,Married,PhD,2021-08-12,93,No 10,Olivia Miller,27,68000,Dallas,Female,Single,Master's,2022-05-25,76,Yes 11,James Taylor,34,78000,Denver,Male,Married,Bachelor's,2021-07-10,82,No 12,Emma Moore,32,72000,Austin,Female,Single,Master's,2022-06-18,88,Yes 13,William Hall,39,92000,Phoenix,Male,Married,PhD,2021-06-05,90,No 14,Grace Johnson,26,65000,Portland,Female,Single,Bachelor's,2022-07-30,78,Yes 15,Liam Anderson,37,88000,San Diego,Male,Married,Master's,2021-05-15,87,No 16,Ava Garcia,30,71000,Philadelphia,Female,Single,Bachelor's,2022-08-12,83,Yes 17,Mason Lewis,41,97000,Minneapolis,Male,Married,PhD,2021-04-02,95,No 18,Sofia Wright,28,69000,Charlotte,Female,Single,Master's,2022-09-25,79,Yes 19,Ethan Davis,43,89000,Detroit,Male,Married,Bachelor's,2021-03-18,88,No 20,Mia Martin,29,74000,San Antonio,Female,Single,Master's,2022-10-10,81,Yes 21,Aiden Allen,36,82000,Orlando,Male,Married,PhD,2021-02-09,87,No 22,Chloe Johnson,31,68000,Tampa,Female,Single,Bachelor's,2022-11-05,76,Yes 23,Lucas Smith,44,92000,Raleigh,Male,Married,Master's,2021-01-22,90,No 24,Zoe Brown,27,71000,Columbus,Female,Single,Bachelor's,2022-12-20,77,Yes 25,Jackson Lee,38,85000,Indianapolis,Male,Married,PhD,2020-12-01,85,No 26,Lily Wang,33,77000,San Jose,Female,Single,Master's,2023-01-15,79,Yes 27,Logan Taylor,40,93000,Nashville,Male,Married,Bachelor's,2020-11-10,93,No 28,Isabella Wilson,26,66000,Memphis,Female,Single,Master's,2023-02-28,68,Yes 29,Christopher Miller,35,86000,New Orleans,Male,Married,PhD,2020-10-25,86,No 30,Sophie Davis,32,70000,Louisville,Female,Single,Bachelor's,2023-03-20,78,Yes</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import (\n    StructType,\n    StructField,\n    StringType,\n    IntegerType,\n    FloatType,\n    DateType,\n)\nfrom pyspark.sql.functions import round, col\n\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\n\n# Define a schema for the DataFrame\nschema = StructType(\n    [\n        StructField(\"id\", IntegerType(), True),\n        StructField(\"name\", StringType(), True),\n        StructField(\"age\", IntegerType(), True),\n        StructField(\"salary\", FloatType(), True),\n        StructField(\"city\", StringType(), True),\n        StructField(\"gender\", StringType(), True),\n        StructField(\"marital_status\", StringType(), True),\n        StructField(\"education_level\", StringType(), True),\n        StructField(\"joining_date\", DateType(), True),\n        StructField(\"performance_rating\", IntegerType(), True),\n        StructField(\"attrition\", StringType(), True),\n    ]\n)\n\n# Create a DataFrame from the CSV data\ndf = spark.read.csv(\n    spark.sparkContext.parallelize(csv_data.split(\"\\n\")), header=True, schema=schema\n)\n\n# Display the DataFrame\ndf.show()\n</code></pre>"},{"location":"tools/etl/spark/spark-dataframe/#transformations-and-actions","title":"Transformations and Actions","text":""},{"location":"tools/etl/spark/spark-dataframe/#filtering","title":"Filtering","text":"<pre><code>df.filter(df.age &gt; 30).show()\n</code></pre>"},{"location":"tools/etl/spark/spark-dataframe/#selecting-columns","title":"Selecting Columns","text":"<pre><code>df.select(\"name\", \"salary\").show()\n</code></pre>"},{"location":"tools/etl/spark/spark-dataframe/#aggregations-and-rounding-decimals","title":"Aggregations and rounding decimals","text":"<pre><code>df.groupBy(\"gender\").agg({\"salary\": \"avg\"}).withColumn(\n    \"avg(salary)\", round(col(\"avg(salary)\"), 2)\n).show()\n</code></pre>"},{"location":"tools/etl/spark/spark-dataframe/#joining","title":"Joining","text":"<pre><code>df.filter(df.age &gt; 30).join(df.filter(df.salary &gt; 80000), \"id\", \"inner\")\n</code></pre>"},{"location":"tools/etl/spark/spark-dataframe/#references","title":"References","text":"<ul> <li>Navigating PySpark DataFrames: An In-Depth Guide</li> </ul>"},{"location":"tools/etl/spark/spark-dynamic-allocate/","title":"Spark: Dynamic Allocation","text":"<ul> <li>Apache Spark 101: Dynamic Allocation, <code>spark-submit</code> Command and Cluster Management</li> </ul>"},{"location":"tools/etl/spark/spark-memory/","title":"Spark: Memory","text":"<ul> <li>https://medium.com/@think-data/understanding-the-memory-components-of-spark-e3070f315d17</li> <li>Decoding Spark Executor Memory Management : Tuning spark configs</li> </ul>"},{"location":"tools/etl/spark/spark-memory/#on-heap-vs-off-heap-memory-management","title":"On-Heap vs Off-Heap Memory Management","text":""},{"location":"tools/etl/spark/spark-memory/#tungsten-optimizer-enhancing-sparks-memory-management","title":"Tungsten Optimizer: Enhancing Spark\u2019s Memory Management","text":"<p>The Tungsten optimizer represents a significant advancement in Spark\u2019s ability to efficiently process data. It optimizes both CPU and memory usage in Spark applications, leveraging on-heap and off-heap memory management techniques. Tungsten employs advanced strategies like binary in-memory data representation and explicit memory management.</p> <p>Key Benefits:</p> <ul> <li>Improved Execution Speed: Tungsten\u2019s memory management strategies enable faster operation execution.</li> <li>Efficient Resource Utilization: By balancing the use of on-heap and off-heap memory, Tungsten optimizes resource utilization.</li> <li>Reduced Overhead: Minimizes the overhead of garbage collection, enhancing the overall performance of Spark applications.</li> </ul>"},{"location":"tools/etl/spark/spark-memory/#integration-with-spark-memory-management","title":"Integration with Spark Memory Management","text":"<p>When integrating Tungsten with Spark\u2019s memory management, particularly in off-heap mode, certain Spark configurations need to be adjusted for optimal performance:</p> <ul> <li><code>spark.memory.offHeap.enabled</code>:</li> </ul> <p>This configuration must be set to true to enable the use of off-heap memory.</p> <ul> <li><code>spark.memory.offHeap.size</code>:</li> </ul> <p>It specifies the amount of off-heap memory to be allocated (in bytes).   This setting is crucial as it determines the memory capacity available for   off-heap storage.</p> <ul> <li><code>spark.executor.memory</code>:</li> </ul> <p>While this parameter primarily configures on-heap memory, it's important to   balance it with off-heap settings to ensure overall memory allocation is sufficient   for the workload.</p> <ul> <li><code>spark.executor.memoryOverhead</code>:</li> </ul> <p>This setting is used to account for additional memory overhead apart from the   memory used by the executor process. In off-heap mode, this might need to be   increased to accommodate the extra memory usage.</p>"},{"location":"tools/etl/spark/spark-memory/#conclusion","title":"Conclusion","text":"<p>Apache Spark's on-heap and off-heap memory management, integral to its performance in big data processing, are significantly enhanced by the Tungsten optimizer. This synergy ensures optimal resource utilization, enabling Spark to handle large-scale and complex data processing tasks effectively. As the demands for data processing continue to escalate, innovations in Spark\u2019s memory management and optimization remain crucial in the realm of big data analytics.</p>"},{"location":"tools/etl/spark/spark-memory/#out-of-memory","title":"Out of Memory","text":"<p>Spark Out Of Memory Error</p>"},{"location":"tools/etl/spark/spark-memory/#references","title":"References","text":"<ul> <li>Apache Spark Memory Management: On-Heap vs Off-Heap in the Context of Tungsten Optimizer</li> </ul>"},{"location":"tools/etl/spark/spark-nested-datatypes/","title":"Spark Nested Data Types","text":"<p>In the previous article on Higher-Order Functions, we described three complex data types: arrays, maps, and structs and focused on arrays in particular. In this follow-up article, we will take a look at structs and see two important functions for transforming nested data that were released in Spark 3.1.1 version. For the code, we will use Python API.</p>"},{"location":"tools/etl/spark/spark-nested-datatypes/#struct","title":"Struct","text":"<p>The <code>StructType</code> is a very important data type that allows representing nested hierarchical data. It can be used to group some fields together. Each element of a <code>StructType</code> is called StructField and it has a name and also a type. The elements are also usually referred to just as fields or subfields, and they are accessed by the name. The <code>StructType</code> is also used to represent the schema of the entire DataFrame. Let\u2019s see a simple example</p> <pre><code>from pyspark.sql.types import *\nmy_schema = StructType([\n    StructField('id', LongType()),\n    StructField('country', StructType([\n        StructField('name', StringType()),\n        StructField('capital', StringType())\n    ])),\n    StructField('currency', StringType())\n])\nl = [\n        (1, {'name': 'Italy', 'capital': 'Rome'}, 'euro'),\n        (2, {'name': 'France', 'capital': 'Paris'}, 'euro'),\n        (3, {'name': 'Japan', 'capital': 'Tokyo'}, 'yen')\n    ]\ndf = spark.createDataFrame(l, schema=my_schema)\ndf.printSchema()\nroot\n |-- id: long (nullable = true)\n |-- country: struct (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- capital: string (nullable = true)\n |-- currency: string (nullable = true)\n\ndf.show()\n+---+---------------+--------+\n| id|        country|currency|\n+---+---------------+--------+\n|  1|  {Italy, Rome}|    euro|\n|  2|{France, Paris}|    euro|\n|  3| {Japan, Tokyo}|     yen|\n+---+---------------+--------+\n</code></pre> <p>The created DataFrame has one struct <code>country</code> that has two subfields: <code>name</code> and <code>capital</code>.</p>"},{"location":"tools/etl/spark/spark-nested-datatypes/#creating-a-struct","title":"Creating a struct","text":"<p>There are at least four basic ways how to create a <code>StructType</code> in the DataFrame. The first one we have already seen above \u2014 create DataFrame from a local collection. The second and very common way is that it will come by reading data from a source that supports complex data structures, such as JSON or Parquet. Next, there are some functions that will create a struct as a result. One particular example of such transformation is grouping by a window that will produce a struct with two subfields <code>start</code> and <code>end</code> as you can see here:</p> <pre><code>l = [(1, 10, '2021-01-01'), (2, 20, '2021-01-02')]\ndx = spark.createDataFrame(l, ['id', 'price', 'date'])\n(\n    dx\n    .groupBy(window('date', '1 day'))\n    .agg(sum('price').alias('daily_price'))\n).printSchema()\nroot\n |-- window: struct (nullable = false)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- daily_price: long (nullable = true)\n</code></pre> <p>The fourth way how to create a struct is by using the function <code>struct()</code>. The function will create a <code>StructType</code> from other columns that are passed as arguments and the <code>StructFields</code> will have the same names as the original columns unless we rename them using <code>alias()</code>:</p> <pre><code>df.withColumn('my_struct', struct('id', 'currency')).printSchema()\nroot\n |-- id: long (nullable = true)\n |-- country: struct (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- capital: string (nullable = true)\n |-- currency: string (nullable = true)\n |-- my_struct: struct (nullable = false)\n |    |-- id: long (nullable = true)\n |    |-- currency: string (nullable = true)\n</code></pre> <p>Here, we created a column <code>my_struct</code> that has two subfields that are derived from two columns that were present in the DataFrame.</p>"},{"location":"tools/etl/spark/spark-nested-datatypes/#accessing-the-elements","title":"Accessing the elements","text":"<p>As we mentioned above the subfields of a struct are accessed by the name, and it is done with a dot notation:</p> <pre><code>df.select('country.capital').show()\n+-------+\n|capital|\n+-------+\n|   Rome|\n|  Paris|\n|  Tokyo|\n+-------+\n</code></pre> <p>What might be not obvious is that this works also for arrays of structs. Let\u2019s assume that we have an array <code>countries</code> and each element of the array is a struct. If we want to access only the <code>capital</code> subfield of each struct we would do it exactly in the same way and the resulting column would be an array containing all capitals:</p> <pre><code>my_new_schema = StructType([\n    StructField('id', LongType()),\n    StructField('countries', ArrayType(StructType([\n        StructField('name', StringType()),\n        StructField('capital', StringType())\n    ])))\n])\nl = [(1, [\n        {'name': 'Italy', 'capital': 'Rome'},\n        {'name': 'Spain', 'capital': 'Madrid'}\n    ])\n]\n\ndz = spark.createDataFrame(l, schema=my_new_schema)\n# we have array of structs:\ndz.show(truncate=False)\n+---+--------------------------------+\n|id |countries                       |\n+---+--------------------------------+\n|1  |[{Italy, Rome}, {Spain, Madrid}]|\n+---+--------------------------------+\n# access all capitals:\ndz.select('countries.capital').show(truncate=False)\n+--------------+\n|capital       |\n+--------------+\n|[Rome, Madrid]|\n+--------------+\n</code></pre> <p>For another specific example of accessing elements in nested structs inside array see  This Stack Overflow question.</p>"},{"location":"tools/etl/spark/spark-nested-datatypes/#adding-new-elements","title":"Adding new elements","text":"<p>Adding a new subfield to an existing struct is supported since Spark 3.1 using the function <code>withField()</code>. Let\u2019s see our example in which we add the column <code>currency</code> to the struct country:</p> <pre><code>(\n    df\n        .withColumn(\n            'country',\n            col('country').withField('currency', col('currency'))\n        )\n).show(truncate=False)\n+---+---------------------+--------+\n|id |country              |currency|\n+---+---------------------+--------+\n|1  |{Italy, Rome, euro}  |euro    |\n|2  |{France, Paris, euro}|euro    |\n|3  |{Japan, Tokyo, yen}  |yen     |\n+---+---------------------+--------+\n</code></pre> <p>Before Spark 3.1, the situation was more complex, and adding a new field to a struct was possible by redefining the entire struct:</p> <pre><code>new_df = (\n  df.withColumn('country', struct(\n    col('country.name'),\n    col('country.capital'),\n    col('currency')\n  ))\n)\n</code></pre> <p>As you can see, we had to list all the struct subfields and after that add the new one \u2014 this can be quite cumbersome especially for large structs with many subfields. In this case, there is however a nice trick by which you can handle all subfields at once \u2014 using the star notation:</p> <pre><code>new_df = (\n  df.withColumn('country', struct(\n    col('country.*'),\n    col('currency')\n  ))\n)\n</code></pre> <p>The asterisk in the <code>country.*</code> will take all subfields of the original struct. The situation will however become more complicated in the next example where we want to remove a field.</p>"},{"location":"tools/etl/spark/spark-nested-datatypes/#removing-elements","title":"Removing elements","text":"<p>Dropping subfields from a struct is again a simple task since Spark 3.1 because the function <code>dropFields()</code> was released. Let\u2019s now work with the modified DataFrame new_df where the struct contains three subfields <code>name</code>, <code>capital</code>, and <code>currency</code>. Removing a subfield, for example, capital can be done as follows:</p> <pre><code>new_df.withColumn('country',col('country').dropFields('capital')) \\\n.show(truncate=False)\n+---+--------------+--------+\n|id |country       |currency|\n+---+--------------+--------+\n|1  |{Italy, euro} |euro    |\n|2  |{France, euro}|euro    |\n|3  |{Japan, yen}  |yen     |\n+---+--------------+--------+\n</code></pre> <p>As we can see the subfield <code>capital</code> was dropped. The situation gets complicated again in previous versions before Spark 3.1 where we have to redefine the entire struct and leave out the subfield that we want to drop:</p> <pre><code>(\n    new_df\n    .withColumn('country', struct(\n        col('country.name'),\n        col('country.currency')\n    ))\n)\n</code></pre> <p>For large structs this is again tedious, so we can make this more feasible by listing all the subfields as follows:</p> <pre><code># list all fields in the struct:\nsubfields = new_df.schema['country'].dataType.fieldNames()\n# remove the subfield from the list:\nsubfields.remove('capital')\n# use the new list to recreate the struct:\n(\n    new_df.withColumn(\n        'country',\n        struct(\n            ['country.{}'.format(x) for x in subfields]\n        )\n    )\n).show()\n+---+--------------+--------+\n| id|       country|currency|\n+---+--------------+--------+\n|  1| {Italy, euro}|    euro|\n|  2|{France, euro}|    euro|\n|  3|  {Japan, yen}|     yen|\n+---+--------------+--------+\n</code></pre> <p>Notice that both functions <code>withField</code> and <code>dropFields</code> are members of the Column class, therefore they are called as methods on the column object (to understand more how methods from the Column class are used, check my recent article where I discuss it more in detail).</p>"},{"location":"tools/etl/spark/spark-nested-datatypes/#structs-in-sql-expressions","title":"Structs in SQL expressions","text":"<p>When you check the SQL documentation you will find that there are two functions that can be used to create structs, namely, it is <code>struct()</code> and <code>named_struct()</code> and they differ in the syntax because named_struct requires also passing a name for each subfield:</p> <pre><code>(\n  df\n  .selectExpr(\"struct(id, currency) as my_struct\")\n).show(truncate=False)\n+---------+\n|my_struct|\n+---------+\n|{1, euro}|\n|{2, euro}|\n|{3, yen} |\n+---------+\n(\n  df.selectExpr(\n    \"named_struct('id', id, 'currency', currency) as my_struct\")\n).show()\n+---------+\n|my_struct|\n+---------+\n|{1, euro}|\n|{2, euro}|\n| {3, yen}|\n+---------+\n</code></pre>"},{"location":"tools/etl/spark/spark-nested-datatypes/#conclusion","title":"Conclusion","text":"<p>In this article, we continued our description of complex data types in Spark SQL. In the previous article, we covered arrays, here we focused on structs, and in the future post, we will cover maps. We have seen two important functions <code>withField()</code>and <code>dropFields()</code> that were released in the recent version 3.1.1 and that can simplify the code quite a bit when manipulating subfields of an existing struct.</p>"},{"location":"tools/etl/spark/spark-nested-datatypes/#references","title":"References","text":"<ul> <li>https://towardsdatascience.com/nested-data-types-in-spark-3-1-663e5ed2f2aa</li> </ul>"},{"location":"tools/etl/spark/spark-pivot/","title":"Spark: Pivot","text":""},{"location":"tools/etl/spark/spark-pivot/#create-dataframe","title":"Create DataFrame","text":"<pre><code>from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\ndata = [\n    (1, \"Ross\", \"Geller\", \"Fall94\", \"Mathematics\", 93),\n    (2, \"Monica\", \"Geller\", \"Fall94\", \"Mathematics\", 99),\n    (3, \"Rachel\", \"Green\", \"Fall94\", \"Mathematics\", 87),\n    (4, \"Joey\", \"Tribbiani\", \"Fall94\", \"Mathematics\", 73),\n    (5, \"Chandler\", \"Bing\", \"Fall94\", \"Mathematics\", 82),\n    (6, \"Phoebe\", \"Buffay\", \"Fall94\", \"Mathematics\", 87),\n    (1, \"Ross\", \"Geller\", \"Spring95\", \"CS101\", 90),\n    (2, \"Monica\", \"Geller\", \"Spring95\", \"CS101\", 89),\n    (3, \"Rachel\", \"Green\", \"Spring95\", \"CS101\", 95),\n    (4, \"Joey\", \"Tribbiani\", \"Spring95\", \"CS101\", 83),\n    (5, \"Chandler\", \"Bing\", \"Spring95\", \"CS101\", 93),\n    (6, \"Phoebe\", \"Buffay\", \"Spring95\", \"CS101\", 89),\n    (1, \"Ross\", \"Geller\", \"Fall95\", \"Science\", 90),\n    (2, \"Monica\", \"Geller\", \"Fall95\", \"Science\", 99),\n    (3, \"Rachel\", \"Green\", \"Fall95\", \"Science\", 87),\n    (4, \"Joey\", \"Tribbiani\", \"Fall95\", \"Science\", 78),\n    (5, \"Chandler\", \"Bing\", \"Fall95\", \"Science\", 85),\n    (6, \"Phoebe\", \"Buffay\", \"Fall95\", \"Science\", 89)\n]\n\nschema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"fname\", StringType(), True),\n    StructField(\"lname\", StringType(), True),\n    StructField(\"semester\", StringType(), True),\n    StructField(\"course\", StringType(), True),\n    StructField(\"grade\", IntegerType(), True)\n])\n\n# Create the DataFrame\ndf = spark.createDataFrame(data, schema=schema)\ndisplay(df)\n</code></pre>"},{"location":"tools/etl/spark/spark-pivot/#pivot","title":"Pivot","text":"<pre><code>from pyspark.sql.functions import concat_ws, col, first\n\npivot_df = (\n    df\n    .withColumn(\"semester_course\", concat_ws(\"_\", col(\"semester\"), col(\"course\")))\n    .withColumn(\"name\", concat_ws(\" \", col(\"fname\"), col(\"lname\")))\n    .groupBy(\"id\", \"name\")\n    .pivot(\"semester_course\")\n    .agg(first(\"grade\"))\n    .orderBy(\"id\")\n)\n\ndisplay(pivot_df)\n</code></pre>"},{"location":"tools/etl/spark/spark-read-json/","title":"Spark: Read Json","text":"<pre><code># Example Data Frame with column having JSON data\n_data = [\n    ['EMP001', '{\"dept\" : \"account\", \"fname\": \"Ramesh\", \"lname\": \"Singh\", \"skills\": [\"excel\", \"tally\", \"word\"]}'],\n    ['EMP002', '{\"dept\" : \"sales\", \"fname\": \"Siv\", \"lname\": \"Kumar\", \"skills\": [\"biking\", \"sales\"]}'],\n    ['EMP003', '{\"dept\" : \"hr\", \"fname\": \"MS Raghvan\", \"skills\": [\"communication\", \"soft-skills\"]}']\n]\n# Columns for the data\n_cols = ['emp_no', 'raw_data']\n\n# Let's create the raw Data Frame\ndf_raw = spark.createDataFrame(data = _data, schema = _cols)\ndf_raw.printSchema()\n</code></pre> <pre><code># Determine the schema of the JSON payload from the column\njson_schema_df = spark.read.json(df_raw.rdd.map(lambda row: row.raw_data))\njson_schema = json_schema_df.schema\n</code></pre> <pre><code># Apply the schema to payload to read the data\nfrom pyspark.sql.functions import from_json\n\ndf_details = (\n    df_raw\n        .withColumn(\"parsed_data\", from_json(df_raw[\"raw_data\"], json_schema))\n        .drop(\"raw_data\")\n)\ndf_details.printSchema()\n</code></pre> <pre><code># Lets verify the data\ndf_details.select(\"emp_no\", \"parsed_data.*\").show(10, False)\n</code></pre> <pre><code># We can explode the data further from list\nfrom pyspark.sql.functions import explode\ndf_details.select(\"emp_no\", \"parsed_data.dept\", \"parsed_data.fname\", \"parsed_data.lname\", \"parsed_data\") \\\n    .withColumn(\"skills\", explode(\"parsed_data.skills\")) \\\n    .drop(\"parsed_data\") \\\n    .show(100, False)\n</code></pre> <p>https://subhamkharwal.medium.com/pyspark-read-parse-json-column-from-another-data-frame-e2e98d80e9b6</p>"},{"location":"tools/etl/spark/spark-repartitioning/","title":"Spark Repartitioning","text":"<p>In a distributed environment, having proper data distribution becomes a key tool for boosting performance. In the DataFrame API of Spark SQL, there is a function <code>repartition()</code> that allows controlling the data distribution on the Spark cluster. The efficient usage of the function is however not straightforward because changing the distribution is related to a cost for physical data movement on the cluster nodes (a so-called shuffle).</p> <p>A general rule of thumb is that using repartition is costly because it will induce shuffle. In this article, we will go further and see that there are situations in which adding one shuffle at the right place will remove two other shuffles, so it will make the overall execution more efficient. We will first cover a bit of theory to understand how the information about data distribution is internally utilized in Spark SQL, and then we will go over some practical examples where using repartition becomes useful.</p> <p>The theory described in this article is based on the Spark source code, the version being the current snapshot 3.1 (written in June 2020) and most of it is valid also in previous versions 2.x. Also, the theory and the internal behavior are language-agnostic, so it doesn't matter whether we use it with Scala, Java, or Python API.</p>"},{"location":"tools/etl/spark/spark-repartitioning/#query-planning","title":"Query Planning","text":"<p>The DataFrame API in Spark SQL allows the users to write high-level transformations. These transformations are lazy, which means that they are not executed eagerly but instead under the hood they are converted to a query plan. The query plan will be materialized when the user calls an action which is a function where we ask for some output, for example when we are saving the result of the transformations to some storage system. The query plan itself can be of two major types: a logical plan and a physical plan. And the steps of the query plan processing can be called accordingly as logical planning and physical planning.</p>"},{"location":"tools/etl/spark/spark-repartitioning/#logical-plan","title":"Logical Plan","text":"<p>The phase of the logical planning is responsible for multiple steps related to a logical plan where the logical plan itself is just an abstract representation of the query, it has a form of a tree where each node in the tree is a relational operator. The logical plan itself does not carry any specific information about the execution or about algorithms used to compute transformations such as joins or aggregations. It just represents the information from the query in a way that is convenient for optimizations.</p> <p>During logical planning, the query plan is optimized by a Spark optimizer, which applies a set of rules that transform the plan. These rules are based mostly on heuristics, for instance, that it is better to first filter the data and then do other processing and so on.</p>"},{"location":"tools/etl/spark/spark-repartitioning/#physical-plan","title":"Physical Plan","text":"<p>Once the logical plan is optimized, physical planning begins. The purpose of this phase is to take the logical plan and turn it into a physical plan which can be then executed. Unlike the logical plan which is very abstract, the physical plan is much more specific regarding details about the execution, because it contains a concrete choice of algorithms that will be used during the execution.</p> <p>The physical planning is also composed of two steps because there are two versions of the physical plan: <code>spark plan</code> and <code>executed plan</code>. The <code>spark plan</code> is created using so-called <code>strategies</code> where each node in a logical plan is converted into one or more operators in the <code>spark plan</code>. One example of a strategy is <code>JoinSelection</code>, where Spark decides what algorithm will be used to join the data. The string representation of the <code>spark plan</code> can be displayed using the Scala API:</p> <pre><code>df.queryExecution.sparkPlan\n</code></pre> <p>After the <code>spark plan</code> is generated, there is a set of additional rules that are applied to it to create the final version of the physical plan which is the executed plan. This <code>executed plan</code> will then be executed to generate RDD code. To see this executed plan we can simply call explain on the DataFrame because it is actually the final version of the physical plan. Alternatively, we can go to the Spark UI to see its graphical representation.</p>"},{"location":"tools/etl/spark/spark-repartitioning/#ensurerequirements-er-rule","title":"EnsureRequirements (ER rule)","text":"<p>One of these additional rules that are used to transform the spark plan into the executed plan is called EnsureRequirements (in the next we will refer to it as ER rule) and this rule is going to make sure that the data is distributed correctly as is required by some transformations (for example joins and aggregations). Each operator in the physical plan is having two important properties outputPartitioning and outputOrdering (in the next we will call them oP and oO respectively) which carry the information about the data distribution, how the data is partitioned and sorted at the given moment. Besides that, each operator also has two other properties requiredChildDistribution and requiredChildOrdering by which it puts requirements on the values of oP and oO of its child nodes. Some operators don\u2019t have any requirements but some do. Let\u2019s see this on a simple example with SortMergeJoin, which is an operator that has strong requirements on its child nodes, it requires that the data must be partitioned and sorted by the joining key so it can be merged correctly. Let\u2019s consider this simple query in which we join two tables (both of them are based on a file datasource such as parquet):</p> <pre><code># Using Python API:\nspark.table(\"tableA\") \\\n    .join(spark.table(\"tableB\"), \"id\") \\\n    .write\n    ...\n</code></pre> <p>The <code>spark plan</code> for this query will look like this (and I added there also the information about the oP, oO and the requirements of the <code>SortMergeJoin</code>):</p> <p></p> <p>From the spark plan we can see that the child nodes of the SortMergeJoin (two Project operators) have no oP or oO (they are Unknown and None) and this is a general situation where the data has not been repartitioned in advance and the tables are not bucketed. When the ER rule is applied on the plan it can see that the requirements of the SortMergeJoin are not satisfied so it will fill Exchange and Sort operators to the plan to meet the requirements. The Exchange operator will be responsible for repartitioning the data to meet the requiredChildDistribution requirement and the Sort will order the data to meet the requiredChildOrdering, so the final executed plan will look like this (and this is what you can find also in the SparkUI in the SQL tab, you won\u2019t find there the spark plan though, because it is not available there):</p> <p></p>"},{"location":"tools/etl/spark/spark-repartitioning/#bucketing","title":"Bucketing","text":"<p>The situation will be different if both tables are bucketed by the joining key. Bucketing is a technique for storing the data in a pre-shuffled and possibly pre-sorted state where the information about bucketing is stored in the metastore. In such a case the FileScan operator will have the oP set according to the information from the metastore and if there is exactly one file per bucket, the oO will be also set, and it will all be passed downstream to the Project. If both tables were bucketed by the joining key to the same number of buckets, the requirements for the oP will be satisfied and the ER rule will add no Exchange to the plan. The same number of partitions on both sides of the join is crucial here and if these numbers are different, Exchange will still have to be used for each branch where the number of partitions differs from <code>spark.sql.shuffle.partitions</code> configuration setting (default value is 200). So with a correct bucketing in place, the join can be shuffle-free.</p> <p>The important thing to understand is that Spark needs to be aware of the distribution to make use of it, so even if your data is pre-shuffled with bucketing, unless you read the data as a table to pick the information from the metastore, Spark will not know about it and so it will not set the oP on the <code>FileScan</code>.</p>"},{"location":"tools/etl/spark/spark-repartitioning/#repartition","title":"Repartition","text":"<p>As mentioned at the beginning, there is a function repartition that can be used to change the distribution of the data on the Spark cluster. The function takes as argument columns by which the data should be distributed (optionally the first argument can be the number of partitions that should be created). What happens under the hood is that it adds a <code>RepartitionByExpression</code> node to the logical plan which is then converted to Exchange in the spark plan using a strategy, and it sets the oP to <code>HashPartitioning</code> with the key being the column name used as the argument.</p> <p>Another usage of the repartition function is that it can be called with only one argument being the number of partitions that should be created (repartition(n)), which will distribute the data randomly. The use case for this random distribution is however not discussed in this article.</p> <p>Let\u2019s now go over some practical examples where adjusting the distribution using repartition by some specific field will bring some benefits.</p>"},{"location":"tools/etl/spark/spark-repartitioning/#example-i-one-side-shuffle-free-join","title":"Example I: One-side shuffle-free join","text":"<p>Let\u2019s see what happens if one of the tables in the above join is bucketed and the other is not. In such a case the requirements are not satisfied because the oP is different on both sides (on one side it is defined by the bucketing and on the other side it is Unknown). In this case, the ER rule will add Exchange to both branches of the join so each side of the join will have to be shuffled! Spark will simply neglect that one side is already pre-shuffled and will waste this opportunity to avoid the shuffle. Here we can simply use repartition on the other side of the join to make sure that oP is set before the ER rule checks it and adds Exchanges:</p> <pre><code># Using Python API:\n# tableA is not bucketed\n# tableB is bucketed by id into 50 buckets\nspark.table(\"tableA\") \\\n    .repartition(50, \"id\") \\\n    .join(spark.table(\"tableB\"), \"id\") \\\n    .write \\\n    ...\n</code></pre> <p>Calling repartition will add one Exchange to the left branch of the plan but the right branch will stay shuffle-free because requirements will now be satisfied and ER rule will add no more Exchanges. So we will have only one shuffle instead of two in the final plan. Alternatively, we could change the number of shuffle partitions to match the number of buckets in tableB, in such case the repartition is not needed (it would bring no additional benefit), because the ER rule will leave the right branch shuffle-free, and it will adjust only the left branch (in the same way as repartition does):</p> <pre><code># match number of buckets in the right branch of the join\n# with the number of shuffle partitions:\nspark.conf.set(\"spark.sql.shuffle.partitions\", 50)\n</code></pre>"},{"location":"tools/etl/spark/spark-repartitioning/#example-ii-aggregation-followed-by-a-join","title":"Example II: Aggregation followed by a join","text":"<p>Another example where repartition becomes useful is related to queries where we aggregate a table by two keys and then join an additional table by one of these two keys (neither of these tables is bucketed in this case). Let\u2019s see a simple example which is based on transactional data of this kind:</p> <pre><code>{\"id\": 1, \"user_id\": 100, \"price\": 50, \"date\": \"2020-06-01\"}\n{\"id\": 2, \"user_id\": 100, \"price\": 200, \"date\": \"2020-06-02\"}\n{\"id\": 3, \"user_id\": 101, \"price\": 120, \"date\": \"2020-06-01\"}\n</code></pre> <p>Each user can have many rows in the dataset because he/she could have made many transactions. These transactions are stored in tableA. On the other hand, tableB will contain information about each user (name, address, and so on). The tableB has no duplicities, each record belongs to a different user. In our query we want to count the number of transactions for each user and date and then join the user information:</p> <pre><code># Using Python API:\ndfA = spark.table(\"tableA\") # transactions (not bucketed)\ndfB = spark.table(\"tableB\") # user information (not bucketed)\ndfA \\\n    .groupBy(\"user_id\", \"date\") \\\n    .agg(count(\"*\")) \\\n    .join(dfB, \"user_id\")\n</code></pre> <p>The spark plan of this query looks like this</p> <p></p> <p>In the spark plan, you can see a pair of HashAggregate operators, the first one (on the top) is responsible for a partial aggregation and the second one does the final merge. The requirements of the SortMergeJoin are the same as previously. The interesting part of this example are the HashAggregates. The first one has no requirements from its child, however, the second one requires for the oP to be <code>HashPartitioning</code> by <code>user_id</code> and date or any subset of these columns and this is what we will take advantage of shortly. In the general case, these requirements are not fulfilled so the ER rule will add Exchanges (and Sorts). This will lead to this executed plan:</p> <p></p> <p>As you can see we end up with a plan that has three Exchange operators, so three shuffles will happen during the execution. Let\u2019s now see how using repartition can change the situation:</p> <pre><code>dfA = spark.table(\"tableA\").repartition(\"user_id\")\ndfB = spark.table(\"tableB\")\ndfA \\\n    .groupBy(\"user_id\", \"date\") \\\n    .agg(count(\"*\")) \\\n    .join(dfB, \"user_id\")\n</code></pre> <p>The spark plan will now look different, it will contain Exchange that is generated by a strategy that converts RepartitionByExpression node from the logical plan. This Exchange will be a child of the first HashAggregate operator, and it will set the oP to HashPartitioning (user_id) which will be passed downstream:</p> <p></p> <p>The requirements for oP of all operators in the left branch are now satisfied so ER rule will add no additional Exchanges (it will still add Sort to satisfy oO). The essential concept in this example is that we are grouping by two columns and the requirements of the HashAggregate operator are more flexible so if the data will be distributed by any of these two fields, the requirements will be met. The final executed plan will have only one Exchange in the left branch (and one in the right branch) so using repartition we reduced the number of shuffles by one:</p> <p></p>"},{"location":"tools/etl/spark/spark-repartitioning/#discussion","title":"Discussion","text":"<p>It is <code>true</code> that using repartition we have now only one shuffle in the left branch instead of two, it is however important to understand that these shuffles are not of the same kind! In the original plan, both of the Exchanges came after HashAggregate which was responsible for a partial aggregation, so the data were reduced (aggregated locally on each node) before shuffling. In the new plan, the Exchange comes before HashAggregate so the full data will be shuffled.</p> <p>So what is better? One full shuffle or two reduced shuffles? Well, that ultimately depends on the properties of the data. If there are only a few records per user_id and date, it means that the aggregation will not reduce the data much, so the total shuffle will be comparable with the reduced one, and having only one shuffle will be better. On the other hand, if there are lots of records per user_id and date, the aggregation will make the data much smaller and so going with the original plan might be better because these two small shuffles can be faster than one big shuffle. This can be expressed also in terms of the cardinality of all distinct combinations of these two fields user_id and date. If this cardinality is comparable with the total row count it means that the groupBy transformation will not reduce the data much.</p>"},{"location":"tools/etl/spark/spark-repartitioning/#example-iii-union-of-two-aggregations","title":"Example III: Union of two aggregations","text":"<p>Let\u2019s consider one more example where repartition will bring optimization to our query. The problem is based on the same data as the previous example. Now in our query we want to make a union of two different aggregations, in the first one we will count the rows for each user and in the second we will sum the price column:</p> <pre><code># Using Python API:\ncountDF = df.groupBy(\"user_id\") \\\n    .agg(count(\"*\").alias(\"metricValue\")) \\\n    .withColumn(\"metricName\", lit(\"count\"))\n\nsumDF = df.groupBy(\"user_id\") \\\n    .agg(sum(\"price\").alias(\"metricValue\")) \\\n    .withColumn(\"metricName\", lit(\"sum\"))\n\ncountDF.union(sumDF)\n</code></pre> <p>Here is the final executed plan for this query:</p> <p></p> <p>It is a typical plan for a union-like query, one branch for each DataFrame in the union. We can see that there are two shuffles, one for each aggregation. Besides that, it also follows from the plan that the dataset will be scanned twice. Here the repartition function together with a small trick can help us to change the shape of the plan</p> <pre><code>df = spark.read.parquet(...).repartition(\"user_id\")\n\ncountDF = df.groupBy(\"user_id\") \\\n    .agg(count(\"price\").alias(\"metricValue\")) \\\n    .withColumn(\"metricName\", lit(\"count\"))\n\nsumDF = df.groupBy(\"user_id\") \\\n    .agg(sum(\"price\").alias(\"metricValue\")) \\\n    .withColumn(\"metricName\", lit(\"sum\"))\n\ncountDF.union(sumDF)\n</code></pre> <p></p> <p>The repartition function will move the Exchange operator before the HashAggregate, and it will make the Exchange sub-branches identical so it will be reused by another rule called ReuseExchange. In the count function, changing the star to the price column becomes important here because it will make sure that the projection will be the same in both DataFrames (we need to project the price column also in the left branch to make it the same as the second branch). It will however produce the same result as the original query only if there are no null values in the price column. To understand the logic behind this rule see my other article where I explain the ReuseExchange rule more in detail on a similar example.</p> <p>Similarly, as before, we reduced here the number of shuffles by one, but again we have now a total shuffle as opposed to reduced shuffles in the original query. The additional benefit here is that after this optimization the dataset will be scanned only once because of the reused computation.</p>"},{"location":"tools/etl/spark/spark-repartitioning/#distribution-information-loss","title":"Distribution information loss","text":"<p>As we already mentioned, it is not only important to have the data distributed in an optimal way, but it is also important to have Spark know about it. The information about the oP is propagated through the plan from a node to its parent, however, there are some operators that will stop propagating the information even if the actual distribution doesn\u2019t change. One of these operators is <code>BatchEvalPython</code> \u2014 an operator that represents a user-defined function in the Python API. So if you repartition the data, then call a Python UDF and then do a join (or some aggregation), the ER rule will add a new <code>Exchange</code> because the <code>BatchEvalPython</code> will not pass the oP information downstream. We can simply say that after calling a Python UDF, Spark will forget how the data was distributed.</p>"},{"location":"tools/etl/spark/spark-repartitioning/#controlling-the-number-of-produced-files","title":"Controlling the number of produced files","text":"<p>Let me just mention very briefly yet another use case for the <code>repartition</code> function which is to steer the number of produced files when partitioning and/or bucketing the data to the storage system. If you are partitioning data to a file system like this:</p> <pre><code>(\n    df.write\n        .partitionBy(key)\n        .save(path)\n)\n</code></pre> <p>it can produce lots of small files if the data is distributed randomly in the last stage of the Spark job. Each task in the final stage can possibly contain values for all the keys so it will create a file in each storage partition and thus produce lots of small files. Calling custom <code>repartition</code> just before to write allows us to imitate the distribution required for the file system and thus control the number of produced files. In some future post we will describe more in detail how this works and how it can be used efficiently also for bucketing.</p>"},{"location":"tools/etl/spark/spark-repartitioning/#conclusion","title":"Conclusion","text":"<p>The <code>repartition</code> function allows us to change the distribution of the data on the Spark cluster. This distribution change will induce shuffle (physical data movement) under the hood, which is quite an expensive operation. In this article, we have seen some examples in which this additional shuffle can however remove some other shuffles at the same time and thus make the overall execution more efficient. We have also seen that it is important to distinguish between two kinds of shuffles \u2014 a total shuffle (which moves all the data) and a reduced shuffle (which moves the data after partial aggregation). Sometimes deciding what is ultimately more efficient requires understanding the properties of the actual data.</p>"},{"location":"tools/etl/spark/spark-repartitioning/#references","title":"References","text":"<ul> <li>https://towardsdatascience.com/should-i-repartition-836f7842298c</li> <li>https://medium.com/@tomhcorbin/repartitioning-in-spark-repartition-vs-coalesce-5e2fde5fa471</li> </ul>"},{"location":"tools/etl/spark/spark-udfs/","title":"UDFs","text":"<p>Developers often create custom User-Defined Functions (UDFs) in their Spark code to handle specific transformations. This allows users to develop personalized code for their unique data processing requirements.</p> <ul> <li>https://tsaiprabhanj.medium.com/spark-user-defined-functions-udfs-043f1bdcd09b</li> <li>https://medium.com/geekculture/mastering-pyspark-udfs-advantages-disadvantages-and-best-practices-e70f15b5f75c</li> </ul> <p>Warning</p> <p>Why you should Avoid using UDFs</p>"},{"location":"tools/etl/spark/spark-udfs/#getting-started","title":"Getting Started","text":""},{"location":"tools/etl/spark/spark-udfs/#standard-udf","title":"Standard UDF","text":"<pre><code>from pyspark.sql.types import IntegerType\n\ndef add_one(x):\n    return x + 1\n\nadd_one_udf = udf(add_one, IntegerType())\n\n# Apply traditional UDF\ndf_traditional_udf = df.withColumn(\"new_value\", add_one_udf(\"value\"))\n</code></pre>"},{"location":"tools/etl/spark/spark-udfs/#pandas-udf","title":"Pandas UDF","text":"SeriesIterator of SeriesIterator of Multiple SeriesScalar <pre><code>from pyspark.sql.functions import pandas_udf\nfrom pyspark.sql.types import DoubleType\n\n# UDF to apply state sales tax to the price column\n@pandas_udf(DoubleType())\ndef apply_sales_tax(price_series: pd.Series) -&gt; pd.Series:\n    sales_tax_rate = 0.07  # 7% sales tax\n    return price_series * (1 + sales_tax_rate)\n\n# Usage in Spark DataFrame\ndf = spark.createDataFrame([(1, 19.99), (2, 15.99), (3, 23.99)], [\"id\", \"price\"])\ndf_with_tax = df.withColumn(\"price_with_tax\", apply_sales_tax(df[\"price\"]))\n</code></pre> <pre><code>@pandas_udf('long')\ndef pandas_cumulative_sum(iterator: Iterator[pd.Series]) -&gt; Iterator[pd.Series]:\n    # Compute the cumulative sum for each partition (series) in the iterator\n    return (series.cumsum() for series in iterator)\n\n# Create a DataFrame with a sequence of numbers\ndf = spark.range(10)\n\n# Apply the cumulative sum UDF to the DataFrame\ndf_with_cumsum = df.withColumn(\"cumulative_sum\", pandas_cumulative_sum(\"id\"))\n\ndf_with_cumsum.show()\n</code></pre> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import pandas_udf\nfrom pyspark.sql.types import DoubleType\nfrom typing import Iterator, Tuple\nimport pandas as pd\n\n\n# Pandas UDF definition\n@pandas_udf(\"double\")\ndef calculate_total_sales(\n        iterator: Iterator[Tuple[pd.Series, pd.Series]]) -&gt; Iterator[pd.Series]:\n    return (quantity * price for quantity, price in iterator)\n\n# Create a dataframe for testing purpose\ndf = spark.createDataFrame([\n    (1, 4, 10.0),\n    (2, 2, 20.0),\n    (3, 1, 30.0),\n    (4, 5, 40.0),\n], [\"item_id\", \"quantity\", \"price_per_item\"])\n\n# Apply the UDF on total_sales\ndf_with_total_sales = df.withColumn(\"total_sales\", calculate_total_sales(df[\"quantity\"], df[\"price_per_item\"]))\n\ndf_with_total_sales.show()\n</code></pre> <pre><code>import pandas as pd\nfrom pyspark.sql.functions import pandas_udf\nfrom pyspark.sql import Window\n\ndf = spark.createDataFrame(\n    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"v\"))\n\n@pandas_udf(\"double\")\ndef pandas_mean(v: pd.Series) -&gt; float:\n    return v.sum()\n\ndf.select(pandas_mean(df['v'])).show()\n</code></pre>"},{"location":"tools/etl/spark/spark-udfs/#arrow-optimized-udf","title":"Arrow-Optimized UDF","text":"<p>Unveiling a paradigm shift in Apache Spark 3.5 and Databricks Runtime 14.0, the introduction of Arrow-optimized Python UDFs stands as a game-changer for performance enhancement.</p> <p>Arrow-optimized UDFs were introduced in Spark 3.5 to tackle the limitations of Pandas UDFs and UDFs by</p> <ul> <li>Better serialization/deserialization : Apache Arrow is used here for data serialization/deserialization as Pandas-udf. However, the limitation of only interacting with pandas API is broken.</li> <li>Consistent Type Handling: Apache Arrow provides a consistent and rich set of data types that are well-integrated with both Python and the JVM ecosystem.</li> <li>Type Coercion and Conversion: Arrow\u2019s type system helps in better managing type coercion and conversion. In traditional Pandas UDFs, mismatches between the defined types in UDFs and the actual returned data types could lead to errors or unexpected behavior. Arrow\u2019s robust type system reduces such issues by providing clear and well-defined type conversions.</li> <li>Enhanced Performance for Complex Types: Arrow\u2019s efficient representation of complex data types (like structs, lists, and maps) can lead to performance improvements especially for operations that involve these types. This is because Arrow can handle these types natively in its columnar format efficiently.</li> </ul> <pre><code>spark.conf.set(\"spark.sql.execution.pythonUDF.arrow.enabled\", \"true\")\n</code></pre> <pre><code>@udf(returnType=\"int\", useArrow=True)\ndef add_one_udf(x):\n    return x + 1\n\n# Apply traditional UDF\ndf_traditional_udf = df.withColumn(\"new_value\", add_one_udf(\"value\"))\n</code></pre>"},{"location":"tools/etl/spark/spark-udfs/#performance","title":"Performance","text":"<p>(De)Serialization for Python UDF is carried out as a separate process which takes additional time for its completion. Allow me to enhance the clarity of the performance bottleneck by referring to a diagram from one of my previous posts which explains the under the hood processes involved in converting Spark Dataframe to a Pandas Dataframe.</p> <p></p> <p>I hope the above diagram shows the additional process required in (de)serializing the data using pickle format. Since all the records needs to be moved to driver for serialization, most of the cases that involves huge data volume doesn't fit the dataframe within driver memory and might fail. Though this can be fixed through different options, it needs additional effort.</p>"},{"location":"tools/etl/spark/spark-udfs/#references","title":"References","text":"<p>However, it's important to remember that UDF aren\u2019t optimized by Spark\u2019s Catalyst optimizer. Therefore, whenever feasible, use Spark native transformations functions available in <code>spark.sql.functions</code>.</p> <ul> <li>Fast-Track PySpark UDF execution with Apache Arrow</li> <li>Arrow-optimized Python UDFs in Apache Spark 3.5</li> <li>Apache Spark : A comparative overview of UDF, pandas-UDF and arrow-optimized UDF</li> </ul>"},{"location":"tools/etl/spark/spark-why-success/","title":"Spark: Why _SUCCESS file","text":"<p>https://towardsdev.com/why-does-spark-write-output-a-success-file-83662e43b515</p>"},{"location":"tools/etl/spark/spark-windows/","title":"Spark: Windows","text":""},{"location":"tools/etl/spark/spark-windows/#type-of-windows","title":"Type of Windows","text":"<ul> <li>Tumbling windows are good for simple aggregations within fixed intervals.</li> <li>Sliding windows provide more context for analysis across overlapping time periods.</li> <li>Session windows are ideal for analyzing user behavior or activity logs based on periods of activity and inactivity.</li> </ul>"},{"location":"tools/etl/spark/spark-windows/#references","title":"References","text":"<ul> <li>https://medium.com/@ashutoshkumar2048/apache-spark-structured-streaming-windows-203fa1c961ea</li> <li>https://towardsdatascience.com/5-examples-to-master-pyspark-window-operations-26583066e227</li> </ul>"},{"location":"tools/etl/spark/deploy/spark-on-docker/","title":"On Docker","text":""},{"location":"tools/etl/spark/deploy/spark-on-docker/#references","title":"References","text":"<ul> <li>https://dev.to/mvillarrealb/creating-a-spark-standalone-cluster-with-docker-and-docker-compose-2021-update-6l4</li> </ul>"},{"location":"tools/etl/spark/deploy/spark-on-k8s/","title":"On Kubernetes","text":"<p>As of Apache Spark 3.1 release in March 2021, spark on kubernetes is generally available. This is great in so many ways, including but not limited to:</p> <ul> <li>you have full control of your infra</li> <li>takes less than 10 seconds to start a spark cluster</li> <li>can store logs in a central location, to be viewed later via spark history server</li> <li>can use minio as local storage backend (better throughput compared to calling S3 via home/work internet)</li> <li>cheaper than all managed solutions, even serverless variants (more on this later)</li> </ul>"},{"location":"tools/etl/spark/deploy/spark-on-k8s/#references","title":"References","text":"<ul> <li>Spark on Kubernetes</li> </ul>"},{"location":"tools/etl/spark/deploy/spark-on-local-with-dataproc-serverless/","title":"Pyspark: Local with Dataproc Serverless","text":"<p>https://levelup.gitconnected.com/setting-up-a-pyspark-local-developmet-environment-for-dataproc-serverless-cc7d05779e7d</p>"},{"location":"tools/etl/spark/deploy/spark-on-local/","title":"On Local","text":""},{"location":"tools/etl/spark/deploy/spark-on-local/#getting-started","title":"Getting Started","text":""},{"location":"tools/etl/spark/deploy/spark-on-local/#references","title":"References","text":"<ul> <li>https://medium.com/@midogax272/spark-submit-with-pyspark-and-aws-emr-serverless-6-9-0-aa451c3961e5</li> <li>Spark Essentials: A Guide to Setting Up, Packaging, and Running PySpark Projects</li> </ul>"},{"location":"tools/etl/spark/optimizations/","title":"Spark: Optimization","text":""},{"location":"tools/etl/spark/optimizations/#performance-issues","title":"Performance Issues","text":"<ul> <li>Data Skew</li> <li>Data Spill</li> <li>Serialization</li> <li>Storage</li> <li>Shuffling</li> </ul>"},{"location":"tools/etl/spark/optimizations/#references","title":"References","text":"<ul> <li> The 5 Spark Performance Issues</li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-data-skew/","title":"Spark: Data Skew","text":"<p>Data skew in Spark occurs when one or a few partitions have much more data than others. It usually happens during shuffling operations (like <code>.join</code> or <code>.agg</code>) when a disproportionate amount of data gets assigned to certain keys.</p> <p>Skewed data can lead to a few tasks taking much longer to run than others, resulting in inefficient resource utilization and increased overall processing time.</p> <p>Consequences of Data Skew:</p> <ul> <li> <p>Performance Bottleneck: Tasks dealing with larger partitions take disproportionately   longer to complete, causing other tasks and resources to idle.</p> </li> <li> <p>Out of Memory (OOM) Errors: Excessive data in certain tasks can lead to memory   overflow, causing OOM errors and task failures.</p> </li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-data-skew/#handling-data-skew","title":"Handling Data Skew","text":""},{"location":"tools/etl/spark/optimizations/spark-data-skew/#salting-keys","title":"Salting Keys","text":"<p>Salting involves adding a random value to the join key, which helps distribute the data more evenly across the partitions.</p> <p>Note</p> <p>Use salting in join operations where you have a skewed key. By appending a random value to the key, you can prevent a large number of values from being mapped to the same key.</p> <p>Example</p> <p>If joining on a highly skewed customer ID, append a random number (like customer ID + \u201c_1\u201d, customer ID + \u201c_2\u201d, etc.) to distribute the load.</p> <p>Quote</p> <p>Problem: A significant data skew is present with customer_id = 123.</p> <p>Original Datasets:</p> ordersDFcustomersDF <pre><code>| order_id | customer_id | amount |\n+----------+-------------+--------+\n| O1       | 123         | 150    |\n| O2       | 123         | 200    |\n| O3       | 456         | 50     |\n| O4       | 789         | 100    |\n| O5       | 123         | 300    |\n</code></pre> <pre><code>| customer_id | name      |\n+-------------+-----------+\n| 123         | John Doe  |\n| 456         | Tom Smith |\n| 789         | Bob Brown |\n</code></pre> <p>Applying Salting Technique:</p> <ol> <li> <p>Modify customer_id by appending a random number</p> ordersDFcustomersDF <pre><code>| order_id | customer_id_salted | amount |\n+----------+--------------------+--------+\n| O1       | 123_1              | 150    |\n| O2       | 123_2              | 200    |\n| O3       | 456_1              | 50     |\n| O4       | 789_1              | 100    |\n| O5       | 123_1              | 300    |\n</code></pre> <pre><code>| customer_id_salted | name      |\n+--------------------+-----------+\n| 123_1              | John Doe  |\n| 123_2              | John Doe  |\n| 456_1              | Tom Smith |\n| 789_1              | Bob Brown |\n</code></pre> </li> <li> <p>Perform the Join Operation</p> <p>After joining <code>ordersDF</code> with <code>customersDF</code> on <code>customer_id_salted</code>, the skew towards <code>customer_id = 123</code> is reduced as the orders for this customer are now distributed across two keys (<code>123_1</code> and <code>123_2</code>).</p> </li> <li> <p>Remove the Salted Component</p> <p>After joining, you can remove the appended random number to get the original <code>customer_id</code>.</p> </li> </ol>"},{"location":"tools/etl/spark/optimizations/spark-data-skew/#increasing-parallelism","title":"Increasing Parallelism","text":"<p>Increasing the level of parallelism can help distribute skewed data across more partitions.</p> <p>Note</p> <p>Adjust the <code>spark.default.parallelism</code> and <code>spark.sql.shuffle.partitions</code> properties to increase the number of partitions.</p> <p>Example</p> <p>For a large skewed dataset, increasing the shuffle partitions can help distribute data more evenly across the cluster.</p> <p>Quote</p> <p>Problem: A significant data skew is present in the sales dataset, particularly for a few region IDs, causing uneven load distribution.</p> <p>Original Datasets:</p> salesDFregionsDF <pre><code>| sale_id | region_id | amount |\n+---------+-----------+--------+\n| S1      | R1        | 500    |\n| S2      | R1        | 450    |\n| S3      | R2        | 300    |\n| S4      | R3        | 200    |\n| S5      | R1        | 550    |\n</code></pre> <pre><code>| region_id | name  |\n+-----------+-------+\n| 1         | North |\n| 2         | South |\n| 3         | East  |\n</code></pre> <p>Increasing Parallelism:</p> <ol> <li> <p>Adjust Spark Configurations</p> <p>Increase the number of partitions by adjusting <code>spark.sql.shuffle.partitions</code>. Suppose we increase it from the default value to a higher number, say 50.</p> </li> <li> <p>Perform the Join Operation</p> <p>After adjusting the number of partitions, perform the join operation. The sales data will now be distributed across more partitions, reducing the load on individual tasks.</p> </li> </ol> <p>Effect of Increased Parallelism:</p> <ul> <li>By increasing the number of partitions, the data for the skewed key (<code>region_id = R1</code>)   is spread across more tasks, preventing any single task from being overloaded.</li> <li>This results in more uniform execution times across tasks and better resource   utilization, improving overall performance.</li> </ul> <p>Increasing parallelism is a straightforward way to mitigate the impact of data skew in Spark applications. It does not alter the data itself but optimizes how data is distributed and processed across the cluster. This strategy is particularly effective when dealing with large datasets with skewed distributions, as it can lead to more balanced workload distribution and prevent bottlenecks in specific tasks.</p>"},{"location":"tools/etl/spark/optimizations/spark-data-skew/#broadcast-join-for-skewed-keys","title":"Broadcast Join for Skewed Keys","text":"<p>For a skewed join where one side of the join is much smaller than the other, broadcasting the smaller table can prevent skew.</p> <p>Note</p> <p>Force a broadcast join for the smaller DataFrame to avoid shuffling the larger DataFrame.</p> <p>Example</p> <p>In a join between a large transactions table and a small customers table, broadcasting the customers table can avoid data skew.</p> <p>Quote</p> <p>Problem: A significant data skew is present in the product sales dataset for a few customer IDs, leading to inefficient processing during the join operation.</p> <p>Original Datasets:</p> salesDFcustomersDF <pre><code>| sale_id | customer_id | amount |\n+---------+-------------+--------+\n| S1      | C123        | 600    |\n| S2      | C123        | 450    |\n| S3      | C456        | 150    |\n| S4      | C789        | 200    |\n| S5      | C123        | 250    |\n</code></pre> <pre><code>| customer_id | name      |\n+-------------+-----------+\n| C123        | John Doe  |\n| C456        | Tom Smith |\n| C789        | Bob Brown |\n</code></pre> <p>Using Broadcast Join:</p> <ol> <li> <p>Identify the Smaller DataFrame</p> <p>The <code>customersDF</code> is identified as the smaller DataFrame suitable for broadcasting.</p> </li> <li> <p>Perform the Broadcast Join</p> <p>Explicitly broadcast the <code>customersDF</code> during the join operation with <code>salesDF</code>.</p> <pre><code>from pyspark.sql.functions import broadcast\n\njoinedDF = (\n    salesDF.join(\n        broadcast(customersDF),\n        on=(salesDF.customer_id == customersDF.customer_id)\n    )\n)\n</code></pre> </li> </ol> <p>Effect of Broadcast Join:</p> <ul> <li>By broadcasting the smaller <code>customersDF</code>, Spark sends this DataFrame to each   node only once, significantly reducing the shuffle needed for the join.</li> <li>The skew in <code>salesDF</code> is less impactful, as the larger dataset does not need   to be shuffled. This results in a more efficient join operation, especially   when the skew is present in the larger DataFrame.</li> </ul> <p>Utilizing a Broadcast Join is particularly effective in scenarios where one of the DataFrames involved in the join is significantly smaller and can fit into the memory of each worker node. This technique is a powerful tool to combat the challenges posed by data skew, as it avoids the expensive operation of shuffling the larger skewed DataFrame, leading to more efficient and faster join operations in Spark.</p>"},{"location":"tools/etl/spark/optimizations/spark-data-skew/#filtering-and-splitting-skewed-keys","title":"Filtering and Splitting Skewed Keys","text":"<p>Identify skewed keys and process them separately from the rest of the data.</p> <p>Note</p> <p>Filter out the skewed keys, process them separately, and then union the result with the rest of the processed data.</p> <p>Example</p> <p>If a particular customer ID is responsible for most of the data, filter out this ID, perform necessary operations, and then union the result with the operations performed on the rest of the data.</p> <p>Quote</p> <p>Problem: In an e-commerce order dataset, there is significant skew due to a few customers placing an unusually high number of orders, resulting in inefficient processing during join operations.</p> <p>Original Datasets:</p> ordersDFcustomersDF <pre><code>| order_id | customer_id | amount |\n+----------+-------------+--------+\n| O1       | 123         | 150    |\n| O2       | 123         | 200    |\n| O3       | 456         | 50     |\n| O4       | 789         | 100    |\n| O5       | 123         | 300    |\n</code></pre> <pre><code>| customer_id | name      |\n+-------------+-----------+\n| 123         | John Doe  |\n| 456         | Tom Smith |\n| 789         | Bob Brown |\n</code></pre> <p>Applying Filtering and Splitting Skewed Keys:</p> <ol> <li> <p>Identify the Skewed Key</p> <p>Detect the skewed key (e.g., <code>customer_id = 123</code>) causing the skew.</p> </li> <li> <p>Split and Process the Data</p> </li> <li> <p>Filter out the skewed key from the main DataFrame and process these records separately.</p> </li> <li>Perform join operations on the non-skewed part and the skewed part independently.</li> <li>Union the results to get the final output.</li> </ol> <p>Effect of Filtering and Splitting Skewed Keys:</p> <ul> <li>By separating the skewed data (customer_id = C100), the join operation is more balanced and efficient.</li> <li>The skewed part can be processed using specialized strategies, such as increasing parallelism or using broadcast joins, to manage the heavier load.</li> <li>This approach leads to better performance and resource utilization during the join operation.</li> </ul> <p>Filtering and splitting skewed keys is an effective strategy to address data skew in Spark. It involves identifying the skew-causing keys, processing the skewed and non-skewed data separately, and then combining the results. This method not only alleviates the impact of skew on performance but also offers the flexibility to apply different optimization techniques to the skewed subset of the data. It\u2019s particularly useful in scenarios where a small subset of keys disproportionately affects the performance of join operations.</p>"},{"location":"tools/etl/spark/optimizations/spark-data-skew/#custom-partitioning","title":"Custom Partitioning","text":"<p>Use custom partitioning logic to distribute data more evenly across partitions.</p> <p>Note</p> <p>Implement a custom partitioner that can distribute data more uniformly, even when dealing with skewed keys.</p> <p>Example</p> <p>Create a partitioner that assigns data to partitions based on a custom logic that accounts for the skewness in the key distribution.</p> <p>Quote</p> <p>Problem: In a large transaction dataset, there is significant skew due to a few customers having a disproportionately high number of transactions, leading to inefficient processing during join operations.</p> transactionsDFcustomersDF <pre><code>| transaction_id | customer_id | amount |\n+----------------+-------------+--------+\n| T1             | 123         | 150    |\n| T2             | 123         | 200    |\n| T3             | 456         | 50     |\n| T4             | 789         | 100    |\n| T5             | 123         | 300    |\n</code></pre> <pre><code>| customer_id | name    |\n+-------------+---------+\n| 123         | Alice   |\n| 456         | Bob     |\n| 789         | Charlie |\n</code></pre> <p>Applying Custom Partitioning:</p> <ol> <li> <p>Define Custom Partitioning Logic</p> <p>Create a custom partitioner that distributes the data based on some logic that accounts for the skew. For example, you might create partitions based on a hash of the <code>customer_id</code> or use some domain knowledge to distribute the data more evenly.</p> </li> <li> <p>Apply Custom Partitioning to DataFrames</p> <p>Apply this custom partitioning to both <code>transactionsDF</code> and <code>customerDF</code> before the join operation. This ensures that the data for each customer is distributed across multiple partitions if needed.</p> </li> </ol> <p>Effect of Custom Partitioning:</p> <ul> <li>By using custom partitioning, the transactions for the heavily skewed <code>customer_id</code> (e.g., C001)   are distributed across multiple partitions.</li> <li>This results in a more even distribution of the workload across the cluster,   preventing individual tasks from becoming bottlenecks due to processing a large portion of skewed data.</li> <li>Custom partitioning leads to better parallelism and more efficient utilization   of cluster resources during the join operation.</li> </ul> <p>Custom partitioning in Spark is a powerful technique to address data skew, especially in large-scale join operations. By distributing data more evenly across partitions based on a custom-defined logic, it mitigates the impact of skew and ensures more balanced processing. This approach requires an understanding of the data distribution and might involve some experimentation to identify the most effective partitioning strategy. However, when properly implemented, custom partitioning can significantly enhance the performance and efficiency of Spark applications dealing with skewed datasets.</p>"},{"location":"tools/etl/spark/optimizations/spark-data-skew/#references","title":"References","text":"<ul> <li> Spark - Data Skew Odyssey Conquering the Chaos</li> <li> Spark Data Skew Solution (With Examples)</li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-data-spill/","title":"Spark: Data Spill","text":"<p>Data Spill in Spark refers to the scenario where data being processed does not fit into memory and has to be spilled to disk. This usually happens during operations that require a lot of memory, like shuffles, sorts, and aggregations.</p> <p>Spilling to disk is much slower than processing in memory, leading to longer processing times.</p> <p>Consequences of Data Spill:</p> <ul> <li> <p>Slowed Down Processing: Disk I/O is significantly slower than memory operations,   slowing down the overall application.</p> </li> <li> <p>Increased Disk I/O: Frequent spilling increases disk I/O, which can affect the   lifespan of the hardware and lead to increased maintenance.</p> </li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-data-spill/#handling-data-spill","title":"Handling Data Spill","text":"<ul> <li> <p>Memory Management: Tuning memory-related configurations like <code>spark.memory.fraction</code>   to optimize the amount of memory available for different operations.</p> </li> <li> <p>Optimizing Operations: Refactoring operations to be more memory-efficient, such   as using more efficient data structures or algorithms.</p> </li> <li> <p>Adjusting Partition Size: Increasing or decreasing the number of partitions to   ensure that data fits comfortably in memory.</p> </li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-joining/","title":"Spark: Joining","text":""},{"location":"tools/etl/spark/optimizations/spark-joining/#broadcast-hash-join","title":"Broadcast Hash Join","text":"<p>Utilizes the broadcasting of a smaller DataFrame to all worker nodes, thus avoiding the need to shuffle the larger DataFrame. This strategy significantly improves performance, especially for joins between a large and a small DataFrame.</p> <p>Use Case: Best used when joining a small reference or lookup table with a larger fact table.</p> <p>Example: Joining a small product catalog DataFrame with a large sales transactions DataFrame, where broadcasting the product catalog allows for efficient joins at each node.</p>"},{"location":"tools/etl/spark/optimizations/spark-joining/#sortmerge-joins","title":"SortMerge Joins","text":"<p>It consists of hashing each row on both table and shuffle the rows with the same hash into the same partition. There the keys are sorted on both side and the <code>sortMerge</code> algorithm is applied.</p> <p>To drastically speed up your sortMerges, write your large datasets as a Hive table with pre-bucketing and pre-sorting option (same number of partitions) instead of flat parquet dataset.</p> <p>Use Case: Ideal for large datasets with relatively uniform distribution and when broadcasting is not feasible.</p> <p>Example: Merging two large datasets, such as customer data and transaction data, where both are sorted by customer ID for efficient merging.</p> <pre><code>(\n    df.repartition(2200, \"colA\", \"colB\")\n        .write\n        .bucketBy(2200, \"col_A\", \"col_B\")\n        .sortBy(\"col_A\", \"col_B\")\n        .mode(\"overwrite\")\n        .format(\"parquet\")\n        .saveAsTable(\"schema.large_tbl\")\n)\n</code></pre> <p>The overhead cost of writing pre-bucketed/pre-sorted table is modest compared to the benefits.</p> <p>The underlying dataset will still be parquet by default, but the Hive metastore (can be Glue metastore on AWS) will contain precious information about how the table is structured. Because all possible \"joinable\" rows are colocated, Spark won't shuffle the tables that are pre-bucketd (big savings!) and won't sort the rows within the partition of table that are pre-sorted.</p> <p>Warning</p> <p>Spark bucketing is incompatible with hive bucketing</p> <ul> <li>https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53</li> <li>https://subhamkharwal.medium.com/pyspark-optimize-joins-in-spark-804fb098d4ee</li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-joining/#shuffle-hash-join","title":"Shuffle Hash Join","text":"<p>Involves partitioning and shuffling data based on the join key, then building a hash table for the smaller partition to join with the larger one. It\u2019s a compromise between broadcast and sort-merge joins.</p> <p>Use Case: Suitable for datasets where one is significantly smaller than the other, but still too large to broadcast. Example: Joining customer demographic data with a larger dataset of customer transactions, where the demographic data forms the smaller partition.</p>"},{"location":"tools/etl/spark/optimizations/spark-joining/#skewed-join","title":"Skewed Join","text":"<p>Tackles the issue of data skewness, where certain keys are overly represented. Spark implements techniques to split and replicate these skewed keys, balancing the data distribution across nodes.</p> <p>Use Case: Useful when dealing with large datasets that have uneven key distribution, which can lead to performance bottlenecks. Example: In a join operation between a customer orders table and a products table, if a few products account for most of the orders, a skewed join can distribute the load more evenly.</p>"},{"location":"tools/etl/spark/optimizations/spark-joining/#conditional-join","title":"Conditional Join","text":"<p>This approach is used for joins based on complex conditions beyond simple key equality, incorporating multiple conditions or sophisticated business logic.</p> <p>Use Case: Appropriate for scenarios where the relationship between tables is defined by more than just key matching. Example: Joining a sales table with a customer table where the join condition includes multiple factors like customer demographics, purchase history, and recent activity.</p>"},{"location":"tools/etl/spark/optimizations/spark-joining/#broadcast-nested-loop-join","title":"Broadcast Nested Loop Join","text":"<p>Applied when both DataFrames are small and involve a nested loop join, where each row of one DataFrame is compared with all rows of the other DataFrame.</p> <p>Use Case: Suitable for small datasets or when other join strategies are not applicable. Example: Joining two small datasets where one contains discount information and the other contains product categories, and the relationship does not depend on a specific key.</p>"},{"location":"tools/etl/spark/optimizations/spark-joining/#cartesian-product-cross-join","title":"Cartesian Product (Cross Join)","text":"<p>Produces a Cartesian product of the datasets; every row of the first DataFrame is paired with every row of the second DataFrame. It is a resource-intensive operation.</p> <p>Use Case: Used when every combination of rows from the datasets needs to be considered, though generally avoided due to its high computational cost. Example: Generating all possible combinations of two small datasets, such as colors and sizes for a clothing line.</p>"},{"location":"tools/etl/spark/optimizations/spark-joining/#bucketed-joins","title":"Bucketed Joins","text":"<p>Bucketing involves organizing data into fixed-size buckets based on a hash function of the join key. When two DataFrames are bucketed on the same columns, Spark can efficiently join them by matching bucket IDs, reducing shuffles.</p> <p>Use Case: Effective for frequent joins on the same key columns, especially in iterative ETL processes. Example: If both customer and transaction DataFrames are bucketed by customer ID, joins between these DataFrames can be more efficient, as Spark can bypass the shuffle phase.</p>"},{"location":"tools/etl/spark/optimizations/spark-joining/#z-order-join-space-filling-curve-optimization","title":"Z-Order Join (Space-filling Curve Optimization)","text":"<p>This technique involves organizing data using a Z-order curve to colocate related information. By doing so, range queries and certain types of joins are optimized, reducing the amount of data scanned.</p> <p>Use Case: Beneficial for multidimensional data, particularly when range queries are common. Example: In a geospatial analysis scenario, joining geographic data organized by Z-order can significantly reduce the data scanned, making queries for a specific region more efficient.</p>"},{"location":"tools/etl/spark/optimizations/spark-joining/#references","title":"References","text":"<ul> <li>https://towardsdatascience.com/the-art-of-joining-in-spark-dcbd33d693c</li> <li>https://towardsdatascience.com/strategies-of-spark-join-c0e7b4572bcf</li> <li> Spark Joining Strategy Part I</li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-serialization/","title":"Spark: Serialization","text":"<p>Serialization in Spark involves converting data objects into a format suitable for storage or transmission. It\u2019s essential for moving data across the network, especially during shuffling or when persisting data to disk. The choice of serialization framework (Java serialization vs. Kryo serialization) and how efficiently data is serialized have a substantial impact on the performance of Spark applications.</p> <p>Consequences of Inefficient Serialization:</p> <ul> <li>Increased Data Size: Poor serialization can significantly increase the size of the data being transferred or stored, leading to higher network and storage overhead.</li> <li>Performance Overhead: Inefficient serialization can slow down the process of data transfer and increase the overall runtime of Spark jobs.</li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-serialization/#handling-serialization","title":"Handling Serialization","text":"<ul> <li>Use Kryo Serialization: Kryo is generally faster and more compact than Java serialization. Configuring Spark to use Kryo can optimize performance.</li> <li>Optimize Serializable Classes: Ensure that classes are designed for efficient serialization, avoiding the serialization of unnecessary data.</li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-shuffling/","title":"Spark: Shuffling","text":"<p>Shuffling is the process of redistributing data across different partitions and nodes. It occurs during operations like <code>.join</code>, <code>.groupBy</code>, and <code>.reduceBy</code>. Shuffling is one of the most resource-intensive operations in Spark.</p> <p>Consequences of Excessive Shuffling:</p> <ul> <li>Network Overhead: Shuffling involves moving large amounts of data over the network, which can be time-consuming and lead to bottlenecks.</li> <li>Disk I/O: Excessive shuffling can result in significant disk I/O when data is spilled to disk, slowing down the processing.</li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-shuffling/#handling-shuffling","title":"Handling Shuffling","text":"<ul> <li>Minimize Shuffling Operations: Restructuring Spark jobs to reduce the need for shuffling, such as using map-side joins.</li> <li>Tuning Partition Sizes: Adjusting the number of partitions to balance load and reduce unnecessary data movement.</li> <li> <p>Custom Partitioners: Using custom partitioners to control the distribution of data and minimize shuffling.</p> </li> <li> <p></p> </li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-shuffling/#references","title":"References","text":""},{"location":"tools/etl/spark/optimizations/spark-storage/","title":"Spark: Storage","text":"<p>Storage optimization in Spark revolves around how data is stored, both in-memory and on disk. This includes the choice of data formats (like Parquet, ORC) and the efficient usage of caching and persistence mechanisms.</p> <p>Consequences of Suboptimal Storage:</p> <ul> <li>Slower Data Access: Inefficient data formats or storage strategies can lead to slower read/write operations.</li> <li>Increased Resource Consumption: Poorly optimized storage can consume more memory or disk space, affecting the overall efficiency of Spark applications.</li> </ul>"},{"location":"tools/etl/spark/optimizations/spark-storage/#handling-storage","title":"Handling Storage","text":"<ul> <li>Choose Efficient Data Formats: Formats like Parquet and ORC are optimized for performance, offering advantages like columnar storage and data compression.</li> <li>Effective Use of Caching: Caching data in memory can speed up access, but it needs to be done judiciously to avoid excessive memory consumption.</li> </ul>"},{"location":"tools/etl/spark/pyspark/pyspark-avoid-these-at-any-cost/","title":"Pyspark: Avoid These at Any Cost","text":"<p>https://medium.com/@think-data/avoid-these-at-any-cost-in-pyspark-13c55c98c95c</p>"},{"location":"tools/etl/spark/pyspark/pyspark-data-wrangling-functions/","title":"Pyspark: Data Wrangling Functions","text":"<p>https://towardsdatascience.com/best-data-wrangling-functions-in-pyspark-3e903727319e</p>"},{"location":"tools/etl/spark/pyspark/pyspark-media-files/","title":"Pyspark Handling Media Files","text":"<p>Update: <code>2023-05-08</code> | Type: <code>Use-Cases</code> | Tag: <code>Big Data</code> <code>Spark</code> <code>PySpark</code> <code>Binary File</code></p> <p>Pyspark provides several APIs to deal with image, audio, and video files. In this article we will discuss some ways to handle these files in PySpark.</p> <p>Table of Contents:</p> <ul> <li>Basic Features</li> <li>Additional Features provided by PySpark API</li> <li>Advantages of Using PySpark</li> </ul>"},{"location":"tools/etl/spark/pyspark/pyspark-media-files/#basic-features","title":"Basic Features","text":"<p>It just basic ways to handle these files. Depending on the specific use case, you may need to perform additional operations such as resizing images, extracting audio features, or processing video frames.</p>"},{"location":"tools/etl/spark/pyspark/pyspark-media-files/#image-files","title":"Image Files","text":"<pre><code>from pyspark.ml.image import ImageSchema\nfrom PIL import Image\n\n# Read image file\nimage = Image.open(\"path/to/image.jpg\")\n\n# Convert to PySpark DataFrame\ndf = ImageSchema.readImages(\"path/to/image.jpg\")\n</code></pre>"},{"location":"tools/etl/spark/pyspark/pyspark-media-files/#audio-files","title":"Audio Files","text":"<pre><code>from pyspark.sql.functions import udf\nfrom pyspark.sql.types import BinaryType\nfrom pydub import AudioSegment\n\n# Define a UDF to read audio file\n@udf(returnType=BinaryType())\ndef read_audio_file(path):\n    audio = AudioSegment.from_file(path)\n    return audio.export(format=\"wav\").read()\n\n# Read audio file and convert to PySpark DataFrame\ndf = (\n    spark.read.format(\"binaryFile\")\n        .load(\"path/to/audio.mp3\")\n        .selectExpr(\"path\", \"read_audio_file(content) as audio_data\")\n)\n</code></pre>"},{"location":"tools/etl/spark/pyspark/pyspark-media-files/#video-files","title":"Video Files","text":"<pre><code>import cv2\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import BinaryType\n\n# Define a UDF to read video file\n@udf(returnType=BinaryType())\ndef read_video_file(path):\n    cap = cv2.VideoCapture(path)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frames = []\n    for i in range(frame_count):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frames.append(frame)\n    cap.release()\n    return frames\n\n# Read video file and convert to PySpark DataFrame\ndf = (\n    spark.read.format(\"binaryFile\")\n        .load(\"path/to/video.mp4\")\n        .selectExpr(\"path\", \"read_video_file(content) as video_data\")\n)\n</code></pre>"},{"location":"tools/etl/spark/pyspark/pyspark-media-files/#additional-features-provided-by-pyspark-api","title":"Additional Features provided by PySpark API","text":"<p>In addition to reading and converting image, audio, and video files to PySpark DataFrames, there are several other operations that you can perform on these files in PySpark.</p>"},{"location":"tools/etl/spark/pyspark/pyspark-media-files/#image-files_1","title":"Image Files","text":"<ul> <li>Resize images</li> </ul> <pre><code>from pyspark.ml.image import ImageSchema\nfrom PIL import Image\n\n# Read image file\nimage = Image.open(\"path/to/image.jpg\")\n\n# Resize image\nresized_image = image.resize((224, 224))\n\n# Convert to PySpark DataFrame\ndf = ImageSchema.readImages(\"path/to/image.jpg\")\n</code></pre> <ul> <li>Convert images to different formats</li> </ul> <pre><code>from pyspark.ml.image import ImageSchema\nfrom PIL import Image\n\n# Read image file\nimage = Image.open(\"path/to/image.jpg\")\n\n# Convert to PNG format\nimage.save(\"path/to/image.png\")\n\n# Convert to PySpark DataFrame\ndf = ImageSchema.readImages(\"path/to/image.png\")\n</code></pre> <ul> <li>Extract image features</li> </ul> <pre><code>from pyspark.ml.image import ImageSchema\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\n\n# Read image file\ndf = ImageSchema.readImages(\"path/to/image.jpg\")\n\n# Load pre-trained VGG16 model\nmodel = VGG16(weights=\"imagenet\", include_top=False)\n\n# Preprocess input image\ndf = df.select(\"image.origin\", preprocess_input(\"image.data\").alias(\"features\"))\n\n# Extract image features\ndf = model.transform(df)\n</code></pre>"},{"location":"tools/etl/spark/pyspark/pyspark-media-files/#audio-files_1","title":"Audio Files","text":"<ul> <li>Extract audio features</li> </ul> <pre><code>from pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, DoubleType\nfrom pyAudioAnalysis import audioFeatureExtraction\n\n# Define a UDF to extract MFCC features from audio file\n@udf(returnType=ArrayType(DoubleType()))\ndef extract_mfcc_features(audio_data):\n    return audioFeatureExtraction.stFeatureExtraction(\n        audio_data, 44100, 44100, 0.050*44100, 0.025*44100\n    )[0].tolist()\n\n# Read audio file and convert to PySpark DataFrame\ndf = (\n    spark.read.format(\"binaryFile\")\n        .load(\"path/to/audio.wav\")\n        .selectExpr(\"path\", \"content\")\n)\n\n# Extract MFCC features\ndf = df.select(\"path\", extract_mfcc_features(\"content\").alias(\"features\"))\n</code></pre> <ul> <li>Convert audio files to different formats</li> </ul> <pre><code>from pyspark.sql.functions import udf\nfrom pyspark.sql.types import BinaryType\nfrom pydub import AudioSegment\n\n# Define a UDF to convert audio file to MP3 format\n@udf(returnType=BinaryType())\ndef convert_to_mp3(audio_data):\n    audio = AudioSegment.from_file(audio_data, format=\"wav\")\n    return audio.export(format=\"mp3\").read()\n\n# Read audio file and convert to PySpark DataFrame\ndf = spark.read.format(\"binaryFile\").load(\"path/to/audio.wav\").selectExpr(\"path\", \"content\")\n\n# Convert to MP3 format\ndf = df.select(\"path\", convert_to_mp3(\"content\").alias(\"audio_data\"))\n</code></pre> <ul> <li>Remove noise from audio files</li> </ul> <p>You can use techniques such as bandpass filtering, low-pass filtering, or high-pass   filtering to remove noise from audio files.</p>"},{"location":"tools/etl/spark/pyspark/pyspark-media-files/#video-files_1","title":"Video Files","text":"<ul> <li>Extract video frames</li> </ul> <pre><code>import cv2\nimport numpy as np\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, BinaryType\n\n# Define a UDF to extract video frames from video file\n@udf(returnType=ArrayType(BinaryType()))\ndef extract_video_frames(video_data):\n    cap = cv2.VideoCapture(video_data)\n    frames = []\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame = np.asarray(frame)\n        frames.append(frame.tobytes())\n    return frames\n\n# Read video file and convert to PySpark DataFrame\ndf = spark.read.format(\"binaryFile\").load(\"path/to/video.mp4\").selectExpr(\"path\", \"content\")\n\n# Extract video frames\ndf = df.select(\"path\", extract_video_frames(\"content\").alias(\"frames\"))\n</code></pre> <ul> <li>Apply video filters</li> </ul> <pre><code>import cv2\nimport numpy as np\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import BinaryType\nfrom PIL import Image, ImageFilter\n\n# Define a UDF to apply a Gaussian blur filter to video frames\n@udf(returnType=BinaryType())\ndef apply_gaussian_blur(frame_data):\n    # Convert bytes to NumPy array\n    frame = np.frombuffer(frame_data, dtype=np.uint8).reshape((480, 640, 3))\n\n    # Apply Gaussian blur filter\n    img = Image.fromarray(frame)\n    img = img.filter(ImageFilter.GaussianBlur(radius=5))\n    frame = np.asarray(img)\n\n    # Convert back to bytes\n    return frame.tobytes()\n\n# Read video file and convert to PySpark DataFrame\ndf = spark.read.format(\"binaryFile\").load(\"path/to/video.mp4\").selectExpr(\"path\", \"content\")\n\n# Apply Gaussian blur filter to video frames\ndf = df.select(\"path\", apply_gaussian_blur(\"content\").alias(\"frame_data\"))\n</code></pre> <ul> <li>Perform object detection</li> </ul> <pre><code>import cv2\nimport numpy as np\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, BinaryType\nfrom tensorflow.keras.models import load_model\n\n# Load pre-trained object detection model\nmodel = load_model(\"path/to/object_detection_model.h5\")\n\n# Define a UDF to perform object detection on video frames\n@udf(returnType=ArrayType(BinaryType()))\ndef perform_object_detection(frame_data):\n    # Convert bytes to NumPy array\n    frame = np.frombuffer(frame_data, dtype=np.uint8).reshape((480, 640, 3))\n\n    # Perform object detection\n    detections = model.detect(frame)\n\n    # Draw bounding boxes on the frame\n    for detection in detections:\n        x, y, w, h = detection[\"box\"]\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n    # Convert back to bytes\n    return frame.tobytes()\n\n# Read video file and convert to PySpark DataFrame\ndf = spark.read.format(\"binaryFile\").load(\"path/to/video.mp4\").selectExpr(\"path\", \"content\")\n\n# Perform object detection on video frames\ndf = df.select(\"path\", perform_object_detection(\"content\").alias(\"frame_data\"))\n</code></pre>"},{"location":"tools/etl/spark/pyspark/pyspark-media-files/#advantages-of-using-pyspark","title":"Advantages of Using PySpark","text":"<p>Here are some advantages of using PySpark for processing image, audio, and video files:</p> <ul> <li> <p>Scalability:   PySpark is designed for distributed computing, which means that it can process   large files and datasets much faster than traditional computing frameworks.   This makes it an ideal choice for processing image, audio, and video files,   which can be very large.</p> </li> <li> <p>Performance:   PySpark is highly optimized for data processing, which means that it can perform   complex operations on large datasets very quickly. This is especially important   when working with image, audio, and video files, which often require computationally   intensive operations like feature extraction and object detection.</p> </li> <li> <p>Integration with other tools:   PySpark integrates with many popular data processing and machine learning tools,   like TensorFlow, Keras, and OpenCV. This makes it easy to incorporate these tools   into your data processing pipeline, and to leverage their capabilities for tasks   like image classification, object detection, and speech recognition.</p> </li> <li> <p>Flexible data sources:   PySpark can read data from a wide range of sources, including local files, distributed   file systems like Hadoop Distributed File System (HDFS), and cloud storage platforms   like Amazon S3 and Google Cloud Storage. This makes it easy to process image,   audio, and video files regardless of where they are stored.</p> </li> <li> <p>Unified API:   PySpark provides a unified API for processing different types of data, including   image, audio, and video files. This means that you can use the same set of APIs   and functions to process all of these different file types, which can simplify   your code and reduce development time.</p> </li> <li> <p>Fault tolerance:   PySpark is designed to be fault-tolerant, which means that it can recover from   failures and continue processing data even if some nodes fail. This is especially   important when processing large datasets, as the likelihood of a node failure   increases with the size of the dataset.</p> </li> <li> <p>Cost-effective:   PySpark is open-source and can be run on commodity hardware, which makes it a   cost-effective option for processing image, audio, and video files. Additionally,   PySpark can be run on cloud-based infrastructure, which allows you to scale your   processing resources up or down as needed, and to pay only for what you use.</p> </li> </ul>"},{"location":"tools/etl/spark/pyspark/pyspark-media-files/#references","title":"References","text":"<ul> <li>https://blog.devgenius.io/handling-media-files-in-pyspark-image-audio-video-files-8e3bcd7a5c4e</li> </ul>"},{"location":"tools/etl/spark/pyspark/pyspark-regexp/","title":"Pyspark Regular Expression","text":"<p>https://blog.devgenius.io/regular-expression-regexp-in-pyspark-e5f9b5d9617a</p>"},{"location":"tools/etl/spark/pyspark/pyspark-scd2/","title":"Pyspark SCD2","text":""},{"location":"tools/etl/spark/pyspark/pyspark-scd2/#references","title":"References","text":"<ul> <li>https://towardsdatascience.com/slowly-changing-dimension-type-2-in-spark-7d5865ac915b</li> <li>Mastering Slowly Changing Dimensions (SCD) Pythonic Way in Data Warehousing with PySpark and Delta Lake</li> </ul>"},{"location":"tools/etl/spark/pyspark/pyspark-select-and-selectexpr/","title":"Pyspark: select() and selectExpr()","text":"<p>https://medium.com/@ashwin_reddy_/spark-dataframe-transformations-select-and-selectexpr-bc5e00d7d04a</p>"},{"location":"tools/etl/spark/pyspark/pyspark-unittest/","title":"Pyspark: Unittest","text":""},{"location":"tools/etl/spark/pyspark/pyspark-unittest/#getting-started","title":"Getting Started","text":""},{"location":"tools/etl/spark/pyspark/pyspark-unittest/#create-fixtures","title":"Create Fixtures","text":"<pre><code>@pytest.fixture(scope=\"session\")\ndef spark():\n    print(\"----Setup Spark Session---\")\n    spark = (\n        SparkSession.builder.master(\"local[1]\")\n        .appName(\"Unit-Tests\")\n        .config(\"spark.executor.cores\", \"1\")\n        .config(\"spark.executor.instances\", \"1\")\n        .config(\"spark.port.maxRetries\", \"30\")\n        .config(\"spark.sql.shuffle.partitions\", \"1\")\n        .getOrCreate()\n    )\n    yield spark\n    print(\"--- Tear down Spark Session---\")\n    spark.stop()\n</code></pre> <pre><code>@pytest.fixture(scope=\"session\")\ndef input_data(spark):\n    input_schema = StructType(\n        [\n            StructField(\"StoreID\", IntegerType(), True),\n            StructField(\"Location\", StringType(), True),\n            StructField(\"Date\", StringType(), True),\n            StructField(\"ItemCount\", IntegerType(), True),\n        ]\n    )\n    input_data = [\n        (1, \"Bangalore\", \"2021-12-01\", 5),\n        (2, \"Bangalore\", \"2021-12-01\", 3),\n        (5, \"Amsterdam\", \"2021-12-02\", 10),\n        (6, \"Amsterdam\", \"2021-12-01\", 1),\n        (8, \"Warsaw\", \"2021-12-02\", 15),\n        (7, \"Warsaw\", \"2021-12-01\", 99),\n    ]\n    input_df = spark.createDataFrame(data=input_data, schema=input_schema)\n    return input_df\n</code></pre> <pre><code>@pytest.fixture(scope=\"session\")\ndef expected_data(spark):\n    # Define an expected data frame\n    expected_schema = StructType(\n        [\n            StructField(\"Location\", StringType(), True),\n            StructField(\"TotalItemCount\", IntegerType(), True),\n        ]\n    )\n    expected_data = [(\"Bangalore\", 8), (\"Warsaw\", 114), (\"Amsterdam\", 11)]\n    expected_df = spark.createDataFrame(data=expected_data, schema=expected_schema)\n    return expected_df\n</code></pre>"},{"location":"tools/etl/spark/pyspark/pyspark-unittest/#create-test-case","title":"Create Test Case","text":"<pre><code>def test_etl(spark, input_data, expected_data):\n    # Apply transforamtion on the input data frame\n    transformed_df = transform_data(input_data)\n\n    # Compare schema of transformed_df and expected_df\n    field_list = lambda fields: (fields.name, fields.dataType, fields.nullable)\n    fields1 = [*map(field_list, transformed_df.schema.fields)]\n    fields2 = [*map(field_list, expected_data.schema.fields)]\n    res = set(fields1) == set(fields2)\n\n    # assert\n    # Compare data in transformed_df and expected_df\n    assert sorted(expected_data.collect()) == sorted(transformed_df.collect())\n</code></pre>"},{"location":"tools/etl/spark/pyspark/pyspark-unittest/#auto-cicd","title":"Auto CICD","text":"azure-pipeline.yml<pre><code>name: Py Spark Unit Tests\n\npool:\n  vmImage: ubuntu-latest\n\nstages:\n  - stage: Tests\n    displayName: Unit Tests using Pytest\n\n    jobs:\n      - job:\n        displayName: PySpark Unit Tests\n        steps:\n          - script: |\n              sudo apt-get update\n              sudo apt-get install default-jdk -y\n              pip install -r $(System.DefaultWorkingDirectory)/src/tests/test-requirements.txt\n              pip install --upgrade pytest pytest-azurepipelines\n              cd src &amp;&amp; pytest -v -rf --test-run-title='Unit Tests Report'\n            displayName: Run Unit Tests\n</code></pre>"},{"location":"tools/etl/spark/pyspark/pyspark-unittest/#references","title":"References","text":"<ul> <li>https://medium.com/@xavier211192/how-to-write-pyspark-unit-tests-ci-with-pytest-61ad517f2027</li> </ul>"},{"location":"tools/etl/spark/rdd/spark-rdd-foreach-foreachpartition/","title":"Spark RDD: Map &amp; MapPartitions","text":""},{"location":"tools/etl/spark/rdd/spark-rdd-foreach-foreachpartition/#foreach","title":"Foreach","text":"<p>foreach is a PySpark RDD (Resilient Distributed Datasets) action that applies a function to each element of an RDD. It is used to perform some side-effecting operations, such as writing output to a file, sending data to a database, or printing data to the console. The function passed to foreach should have a void return type, meaning it does not return anything.</p> <pre><code>rdd = sc.parallelize([1, 2, 3, 4, 5])\ndef print_num(num):\n    print(num)\nrdd.foreach(print_num)\n\n# OUTPUT\n1\n2\n3\n4\n5\n</code></pre>"},{"location":"tools/etl/spark/rdd/spark-rdd-foreach-foreachpartition/#foreach-partition","title":"Foreach Partition","text":"<p>foreachPartition is similar to foreach, but it applies the function to each partition of the RDD, rather than each element. This can be useful when you want to perform some operation on a partition as a whole, rather than on each element individually. The function passed to foreachPartition should have a void return type.</p> <pre><code>rdd = sc.parallelize([1, 2, 3, 4, 5], 2)\ndef print_partition(iter):\n    for num in iter:\n        print(num)\nrdd.foreachPartition(print_partition)\n\n# OUTPUT\n1\n2\n3\n4\n5\n</code></pre>"},{"location":"tools/etl/spark/rdd/spark-rdd-foreach-foreachpartition/#use-cases","title":"Use Cases","text":"<ul> <li>Writing data to external systems: foreach and foreachPartition are often used   to write the output of a PySpark job to an external system such as a file, database,   or message queue. For example, you could use foreach to write each element of   an RDD to a file, or use foreachPartition to write each partition to a separate   file.</li> <li>Sending data to an external service: Similarly, foreach and foreachPartition can be used to send data to an external service for further analysis. For example, you could use foreach to send each element of an RDD to a web service, or use foreachPartition to send each partition to a separate service.</li> <li>Performing custom processing: foreachPartition can be useful when you want to perform some custom processing on each partition of an RDD. For example, you might want to calculate some summary statistics for each partition or perform some machine learning model training on each partition separately.</li> <li>Debugging and logging: foreach and foreachPartition can be used for debugging and logging purposes. For example, you could use foreach to print the output of each element to the console for debugging purposes, or use foreachPartition to log each partition to a separate file for debugging and monitoring purposes.</li> <li>Performing complex side-effecting operations: Finally, foreach and foreachPartition can be used to perform complex side-effecting operations that cannot be expressed using built-in PySpark transformations. For example, you could use foreach to perform some custom analysis on each element of an RDD, or use foreachPartition to perform some complex transformation on each partition.</li> </ul> <p>It\u2019s worth noting that foreach and foreachPartition are actions, meaning they trigger the execution of the computation on the RDD. Therefore, they should be used sparingly, as they can result in significant overhead and slow down the computation. It's usually better to use transformations like map, filter, and reduceByKey to perform operations on RDDs, and only use foreach and foreachPartition when necessary.</p>"},{"location":"tools/etl/spark/rdd/spark-rdd-foreach-foreachpartition/#references","title":"References","text":""},{"location":"tools/etl/spark/rdd/spark-rdd-map-mappartition/","title":"Spark RDD: Map &amp; MapPartitions","text":"<p><code>map()</code> and <code>mapPartitions()</code> are two transformation operations in PySpark that are used to process and transform data in a distributed manner.</p>"},{"location":"tools/etl/spark/rdd/spark-rdd-map-mappartition/#map","title":"Map","text":"<p><code>map()</code> is a transformation operation that applies a function to each element of an RDD independently and returns a new RDD. The function applied to each element should be a pure function, which means it should not have any side effects and should return a new value based on the input value.</p> MapFilter <pre><code>rdd = sc.parallelize([1, 2, 3, 4])\nsquared_rdd = rdd.map(lambda x: x**2)\nsquared_rdd.collect()\n\n# output\n[1, 4, 9, 16]\n</code></pre> <pre><code>rdd = sc.parallelize([\"This is a sentence.\", \"Another sentence.\", \"Yet another sentence.\"])\nfiltered_rdd = rdd.filter(lambda x: \"sentence\" in x)\nfiltered_rdd.collect()\n\n# output\n['This is a sentence.', 'Another sentence.', 'Yet another sentence.']\n</code></pre>"},{"location":"tools/etl/spark/rdd/spark-rdd-map-mappartition/#map-partitions","title":"Map Partitions","text":"<p><code>mapPartitions()</code> is also a transformation operation that applies a function to each partition of an RDD. Unlike <code>map()</code>, the function applied to each partition is executed once for each partition, not once for each element. This can be useful when the processing of each partition requires some initialization or setup that can be done once for each partition, instead of for each element.</p> MapPartitionMapPartition with Func <pre><code>rdd = sc.parallelize([1, 2, 3, 4], 2)\ndef sum_partition(iterator):\n    yield sum(iterator)\n\nsum_rdd = rdd.mapPartitions(sum_partition)\nsum_rdd.collect()\n\n# output\n[3, 7]\n</code></pre> <pre><code>rdd = sc.parallelize([(1, 2), (3, 4), (5, 6), (7, 8)], 2)\ndef average_partition(iterator):\n     x_sum = 0\n     y_sum = 0\n     count = 0\n     for (x, y) in iterator:\n         x_sum += x\n         y_sum += y\n         count += 1\n     yield (x_sum/count, y_sum/count)\n\navg_rdd = rdd.mapPartitions(average_partition)\navg_rdd.collect()\n\n# output\n[(2.0, 3.0), (6.0, 7.0)]\n</code></pre>"},{"location":"tools/etl/spark/rdd/spark-rdd-map-mappartition/#difference-between-map-and-mappartitions","title":"Difference between map() and mapPartitions()","text":"<p>The main difference between <code>map()</code> and <code>mapPartitions()</code> is that <code>map()</code> applies a function to each element of an RDD independently, while <code>mapPartitions()</code> applies a function to each partition of an RDD. Therefore, <code>map()</code> is more suitable when the processing of each element is independent, while <code>mapPartitions()</code> is more suitable when the processing of each partition requires some initialization or setup that can be done once for each partition.</p> <p>The choice between the two depends on the nature of the data and the processing required for each element or partition.</p>"},{"location":"tools/etl/spark/rdd/spark-rdd-map-mappartition/#references","title":"References","text":"<ul> <li>Understanding PySpark Transformations: Map and MapPartitions Explained</li> </ul>"},{"location":"tools/etl/spark/stream/","title":"Apache Spark: Structured Stream","text":""},{"location":"tools/etl/spark/stream/#etl","title":"ETL","text":"<p>```python titles=\"Spark-streaming-medallion\" class Bronze():     def init(self):         self.base_data_dir = \"/FileStore/spark_structured_streaming\"</p> <pre><code>def getSchema(self):\n    return \"\"\"InvoiceNumber string, CreatedTime bigint, StoreID string, PosID string, CashierID string,\n            CustomerType string, CustomerCardNo string, TotalAmount double, NumberOfItems bigint,\n            PaymentMethod string, TaxableAmount double, CGST double, SGST double, CESS double,\n            DeliveryType string,\n            DeliveryAddress struct&lt;AddressLine string, City string, ContactNumber string, PinCode string,\n            State string&gt;,\n            InvoiceLineItems array&lt;struct&lt;ItemCode string, ItemDescription string,\n                ItemPrice double, ItemQty bigint, TotalValue double&gt;&gt;\n        \"\"\"\n\ndef readInvoices(self):\n    return (\n        spark.readStream\n            .format(\"json\")\n            .schema(self.getSchema())\n            #.option(\"cleanSource\", \"delete\")\n            .option(\"cleanSource\", \"archive\")\n            .option(\"sourceArchiveDir\", f\"{self.base_data_dir}/data/invoices_archive\")\n            .load(f\"{self.base_data_dir}/data/invoices\")\n    )\n\ndef process(self):\n    print(f\"\\nStarting Bronze Stream...\", end='')\n    invoicesDF = self.readInvoices()\n    sQuery =  ( invoicesDF.writeStream\n                        .queryName(\"bronze-ingestion\")\n                        .option(\"checkpointLocation\", f\"{self.base_data_dir}/chekpoint/invoices_bz\")\n                        .outputMode(\"append\")\n                        .toTable(\"invoices_bz\")\n                )\n    print(\"Done\")\n    return sQuery\n</code></pre> <p>class Silver():     def init(self):         self.base_data_dir = \"/FileStore/spark_structured_streaming\"</p> <pre><code>def readInvoices(self):\n    return (\n        spark.readStream\n            .table(\"invoices_bz\")\n    )\n\ndef explodeInvoices(self, invoiceDF):\n    return ( invoiceDF.selectExpr(\"InvoiceNumber\", \"CreatedTime\", \"StoreID\", \"PosID\",\n                                  \"CustomerType\", \"PaymentMethod\", \"DeliveryType\", \"DeliveryAddress.City\",\n                                  \"DeliveryAddress.State\",\"DeliveryAddress.PinCode\",\n                                  \"explode(InvoiceLineItems) as LineItem\")\n                                )\n\ndef flattenInvoices(self, explodedDF):\n    from pyspark.sql.functions import expr\n    return( explodedDF.withColumn(\"ItemCode\", expr(\"LineItem.ItemCode\"))\n                    .withColumn(\"ItemDescription\", expr(\"LineItem.ItemDescription\"))\n                    .withColumn(\"ItemPrice\", expr(\"LineItem.ItemPrice\"))\n                    .withColumn(\"ItemQty\", expr(\"LineItem.ItemQty\"))\n                    .withColumn(\"TotalValue\", expr(\"LineItem.TotalValue\"))\n                    .drop(\"LineItem\")\n            )\n\ndef appendInvoices(self, flattenedDF):\n    return (flattenedDF.writeStream\n                .queryName(\"silver-processing\")\n                .format(\"delta\")\n                .option(\"checkpointLocation\", f\"{self.base_data_dir}/chekpoint/invoice_line_items\")\n                .outputMode(\"append\")\n                .toTable(\"invoice_line_items\")\n    )\n\ndef process(self):\n       print(f\"\\nStarting Silver Stream...\", end='')\n       invoicesDF = self.readInvoices()\n       explodedDF = self.explodeInvoices(invoicesDF)\n       resultDF = self.flattenInvoices(explodedDF)\n       sQuery = self.appendInvoices(resultDF)\n       print(\"Done\\n\")\n       return sQuery\n</code></pre> <p><code></code>python titles=\"Spark_streaming_medallion_test_suite\" %run ../SparkStreaming/Spark-streaming-medallion  class medallionApproachTestSuite:     def init(self):         self.base_data_dir = \"/FileStore/spark_structured_streaming\"      def cleanTests(self):         print(f\"Starting Cleanup...\", end='')         spark.sql(\"drop table if exists invoices_bz\")         spark.sql(\"drop table if exists invoice_line_items\")         dbutils.fs.rm(\"/user/hive/warehouse/invoices_bz\", True)         dbutils.fs.rm(\"/user/hive/warehouse/invoice_line_items\", True)          dbutils.fs.rm(f\"{self.base_data_dir}/chekpoint/invoices_bz\", True)         dbutils.fs.rm(f\"{self.base_data_dir}/chekpoint/invoice_line_items\", True)          dbutils.fs.rm(f\"{self.base_data_dir}/data/invoices_archive\", True)         dbutils.fs.rm(f\"{self.base_data_dir}/data/invoices\", True)         dbutils.fs.mkdirs(f\"{self.base_data_dir}/data/invoices\")         print(\"Done\")      def ingestData(self, itr):         print(f\"\\tStarting Ingestion...\", end='')         dbutils.fs.cp(f\"{self.base_data_dir}/invoices/invoices_{itr}.json\", f\"{self.base_data_dir}/data/invoices/\")         print(\"Done\")      def assertResult(self, expected_count):         print(f\"\\tStarting validation...\", end='')         actual_count = spark.sql(\"select count(*) from invoice_line_items\").collect()[0][0]         assert expected_count == actual_count, f\"Test failed! actual count is {actual_count}\"         print(\"Done\")      def waitForMicroBatch(self, sleep=30):         import time         print(f\"\\tWaiting for {sleep} seconds...\", end='')         time.sleep(sleep)         print(\"Done.\")      def runTests(self):         self.cleanTests()         bzStream = Bronze()         bzQuery = bzStream.process()          slStream = Silver()         slQuery = slStream.process()          print(\"\\nTesting first iteration of invoice stream...\")         self.ingestData(1)         self.waitForMicroBatch()         self.assertResult(1253)         print(\"Validation passed.\\n\")          print(\"Testing second iteration of invoice stream...\")         self.ingestData(2)         self.waitForMicroBatch()         self.assertResult(2510)         print(\"Validation passed.\\n\")          print(\"Testing third iteration of invoice stream...\")         self.ingestData(3)         self.waitForMicroBatch()         self.assertResult(3990)         print(\"Validation passed.\\n\")          bzQuery.stop()         slQuery.stop()          print(\"Validating Archive...\", end=\"\")         archives_expected = [\"invoices_1.json\", \"invoices_2.json\"]         for f in dbutils.fs.ls(f\"{self.base_data_dir}/data/invoices_archive/{self.base_data_dir}/data/invoices\"):             assert f.name in archives_expected, f\"Archive Validation failed for {f.name}\"         print(\"Done\") </p>"},{"location":"tools/etl/spark/stream/#command-","title":"COMMAND ----------","text":"<p>maTS = medallionApproachTestSuite() maTS.runTests() ```</p>"},{"location":"tools/etl/spark/stream/#optimization","title":"Optimization","text":"<p>Optimizing Spark Structured Streaming 5 Tips - Optimizing Spark Structured Streaming Apps</p>"},{"location":"tools/etl/spark/stream/spark-stream-aggregate/","title":"Spark Stream: Aggregate","text":""},{"location":"tools/etl/spark/stream/spark-stream-aggregate/#references","title":"References","text":"<ul> <li>https://medium.com/@vndhya/how-aggregation-works-end-to-end-in-spark-structured-streaming-7ebe99d09ceb</li> </ul>"},{"location":"tools/etl/spark/stream/spark-stream-checkpoint/","title":"Checkpoint","text":"<p>Checkpoint directories are of much help for Spark Streaming applications in order to keep track of the metadata. In order to make sure that Spark Streaming Application resumes from the same point where it stopped, we definitely need to define Checkpoint directory in our application.</p>"},{"location":"tools/etl/spark/stream/spark-stream-checkpoint/#content-of-checkpoint-directory","title":"Content of Checkpoint Directory","text":"<ul> <li>Metadata: file keep tracks of the Query Id for the Streaming application.</li> <li>Sources: keeps track of the Micro batch data sources for the application.</li> <li>Offsets: keeps tracks of the Micro batch data that is being read and Processed.</li> <li>Commits: keeps track of the Micro batch that is processed successfully.</li> </ul>"},{"location":"tools/etl/spark/stream/spark-stream-checkpoint/#references","title":"References","text":"<ul> <li>Spark Streaming Checkpoint Directory</li> </ul>"},{"location":"tools/etl/spark/stream/spark-stream-foreach-batch/","title":"Spark Stream: For Each Batch","text":"<p>https://www.waitingforcode.com/apache-spark-structured-streaming/apache-spark-2.4.0-features-foreachbatch/read#use_case_multiple_sinks_from_single_source_side_output</p>"},{"location":"tools/etl/spark/stream/spark-stream-ml/","title":"Spark Streaming: ML","text":"<p>MLlib is Apache Spark\u2019s scalable machine learning library consisting of common learning algorithms and utilities.</p> <p>Tested with 4 Classifiers:</p> <pre><code>1. Decision Tree\n2. LogisticRegression\n3. NaiveBayes\n4. RandomForest\n</code></pre>"},{"location":"tools/etl/spark/stream/spark-stream-ml/#set-pipeline","title":"Set Pipeline","text":"<pre><code>// Indesing Label\nval labelIndexer = new StringIndexer()\n        .setInputCol(\"flower\")\n        .setOutputCol(\"indexedFlower\")\n        .fit(transformed_data)\n\n// Automatically identify categorical features, and index them.\nval featureIndexer = new VectorIndexer()\n        .setInputCol(\"features\")\n        .setOutputCol(\"indexedFeatures\")\n        .setMaxCategories(4)\n        .fit(transformed_data)\n\n// Convert indexed labels back to original labels.\nval labelConverter = new IndexToString()\n        .setInputCol(\"prediction\")\n        .setOutputCol(\"predictedLabel\")\n        .setLabels(labelIndexer.labels)\n\n// Declaring ML Pipeline\nval pipeline = new Pipeline()\n</code></pre> Decision TreeLogistic RegressionNaive BayesRandom Forest <pre><code>// Train a DecisionTree\nval dt: DecisionTreeClassifier = new DecisionTreeClassifier()\n        .setLabelCol(\"indexedFlower\")\n        .setFeaturesCol(\"indexedFeatures\")\n\n// Setting stages in the ML Pipeline\npipeline.setStages(Array(labelIndexer, featureIndexer, dt, labelConverter))\n</code></pre> <pre><code>// Train the Logistic Regression.\nval lr: LogisticRegression = new LogisticRegression()\n        .setLabelCol(\"indexedFlower\")\n        .setMaxIter(10)\n        .setRegParam(0.3)\n        .setElasticNetParam(0.8)\n        .setFamily(\"multinomial\")\n\n// Setting stages in the ML Pipeline\npipeline.setStages(Array(labelIndexer, featureIndexer, lr, labelConverter))\n</code></pre> <pre><code>// Train the Naive Bayes\nval nb: NaiveBayes = new NaiveBayes().setLabelCol(\"indexedFlower\")\n\n// Setting stages in the ML Pipeline\npipeline.setStages(Array(labelIndexer, featureIndexer, nb, labelConverter))\n</code></pre> <pre><code>// Train a RandomForest\nval rf: RandomForestClassifier = new RandomForestClassifier()\n        .setLabelCol(\"indexedFlower\")\n        .setFeaturesCol(\"indexedFeatures\")\n        .setNumTrees(10)\n\n// Setting stages in the ML Pipeline\npipeline.setStages(Array(labelIndexer, featureIndexer, rf, labelConverter))\n</code></pre>"},{"location":"tools/etl/spark/stream/spark-stream-ml/#references","title":"References","text":"<ul> <li>Machine Learning with Spark Streaming</li> </ul>"},{"location":"tools/etl/spark/stream/spark-stream-multi-query/","title":"Spark Stream: Multi Query","text":"<p>Note</p> <p>If you are looking for writing the same dataset to many different sinks, you should consider the foreachBatch sink.</p> <pre><code>val filesWithNumbers = sparkSession.readStream.text(s\"${basePath}/data\").as[Int]\n\nval multipliedBy2 = filesWithNumbers.map(nr =&gt; nr * 2)\nval multipliedBy2Writer = multipliedBy2.writeStream.format(\"json\")\n    .option(\"path\", s\"${basePath}/output/sink-1\")\n    .option(\"checkpointLocation\", s\"${basePath}/checkpoint/sink-1\")\n    .start()\n\nval multipliedBy3 = filesWithNumbers.map(nr =&gt; nr * 3)\nval multipliedBy3Writer = multipliedBy3.writeStream.format(\"json\")\n    .option(\"path\", s\"${basePath}/output/sink-2\")\n    .option(\"checkpointLocation\", s\"${basePath}/checkpoint/sink-2\")\n    .start()\n\nsparkSession.streams.awaitAnyTermination()\n</code></pre> <ul> <li>https://www.waitingforcode.com/apache-spark-structured-streaming/multiple-queries-running-apache-spark-structured-streaming/read</li> <li>Spark Structured Streaming: Multiple Sinks</li> </ul>"},{"location":"tools/etl/spark/stream/spark-stream-read-files/","title":"Spark Stream: Read from Files","text":""},{"location":"tools/etl/spark/stream/spark-stream-read-files/#getting-started","title":"Getting Started","text":""},{"location":"tools/etl/spark/stream/spark-stream-read-files/#create-spark-session","title":"Create Spark Session","text":"<pre><code># Create the Spark Session\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession\n        .builder\n        .appName(\"Streaming Process Files\")\n        .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n        .master(\"local[*]\")\n        .getOrCreate()\n)\n</code></pre>"},{"location":"tools/etl/spark/stream/spark-stream-read-files/#references","title":"References","text":"<ul> <li>Structured Streaming Read from Files</li> <li>Structured Streaming for multi format scalable Data Ingestion Workloads</li> </ul>"},{"location":"tools/etl/spark/stream/spark-stream-to-eventhubs/","title":"Spark Stream: To EventHubs","text":"<p>https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/spark-streaming-eventhubs-integration.md</p>"},{"location":"tools/etl/spark/stream/spark-stream-watermark/","title":"Spark Stream: Watermark","text":"<ul> <li>Watermarking in Spark Structured Streaming</li> </ul>"},{"location":"tools/git/","title":"Git","text":"<p>Quote</p> <p>Git is an Open Source Distributed Version Control System Designed to handle everything from small to very large projects with speed and efficiency.</p> <p> Git is the best version control tools for any developer, you can read the Official Documents.</p> <ul> <li>Save and Track different versions of your repositories</li> <li>Coordinate changes across different teams without impacting the work of other collaborators</li> <li>Share local copies of the same codebases as other developers when working offline</li> <li>Isolate new fixes and features in development without impacting production</li> </ul>"},{"location":"tools/git/#workflow","title":"Workflow","text":"<pre><code>Initial ---&gt; Working &lt;--&gt; Index/Staging &lt;--&gt; Repository\n</code></pre> InitialWorkingIndex/StagingRepository <pre><code>.git/\n  \u251c\u2500\u2500\u2500&gt; hooks/\n  \u251c\u2500\u2500\u2500&gt; info/\n  \u251c\u2500\u2500\u2500&gt; objects/\n  \u251c\u2500\u2500\u2500&gt; refs/\n  \u251c\u2500\u2500\u2500&gt; config\n  \u251c\u2500\u2500\u2500&gt; description\n  \u251c\u2500\u2500\u2500&gt; FETCH_HEAD\n  \u251c\u2500\u2500\u2500&gt; HEAD\n  \u2514\u2500\u2500\u2500&gt; index\n</code></pre> <pre><code>.git/\n  \u251c\u2500\u2500\u2500&gt; hooks/\n  \u251c\u2500\u2500\u2500&gt; info/\n  \u251c\u2500\u2500\u2500&gt; objects/\n  \u251c\u2500\u2500\u2500&gt; refs/\n  \u251c\u2500\u2500\u2500&gt; config\n  \u251c\u2500\u2500\u2500&gt; description\n  \u251c\u2500\u2500\u2500&gt; FETCH_HEAD\n  \u251c\u2500\u2500\u2500&gt; HEAD\n  \u2514\u2500\u2500\u2500&gt; index\n</code></pre> <pre><code>.git/\n  \u251c\u2500\u2500\u2500&gt; ...\n  \u251c\u2500\u2500\u2500&gt; objects/\n  |       \u251c\u2500\u2500\u2500&gt; 19/\n  |       |     \u2514\u2500\u2500\u2500&gt; 10283f238bc06de68f6a2ac63f3ec8c7a0dfef\n  |       \u251c\u2500\u2500\u2500&gt; info/\n  |       \u2514\u2500\u2500\u2500&gt; pack/\n  \u251c\u2500\u2500\u2500&gt; ...\n  \u2514\u2500\u2500\u2500&gt; index\n</code></pre> <pre><code>.git/\n  \u251c\u2500\u2500\u2500&gt; ...\n  \u251c\u2500\u2500\u2500&gt; objects/\n  |     \u251c\u2500\u2500\u2500&gt; 3c/\n  |     |     \u2514\u2500\u2500\u2500&gt; 6d5a4...\n  |     \u251c\u2500\u2500\u2500&gt; 19/\n  |     |     \u2514\u2500\u2500\u2500&gt; 10283f238bc06de68f6a2ac63f3ec8c7a0dfef\n  |     \u251c\u2500\u2500\u2500&gt; info/\n  |     \u2514\u2500\u2500\u2500&gt; pack/\n  \u251c\u2500\u2500\u2500&gt; ...\n  \u2514\u2500\u2500\u2500&gt; index\n</code></pre> <p>Details:</p> InitialWorkingIndex/StagingRepository <p>When you want to create <code>.git</code> file in your local project, you can use:</p> <pre><code>$ git init\n</code></pre> <p>Another command, <code>git init \u2013bare</code> will create and keep only configuration values of version control without source code.</p> <pre><code>$ echo -e \"Hello World\" &gt; demo.txt\n$ git hash-object demo.txt\n1910283f238bc06de68f6a2ac63f3ec8c7a0dfef\n</code></pre> <pre><code>$ git status\nOn branch main\n...\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to ...)\n      demo.txt\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n</code></pre> <p>Note</p> <p>Remove all untracked files that was created in working, <code>git clean -f</code>.</p> AddRestoreResetRevert <pre><code>$ git add .\n$ git status\nOn branch main\n...\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n      newfile:    demo.txt\n</code></pre> <p>Restore Added file that was added from staging to working</p> <pre><code># Restore Added file that was committed to repository, to working\n$ git retore --staged .\n</code></pre> <p>Reset all files that was added to staging</p> <pre><code>$ git reset\n$ git prune\n</code></pre> <p>Revert files that was added to staging and delete files in <code>object/</code></p> <pre><code>$ git add .\n$ git rm --cached &lt;filename&gt;\n$ git prune\n</code></pre> <pre><code>$ git ls-files\ndemo.txt\n</code></pre> <p>Note</p> <p>The file in <code>object/</code> was compressed, and you will see value in this file when you use <code>zlib</code> like: <code>blob 11&lt;nil&gt;Hello World</code>.</p> <p>Note</p> <p><code>git add -p &lt;file&gt;</code>, this command will split changed file to hunks of code for review and what to do with that hunk,</p> <p>such as <code>y n q a d / j J g e ?</code>.</p> <ul> <li><code>-y</code>: (yes) for add that hunk to staged zone</li> <li><code>-n</code>: (no) ignore to add this hunk</li> <li><code>-q</code>: (quit) quit this interactive with mode <code>git add -p</code></li> <li><code>-s</code>: (split) divide this hunk to smaller hunks</li> </ul> <p>Sub-Folder</p> <p>This command, <code>git add :/</code>, using when you stay in sub-folder of working area and want to add all changed files include outside to staged status.</p> CommitRestoreReset <pre><code>$ git commit -m \"initial\"\n[main (root-commit) 3c6d5a4] initial\n1 file changed, 1 insertion(+)\ncreate mode 100644 demo.txt\n</code></pre> <p>Restore Changed file that was committed to repository</p> <pre><code>$ git restore .\n</code></pre> <p>Re-position of HEAD to hash commit that was committed to repository</p> <pre><code>$ git reset --hard &lt;commit-hash&gt;\n</code></pre> <pre><code># Get type of Git file\n$ git cat-file -t 3c6d\ncommit\n\n# Get content of Git file\n$ git cat-file -p 3c6d\ntree a611c6cc...\nauthor ...\n...\n\n$ git cat-file -p a611\n100644 blob 1910283f238bc06de68f6a2ac63f3ec8c7a0dfef    demo.txt\n</code></pre> <p>Note</p> <p>In commit hash file, Git will add <code>parent</code> information if you add new commit after first commit.</p> <pre><code>$ git cat-file -p 463f\ntree 9f3dc...\nparent 3c6d5a4...\n...\n</code></pre> <p>Note</p> <p>The <code>git commit --amend</code> or <code>git commit --amend -m \"YOUR MESSAGE\"</code> command use for create new commit replace the latest commit.</p> <pre><code>$ git commit -am \"&lt;message&gt;\"\n$ git add .\n$ git commit --amend\n</code></pre> <p>This solution use only local repository, before push branch to remote repository. If it is not any changed file to Staged, this command will allow you to edit the latest Commit Message.</p> <p>Note</p> <p><code>git ldm</code> for list history commits that you do in local repository before daily standup meeting.</p>"},{"location":"tools/git/#common-command","title":"Common Command","text":""},{"location":"tools/git/#git-hash","title":"Git Hash","text":"<p>Git have build-in hash function that use for create file name in Git.</p> <pre><code>$ git hash-object &lt;filname&gt;.&lt;file-extension&gt;\naf5b63bf238bc06de68f6a2ac63f3ec8c7a0dfef\n</code></pre> Manual Hash <pre><code>$ echo -n \"&lt;type:[blob, tree, commit]&gt; &lt;content-length&gt;\\0&lt;content-information&gt;\" \\\n  | shasum\naf5b63bf238bc06de68f6a2ac63f3ec8c7a0dfef\n</code></pre> <p>You see the above hash process, Git does not create hash file name from the real filename, but it uses content information in the file.</p>"},{"location":"tools/git/#git-config","title":"Git Config","text":"<p>The <code>git config</code> will override config values in local from global configuration.</p> ConfigGlobalLocal <pre><code>$ git config user.name \"username\"\n$ git config --get user.name\n$ git config --list\n</code></pre> <pre><code>$ git config --global user.name \"username\"\n$ git config --global user.email \"username@email.com\"\n$ git config --list\n</code></pre> <pre><code>$ git config --local user.name \"username\"\n$ git config --local user.email \"username@email.com\"\n$ git config --list\n</code></pre> Config Editor <p>Configuration in Git can use: <code>git config</code> command line. The simple way to edit Git configuration is using editor like VS Code, Atom, or Vim by Git default.</p> <pre><code># Set Editor in Git config when you use -e option.\n$ git config --global core.editor \"code -w\"\n$ git config --global -e\nhint: Waiting for your editor to close the file...\n</code></pre> <p>Note</p> <p>If you want to set Atom editor, use</p> <pre><code>$ git config --global core.editor \"atom --wait\"\n</code></pre>"},{"location":"tools/git/#git-log","title":"Git Log","text":"<pre><code># Set Disable Git Log pager\n$ git config --globlal pager.log false\n$ git log\ncommit 3c6d5a4... (HEAD -&gt; main)\nAuthor: ...\n...\n</code></pre> <p>Filtering your commit history:</p> <ul> <li>by date - <code>--before</code> or <code>--after</code></li> <li>by message - <code>--grep</code></li> <li>by author - <code>--author</code></li> <li>by file - <code>-- ${filename}</code></li> <li>by branch - <code>${branch-name}</code></li> </ul> <pre><code>$ git log --after=\"2021-7-1\"\n$ git log --after=\"2021-7-1\" --before=\"2021-6-5\"\n$ git log --grep=\"refactor\"\n</code></pre>"},{"location":"tools/git/#git-diff","title":"Git Diff","text":"<pre><code>$ git diff 073c HEAD\n...\nindex 2a3ee71..84f5955 100644\n...\n</code></pre> Use <code>difftool</code> <pre><code>$ git config --global diff.tool vscode\n$ git config --global difftool.vscode.cmd \"code -w -d \\$LOCAL \\$REMOTE\"\n$ git difftool --staged\n...\nViewing (1/1): '&lt;filename&gt;'\nLauch 'vscode' [Y/n]?\n\n$ git config --global difftool.prompt false\n$ git difftool --staged\n</code></pre>"},{"location":"tools/git/#git-branch","title":"Git Branch","text":"ListCreateDelete <pre><code>$ git branch\n</code></pre> <p>Options:</p> <ul> <li><code>-a</code>, <code>git branch -a</code>: List all branchs on local and remote</li> </ul> <pre><code># Create new branch\n$ git branch &lt;branch-name&gt;\n\n# Create new branch and switch HEAD to this branch\n$ git checkout -b &lt;branch-name&gt;\n\n# Support on Git version &gt;= 2.2.3\n$ git switch -c &lt;branch-name&gt;\n</code></pre> <pre><code># Delete branch that does not any new commit\n$ git branch -d &lt;branch_name&gt;\n\n# Delete branch that was created new commit\n$ git branch -D &lt;branch_name&gt;\n$ git reflog expire --expire-unreachable=now --all\n$ git prune\n</code></pre> <p>Note</p> <p><code>git cleanup</code> for delete branches in local repository that was merged.</p> <p>Note</p> <p>If you want to check out previous branch, you can use: <code>git checkout -</code>.</p>"},{"location":"tools/git/#git-checkout","title":"Git Checkout","text":"<pre><code># Revert all modified files to clean status\n$ git checkout .\n\n# Switch to previous branch\n$ git checkout -\n\n# Revert to specific commit\n$ git checkout \"&lt;commit&gt;\"\n\n# Revert to specific file and commit\n$ git checkout \"&lt;commit&gt;\" &lt;file-name&gt;\n\n# Create new branch in local repository from existing branch in remote repository\n$ git checkout -b feature/xxx origin/feature/xxx\n</code></pre>"},{"location":"tools/git/#git-tag","title":"Git Tag","text":"<p>Git Tags are used to capture the specific point in the history that is further used to point to a released version. A tag does not change like a branch.</p> <pre><code>$ git log --oneline\n816998a &lt;commit-message&gt;\n7c576ab &lt;commit-message&gt;\ndd9a333 stable\n...\n\n# Switch HEAD to that commit\n$ git switch --detach dd9a333\n$ git tag alpha\n$ git switch --detach alpha\n</code></pre> <pre><code>$ git tag \"&lt;tag-name&gt;\" \"&lt;commit&gt;\"\n$ git show \"&lt;tag-name&gt;\"\n$ git tag --list\n$ git tag -d \"&lt;tag-name&gt;\"\n</code></pre> <p>Note</p> <ul> <li>Annotated tags - <code>git tag -a '&lt;tag-name&gt;' -m '&lt;message&gt;' HEAD</code></li> <li>Lightweight tags - <code>git tag &lt;tag-name&gt;</code></li> </ul> <pre><code>$ git ls-remote --tags\n$ git ls-remote --tags origin\n$ git push --delete origin \"&lt;tag-name&gt;\"\n$ git push origin :refs/tags/\"&lt;tag-name&gt;\"\n</code></pre> <p>NOTE: If you want to push tag on local repository to remote, you will use <code>git push my_remote \u2013tags</code></p>"},{"location":"tools/git/#git-stash","title":"Git Stash","text":"<p>The <code>git stash</code> does hide all changes, stash the changes in a dirty working directory away. The local repository will be clean because <code>git stash</code> will tell HEAD commit hash revert to any commit (the latest commit always dirty).</p> StashListApply <pre><code>$ touch file.txt\n$ git add .\n$ git stash\nSaved working directory and index state WIP on gh-pages: fe163ee update and done\n</code></pre> <p>Note</p> <p>You can change the name of stash by this command: <code>git stash save \"&lt;name&gt;\"</code>, and stash included untracked file: <code>git stash -u</code>.</p> <pre><code>$ git stash list\nstash@{0}: WIP on gh-pages: fe163ee update and done\n\n$ ls file.txt\nls: cannot access 'file.txt': No such file or directory\n</code></pre> <pre><code>$ git stash apply\n$ ls file.txt\nfile.txt\n\n$ git stash drop stash@{0}\n</code></pre> <p>Note</p> <p>Above command, <code>git stash apply stash@{0}</code>, apply the latest stash to working area and staged area but difference with <code>git stash pop</code> because <code>pop</code> will delete stash history in list.</p>"},{"location":"tools/git/#git-remote","title":"Git Remote","text":"Set RemotePush to RemoteFetch from RemotePull from Remote <pre><code>$ git remote add origin https://github.com/username/myproject.git\n$ git remote -v\norigin  https://github.com/username/myproject.git (fetch)\norigin  https://github.com/username/myproject.git (push)\n</code></pre> <pre><code>$ git push -u origin master\nEnumerating objects: 7, done.\n...\nTo https://github.com/username/myproject.git\n * [new branch]      master -&gt; master\nBranch 'master' set up to track remote branch 'master' from 'origin'.\n</code></pre> <p>Warning</p> <p>The command <code>git push origin master --force</code> for force push to remote repository that mean it does not use <code>git pull origin</code> command before push.</p> <p>Note</p> <p>Above remote is use <code>http</code> access, so it always pass username/password after <code>git push</code> command. The another way is use ssh for connect to remote.</p> <pre><code>$ git fetch\n...\nFrom https://github.com/username/myproject.git\n   5a30adf..ad*****  master     -&gt; origin/master\n\n$ git status\nOn branch master\nYour branch is behind 'origin/master' by 1 commit, and can be fast-forwarded.\n  (use \"git pull\" to update your local branch)\n\nnothing to commit, working tree clean\n</code></pre> <pre><code>$ git pull\nUpdating 5a30adf..adfa804\nFast-forward\n &lt;filename&gt; | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n</code></pre>"},{"location":"tools/git/#git-fork","title":"Git Fork","text":"<p>Git Fork is the GitHub's feature for task owner of remote repository, it seems like clone but the repository will be yours. If you want to sync update from original repository to yours repository, you will add <code>upstream</code>.</p> <pre><code>$ git remote add upstream https://github.com/&lt;original_owner&gt;/&lt;original_repo&gt;.git\n$ git remote -v\norigin  git@github.com:Phonbopit/bootstrap.git (fetch)\norigin  git@github.com:Phonbopit/bootstrap.git (push)\nupstream    git@github.com:Phonbopit/bootstrap.git (fetch)\nupstream    git@github.com:Phonbopit/bootstrap.git (push)\n</code></pre> <pre><code>$ git fetch upstream\n$ git checkout master\n$ git merge upstream/master\n</code></pre>"},{"location":"tools/git/#advance-command","title":"Advance Command","text":""},{"location":"tools/git/#git-revert","title":"Git Revert","text":"<p>Revert specific commit to new commit</p> <pre><code>$ git revert \"&lt;commit&gt;\"\n$ git revert -m 1 \"&lt;commit&gt;\"\n\n# Revert merge change\n$ git reset --hard HEAD^\n</code></pre>"},{"location":"tools/git/#git-merge-rebase","title":"Git Merge &amp; Rebase","text":"<p>Merging vs Rebasing</p> MergeMerge SquashMerge Rebase <ul> <li>Will keep all commits history of the feature branch and move them into the master branch</li> <li>Will add extra dummy commit.</li> </ul> <pre><code>$ git switch main\n$ git merge dev\n</code></pre> <ul> <li>Will group all feature branch commits into one commit then append it in the front of the master branch</li> <li>Will add extra dummy commit.</li> </ul> <pre><code>$ git merge --squash HEAD@{1}\n$ git checkout stable\n$ git merge --squash develop\n$ git commit -m \"squash develop\"\n</code></pre> <ul> <li>Will append all commits history of the feature branch in the front of the master branch</li> <li>Will NOT add extra dummy commit.</li> </ul> <pre><code># Rebase all commit from dev branch to main branch\n$ git switch dev\n$ git rebase main\n$ git rebase --continue\n$ git switch main\n$ git merge dev\n$ git branch -d dev\n</code></pre> <p>Note</p> <p>If you want to cancel the rebase process, you can use <code>git rebase --abort</code>.</p>"},{"location":"tools/git/#git-rebase-self","title":"Git Rebase Self","text":"<p>Instead, use it for cleaning up your local commit history before merging it into a shared team branch. <code>git rebase</code> will use for</p> <ul> <li>Change a commit message</li> <li>Delete/Reorder commits</li> <li>Combine multiple commits into one (squash)</li> <li>Edit/Split an existing commit into multiple new ones</li> </ul> <p>Warning</p> <p>Do NOT use Interactive Rebase on commits that you've already pushed/shared on a remote repository.</p> <pre><code>$ git rebase -i HEAD~3\npick adfa804 Update config.yaml\npick 81529d3 update config.yaml\npick 3d8cc7a Update config.yaml\n\n# Rebase 5a30adf..ae76b2e onto 5a30adf (3 commands)\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n# .       create a merge commit using the original merge commit's\n# .       message (or the oneline, if no original merge commit was\n# .       specified). Use -c &lt;commit&gt; to reword the commit message.\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n# Note that empty commits are commented out\n:q!\n\n$ git rebase --abort\n</code></pre> <pre><code>$ git log --oneline\nae76b2e (HEAD -&gt; master, origin/master) USE OURS ON CONFLICT FILE\n81529d3 update config.yaml\n3d8cc7a Update config.yaml\nadfa804 Update config.yaml\n5a30adf ADD CONFIG\n4e133da YOUR MESSAGE\n</code></pre> <p>Note</p> <p>When you want to control commit with rebase, <code>git rebase master --interactive</code></p> <p>Note</p> <p>If you want to squash all commit together, you can use <code>git merge --squash \"&lt;branch&gt;\"</code> command. (Or <code>git rebase -i --autosquash</code>)</p>"},{"location":"tools/git/#git-conflict","title":"Git Conflict","text":"<p>Note</p> <p>Above command can use opposite option, like <code>git checkout --theirs config.yaml</code>. Or use it together with merge strategy,</p> <ul> <li><code>git merge --strategy-option ours</code></li> <li><code>git merge --strategy-option theirs</code></li> </ul>"},{"location":"tools/git/#git-cherry-pick","title":"Git Cherry Pick","text":"<p>If you want to take some commit (such as bug fix commit) from another branch to your branch (Allows you to select individual commits to be integrated).</p> <p>Cherry Pick fixed file from dev to main branch</p> <pre><code>$ git add &lt;fix-filename&gt;\n$ git commit -m \"bug fix\"\n$ git switch main\n$ git log dev --oneline\n&lt;commit-hash&gt; (dev) bug fix\n...\n$ git cherry-pick \"&lt;commit-hash&gt;\"\n</code></pre> <p>Note</p> <p>git does not that delete commit from source branch and in your branch will be exists that commit in new id.</p>"},{"location":"tools/git/#git-submodules","title":"Git Submodules","text":"<p>Git Submodule help you to develop main project together with subproject and separate the commits of subproject from main project.</p> <p>The file <code>.submodule</code> is configuration of <code>git submodule</code> that keep project\u2019s URL, local subdirectory, and subproject branches that was tracked.</p> AddClonePushDelete <pre><code>$ git submodule add -b master https://github.com/username/module sub-project\n$ git status\nOn branch master\nYour branch is up to date with 'origin/master'.\nChanges to be committed:\n  (use \"git reset HEAD &lt;file&gt;...\" to unstage)\n      new file:   .gitmodules\n      new file:   sub-project\n\n$ cd sub-project\nsub-project$ ls\nREADME.md ...\n</code></pre> <pre><code>$ cat .gitmodules\n[submodule \"sub-project\"]\n    path = sub-project\n    url = https://github.com/username/sub-project.git\n    branch = master\n</code></pre> <p>Note</p> <p>When you already push, you will see submodule in main project repository and the submodule does not keep the source code but it keep hash commit number of submodule.</p> <pre><code>$ git clone https://github.com/username/mainproject.git\n$ cd sub-project\nsub-project$ ls\n\n\nsub-project$ git submodule init\nsub-project$ git submodule update\nCloning into '/username/mainproject/sub-project'...\nSubmodule path 'sub-project': checked out 'a5635f67626c1c224e733fe407aaa132b5e5d1e3\n</code></pre> <p>Note</p> <p>The <code>git clone --recurse-submodules \"&lt;repository-url&gt;\"</code> is auto initialize and update submodules.</p> <p><code>git submodule update --init --recursive</code></p> <pre><code>$ cd sub-project/\nsub-project$ git fetch\nsub-project$ git merge origin/master\nAlready up to date.\n</code></pre> <p>Note</p> <p><code>git submodule update --remote</code> will auto fetch and merge with track branch. If you do not set trank, you will use</p> <pre><code>username/myproject$ git config -f .gitmodules submodule.sub-project.branch develop\nusername/myproject$ cat .gitmodules\n[submodule \"sub-project\"]\n path = sub-project\n url = https://github.com/username/sub-project.git\n branch = develop\n</code></pre> <p>Optional, <code>git submodule update --remote --merge</code> or <code>git submodule update --remote --rebase</code></p> <pre><code>username/myproject/sub-project$ git commit -am 'update readme.md'\nusername/myproject/sub-project$ git push\n\nusername/myproject/sub-project$ cd ..\nusername/myproject$ git commit -am 'update submodule'\nusername/myproject$ git push\n</code></pre> <p>NOTE: You can push along with the main project, where the submodule is pushed before the main project is pushed using the command <code>git push \u2014 recurse-submodules=on-demand</code></p> <pre><code>username/myproject$ git submodule deinit -f sub-project\n&gt; Cleared directory 'sub-project'\n&gt; Submodule 'sub-project' (https://github.com/username/sub-project.git) unregistered for path 'sub-project'\n\nusername/myproject$ rm -rf .git/modules/sub-project\nusername/myproject$ git rm --cached sub-project\n&gt; rm 'sub-project'\n\nusername/myproject$ rm .gitmodules\nusername/myproject$ git commit -am \"REMOVED submodule\"\n&gt; [master 66b1703] removed submodule\n&gt;  2 files changed, 5 deletions(-)\n&gt;  delete mode 160000 sub-project\n\nusername/myproject$ git push origin master\n</code></pre>"},{"location":"tools/git/#git-reflog","title":"Git Reflog","text":"<p>A protocol of HEAD Pointer movements. Reflog is a mechanism to record when the tip of branches is updated. This command is to manage the information recorded in it.</p> <p>\u0e43\u0e0a\u0e49\u0e41\u0e2a\u0e14\u0e07 Log \u0e02\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e41\u0e1b\u0e25\u0e07\u0e43\u0e19 HEAD \u0e02\u0e2d\u0e07 Local Repository \u0e21\u0e31\u0e19\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e04\u0e49\u0e19\u0e2b\u0e32\u0e07\u0e32\u0e19\u0e17\u0e35\u0e48\u0e2a\u0e39\u0e0d\u0e2b\u0e32\u0e22\u0e44\u0e1b</p> <pre><code>username/myproject$ git reflog --all\n&gt; ae76b2e (HEAD -&gt; master, origin/master) HEAD@{0}: rebase -i (abort): updating HEAD\n&gt; 81529d3 HEAD@{1}: rebase -i (start): checkout HEAD~3\n&gt; ae76b2e (HEAD -&gt; master, origin/master) refs/remotes/origin/master@{0}: update by push\n&gt; ae76b2e (HEAD -&gt; master, origin/master) refs/heads/master@{0}: commit (merge): USE OURS ON CONFLICT FILE\n&gt; ae76b2e (HEAD -&gt; master, origin/master) HEAD@{2}: commit (merge): USE OURS ON CONFLICT FILE\n&gt; 81529d3 refs/heads/master@{1}: commit: update config.yaml\n&gt; 81529d3 HEAD@{3}: commit: update config.yaml\n&gt; adfa804 refs/heads/master@{2}: reset: moving to HEAD~1\n&gt; :\n</code></pre>"},{"location":"tools/git/#git-blame","title":"Git Blame","text":"<p>Use to track change inline of file.</p> <pre><code>$ git blame config.yaml\n5a30adf5 (usernaem    2022-03-06 13:52:04 +0700 1) env:\nadfa8044 (usernaemDEV 2022-03-06 19:30:39 +0700 2)   develop: \"/dev\"\nadfa8044 (usernaemDEV 2022-03-06 19:30:39 +0700 3)   remote: \"/remote\"\n81529d32 (usernaem    2022-03-07 11:19:06 +0700 4)   production: \"/prod\"\n</code></pre>"},{"location":"tools/git/#git-bisect","title":"Git Bisect","text":""},{"location":"tools/git/#start-bisect","title":"Start bisect","text":"<pre><code>username/myproject$ git bisect start\nusername/myproject$ git bisect bad \"&lt;commit-start&gt;\"\nusername/myproject$ git bisect good \"&lt;commit-end&gt;\"\n</code></pre>"},{"location":"tools/git/#unittest-until-the-latest-commit-exists","title":"Unittest until the latest commit exists","text":"<pre><code>username/myproject$ run app\nusername/myproject$ git bisect bad\n&gt; Bisecting: 7 revisions left to test after this (roughly 3 steps)\n&gt; [12sasa53261sdfas4235sdab721c2405abv0*****] COMMIT MESSAGE\n\n...\n\nusername/myproject$ run app\nusername/myproject$ git bisect bad\n&gt; Bisecting: 0 revisions left to test after this (roughly 0 steps)\n&gt; [jk1p1634sda78v93kla13asdfscc23140030*****] COMMIT MESSAGE\n\nusername/myproject$ git bisect bad\n&gt; 5a30adf5d4f184fd4586891e26d6826ab66***** COMMIT MESSAGE\n&gt; commit 5a30adf5d4f184fd4586891e26d6826ab66*****\n&gt; Author: username &lt;username@email.com&gt;\n&gt; Date: Sun Jan 01 00:00:00 1999 +0700\n&gt;\n&gt;     COMMIT MESSAGE\n\nusername/myproject$ git show 5a30adf5d4f184fd4586891e26d6826ab66*****\n</code></pre>"},{"location":"tools/git/#if-found-the-bug-success-you-should-exit-bisect-first","title":"If found the bug success, you should exit bisect first","text":"<pre><code>username/myproject$ git bisect reset\n</code></pre> <p>Note</p> <p><code>git bisect log</code> will show the log of bisect, should use this command before reset.</p>"},{"location":"tools/git/#references","title":"References","text":"<ul> <li>TODO -  Git commands I wish I knew earlier as a developer</li> <li>Frequently Used Git Commands</li> <li> Git is your Friend</li> </ul>"},{"location":"tools/git/git-branching-strategies/","title":"Git Branching Strategies","text":"<p> Git - Branching Strategies Explained</p>"},{"location":"tools/git/git-branching-strategies/#data-engineer-branches","title":"Data Engineer branches","text":"<p>Read More</p>"},{"location":"tools/git/git-branching-strategies/#the-mainline-branches","title":"The Mainline branches","text":"<p>It's the simplest but most effective way for small teams. Because of working only on the main branch. So this main branch has to be deployed/published all the time.</p> <p>For each developer will work on his local branch. If a develop process was finished, it would merge a code to main branch.</p> <ul> <li>master</li> </ul> <p>The source code of <code>HEAD</code> always reflects a production-ready   state.</p> <ul> <li>develop</li> </ul> <p>The source code of <code>HEAD</code> always reflects a state with the   latest delivered development changes for the next release.</p> <p>Note</p> <p>Sometime we will split tag version in <code>master</code> to <code>stable</code> for keeping only release tag change.</p> <p>Warning</p> <p>For each feature, it should be small work. If it has long or more code change, it will affect to main branch and has more conflict code.</p>"},{"location":"tools/git/git-branching-strategies/#the-supporting-branches","title":"The Supporting branches","text":""},{"location":"tools/git/git-branching-strategies/#feature-branches","title":"Feature branches","text":"<p>Manage code in separate branches for each feature. When developers complete development and testing, they merge the code from the feature branch into the integration branch. After that, another round of testing is conducted on the integration branch. When everything is ready, the code is merged back into the main branch.</p> <p>Therefore, the state of the code on the main branch is always ready for deployment/release.</p> <ul> <li>May branch off from: <code>develop</code></li> <li>Must merge back into: <code>develop</code></li> <li>Branch naming convention: anything except <code>master</code>, <code>develop</code>, <code>release-*</code>,   <code>support-*</code>, or <code>hotfix-*</code></li> </ul> <p>Warning</p> <p>Things to be cautious about include the lifespan of feature branches or prolonged development. Maintaining them becomes increasingly difficult.</p> <p>Example</p> <p>Creating a feature branch:</p> <pre><code>$ git checkout -b myfeature develop\nSwitched to a new branch \"myfeature\"\n</code></pre> <p>Incorporating a finished feature on develop:</p> <pre><code>$ git checkout develop\nSwitched to branch 'develop'\n\n$ git merge --no-ff myfeature\nUpdating ea1b82a..05e9557\n(Summary of changes)\n\n$ git branch -d myfeature\nDeleted branch myfeature (was 05e9557).\n\n$ git push origin develop\n</code></pre> <p>Info</p> <p>The <code>--no-ff</code> flag causes the merge to always create a new commit object, even if the merge could be performed with a fast-forward. This avoids losing information about the historical existence of a feature branch and groups together all commits that together added the feature.</p>"},{"location":"tools/git/git-branching-strategies/#environment-branches","title":"Environment branches","text":"<ul> <li>May branch off from: <code>master</code></li> <li>Must merge back into: <code>Next environment</code></li> <li>Branch naming convention: <code>testing</code>, <code>production</code>, <code>staging</code>,   <code>pre-production</code></li> </ul>"},{"location":"tools/git/git-branching-strategies/#release-branches","title":"Release branches","text":"<p>Separate according to each version of the system. However, the main work remains in the main branch.</p> <p>But the problem that follows is the management and maintenance in each branch or release, which, first of all, requires editing and merging into the main branch and then using the Git feature called Cherry Pick.</p> <p>To pick up the changes to different branches or releases. And if you make changes in each branch or release, you must also Cherry Pick them to the main branch.</p> <p>This policy or working method is called Upstream First.</p> <ul> <li>May branch off from: <code>develop</code></li> <li>Must merge back into: <code>develop</code> and <code>master</code></li> <li>Branch naming convention: <code>release-*</code></li> </ul> <p>Example</p> <p>Creating a release branch:</p> <pre><code>$ git checkout -b release-1.2 develop\nSwitched to a new branch \"release-1.2\"\n\n$ ./bump-version.sh 1.2\nFiles modified successfully, version bumped to 1.2.\n\n$ git commit -a -m \"Bumped version number to 1.2\"\n[release-1.2 74d9424] Bumped version number to 1.2\n1 files changed, 1 insertions(+), 1 deletions(-)\n</code></pre> <p>Finishing a release branch:</p> <pre><code>$ git checkout master\nSwitched to branch 'master'\n\n$ git merge --no-ff release-1.2\nMerge made by recursive.\n(Summary of changes)\n\n$ git tag -a 1.2\n</code></pre> <p>You might as well want to use the -s or -u  flags to sign your tag cryptographically. <pre><code>$ git checkout develop\nSwitched to branch 'develop'\n\n$ git merge --no-ff release-1.2\nMerge made by recursive.\n(Summary of changes)\n\n$ git branch -d release-1.2\nDeleted branch release-1.2 (was ff452fe).\n</code></pre>"},{"location":"tools/git/git-branching-strategies/#hotfix-branches","title":"Hotfix branches","text":"<ul> <li>May branch off from: <code>master</code></li> <li>Must merge back into: <code>develop</code> and <code>master</code></li> <li>Branch naming convention: <code>hotfix-*</code></li> </ul> <p>Example</p> <p>Creating the hotfix branch:</p> <pre><code>$ git checkout -b hotfix-1.2.1 master\nSwitched to a new branch \"hotfix-1.2.1\"\n\n$ ./bump-version.sh 1.2.1\nFiles modified successfully, version bumped to 1.2.1.\n\n$ git commit -a -m \"Bumped version number to 1.2.1\"\n[hotfix-1.2.1 41e61bb] Bumped version number to 1.2.1\n1 files changed, 1 insertions(+), 1 deletions(-)\n</code></pre> <pre><code>$ git commit -m \"Fixed severe production problem\"\n[hotfix-1.2.1 abbe5d6] Fixed severe production problem\n5 files changed, 32 insertions(+), 17 deletions(-)\n</code></pre> <p>Finishing a hotfix branch:</p> <pre><code>$ git checkout master\nSwitched to branch 'master'\n\n$ git merge --no-ff hotfix-1.2.1\nMerge made by recursive.\n(Summary of changes)\n\n$ git tag -a 1.2.1\n</code></pre> <p>You might as well want to use the -s or -u  flags to sign your tag cryptographically. <pre><code>$ git checkout develop\nSwitched to branch 'develop'\n\n$ git merge --no-ff hotfix-1.2.1\nMerge made by recursive.\n(Summary of changes)\n</code></pre> <p>The one exception to the rule here is that, when a release branch currently exists, the hotfix changes need to be merged into that release branch, instead of <code>develop</code>. Back-merging the bugfix into the release branch will eventually result in the bugfix being merged into <code>develop</code> too, when the release branch is finished. (If work in <code>develop</code> immediately requires this bugfix and cannot wait for the release branch to be finished, you may safely merge the bugfix into <code>develop</code> now already as well.)</p> <pre><code>$ git branch -d hotfix-1.2.1\nDeleted branch hotfix-1.2.1 (was abbe5d6).\n</code></pre>"},{"location":"tools/git/git-branching-strategies/#support-branches","title":"Support branches","text":"<ul> <li>May branch off from: <code>tag</code></li> <li>Must merge back into: <code>None</code></li> <li>Branch naming convention: <code>support-*</code></li> </ul> <p>Example</p> <p>Creating the support branch:</p> <pre><code>$ git checkout -b support-1.2.1.1 v1.2.1\nSwitched to a new branch \"support-1.2.1.1\"\n\n$ ./bump-version.sh 1.2.1.1\nFiles modified successfully, version bumped to 1.2.1.1.\n\n$ git commit -a -m \"Bumped version number to 1.2.1.1\"\n[support-1.2.1.1 13ds7e2] Bumped version number to 1.2.1.1\n1 files changed, 1 insertions(+), 1 deletions(-)\n</code></pre> <p>Support branches would be created \u201con demand\u201d when requested by customers who are stuck on legacy releases and are not able to move forward to current releases, but need security and other bug fixes.</p>"},{"location":"tools/git/git-branching-strategies/#references","title":"References","text":"<ul> <li>Successful Git Branch Model</li> </ul>"},{"location":"tools/git/git-commit-release/","title":"Git Commit Release","text":"<p>This topic will talk about how to write commit message in best practice way. That mean it will reuse this message for tracking and grouping changes for <code>CHANGELOG.md</code> file.</p>"},{"location":"tools/git/git-commit-release/#common-message","title":"Common message","text":"<p>A git commit common message should have a format like:</p> <pre><code>$ git commit -am \"&lt;type&gt;(&lt;scope&gt;): &lt;short-summary-in-present-tense&gt;\n&lt;BLANK LINE&gt;\n&lt;body&gt;\n&lt;BLANK LINE&gt;\n&lt;footer&gt;\"\n</code></pre> <code>&lt;short-summary&gt;</code> <p>use the imperative, present tense: \"change\" not \"changed\" nor \"changes\" - don't capitalize first letter - no dot (.) at the end</p> <code>&lt;type&gt;</code> <ul> <li><code>feat</code>: Features : A new feature.</li> <li><code>fix</code>: Bugfixes : A bug fix.</li> <li><code>docs</code>: Documents : Documentation changes.</li> <li><code>style</code>: Style : Changes that do not affect the meaning of the code   (white-space, formatting, missing semi-colons, etc).</li> <li><code>refactor</code>: Refactor : A code change that neither fixes a bug nor adds a feature.</li> <li><code>perf</code>: Improved performance : A code change that improves performance.</li> <li><code>test</code>: Testing : Changes to the test framework.</li> <li><code>build</code>: Build : Changes to the build process or tools.</li> <li><code>dep</code>: Dependencies and Removals : Changes or update dependencies.</li> </ul> <code>&lt;scope&gt;</code> <p>An optional keyword that provides context for where the change was made. It can be anything relevant to your package or development workflow (e.g., it could be the module or function name affected by the change).</p> <p>Different text in the commit message will trigger PSR to make different kinds of releases:</p> <p>A <code>&lt;type&gt;</code> of fix triggers a patch version bump, e.g.</p> <pre><code>git commit -m \"fix(mod_plotting): fix confusing error message in plot_words\"\n</code></pre> <p>A <code>&lt;type&gt;</code> of feat triggers a minor version bump, e.g.</p> <pre><code>git commit -m \"feat(package): add example data and new module to package\"\n</code></pre> <p>The text <code>BREAKING CHANGE:</code> in the footer will trigger a major release, e.g.</p> <pre><code>git commit -m \"feat(mod_plotting): move code from plotting module to pycounts module\n\nBREAKING CHANGE: plotting module won't exist after this release.\"\n</code></pre> <code>&lt;body&gt;</code> <p>Just as in the subject, use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". The body should include the motivation for the change and contrast this with previous behavior.</p> <code>&lt;footer&gt;</code> <p>The footer should contain any information about Breaking Changes and is also the place to reference GitHub issues that this commit closes.</p> <p>Breaking Changes should start with the word <code>BREAKING CHANGE:</code> with a space or two newlines.</p> <p>Note</p> <p>Any line of the commit message cannot be longer than 100 characters. This allows the message to be easier to read on GitHub as well as in various git tools.</p>"},{"location":"tools/git/git-commit-release/#revert-message","title":"Revert message","text":"<p>If the commit reverts a previous commit, it should begin with <code>revert:</code>, followed by the header of the reverted commit.</p> <p>In the body it should say: <code>This reverts commit &lt;hash&gt;.</code>, where the hash is the SHA of the commit being reverted.</p> <p>The git revert command will undo only the changes associated with a specific commit</p>"},{"location":"tools/git/git-commit-release/#changelogmd","title":"CHANGELOG.md","text":"<p>the CHANGELOG file like release note for developer can see changes history for any tag version.</p> <p>If we set up the commit message and already create release for our project, we can generate a <code>CHANGELOG.md</code> file for show the tracking change histories.</p> <p>We will use <code>git log</code> command to show commit message histories.</p> bash script<pre><code>#!/usr/bin/env bash\n# refs: https://stackoverflow.com/questions/40865597/generate-changelog-from-commit-and-tag\nprevious_tag=0\nfor current_tag in $(git tag --sort=-creatordate)\ndo\nif [ \"$previous_tag\" != 0 ]; then\n    tag_date=$(git log -1 --pretty=format:'%ad' --date=short ${previous_tag})\n    printf \"## ${previous_tag} (${tag_date})\\n\\n\"\n    git log ${current_tag}...${previous_tag} \\\n      --pretty=format:'*  %s [View](https://bitbucket.org/projects/test/repos/my-project/commits/%H)' \\\n      --reverse | grep -v Merge\n    printf \"\\n\\n\"\nfi\nprevious_tag=${current_tag}\ndone\n</code></pre> <pre><code>sh change-log-builder.sh &gt; CHANGELOG.md\n</code></pre> CHANGELOG.md<pre><code>## v1.1.0 (2017-08-29)\n\n- Adds IPv6 support [View](http...)\n- Adds TreeMaker class and its test. [View](http...)\n\n## v1.0.9 (2017-08-22)\n\n- Updates composer.json.lock [View](http...)\n\n## v1.0.8 (2017-08-22)\n\n- Adds S3Gateway as substitute class [View](http...)\n- Remove files no more used [View](http...)\n</code></pre> <p>In the <code>CHANGELOG.md</code> file that will group by typing,</p> CHANGELOG.md<pre><code>## Version 0.1.0\n\n...\n\n**Features**\n\n- `#2094 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2094&gt;`\\_\n  Add `response()` method for closing a stream in a handler\n- `#2097 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2097&gt;`\\_\n  Allow case-insensitive HTTP Upgrade header\n- `#2104 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2104&gt;`\\_\n  Explicit usage of CIMultiDict getters\n- `#2109 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2109&gt;`\\_\n  Consistent use of error loggers\n- `#2114 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2114&gt;`\\_\n  New `client_ip` access of connection info instance\n- `#2133 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2133&gt;`\\_\n  Implement new version of AST router\n\n  - Proper differentiation between `alpha` and `string` param types\n  - Adds a `slug` param type, example: `&lt;foo:slug&gt;`\n\n**Bugfixes**\n\n- `#2119 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2119&gt;`\\_\n  Fix classes on instantiation for `Config`\n\n...\n</code></pre>"},{"location":"tools/git/git-commit-release/#references","title":"References","text":"<ul> <li>Angular Developers Commit</li> <li>Python Package: 07 Releasing Versioning</li> </ul>"},{"location":"tools/git/git-hooks/","title":"Git Hooks","text":"<p><code>git hooks</code> \u0e0a\u0e38\u0e14\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\u0e17\u0e35\u0e48 Git \u0e08\u0e30\u0e40\u0e23\u0e35\u0e22\u0e01 \u0e01\u0e48\u0e2d\u0e19\u0e2b\u0e23\u0e37\u0e2d\u0e2b\u0e25\u0e31\u0e07\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\u0e2b\u0e25\u0e31\u0e01\u0e43\u0e14\u0e46 \u0e40\u0e0a\u0e48\u0e19 commit, push</p> <p>Git hooks \u0e40\u0e1b\u0e47\u0e19\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e15\u0e34\u0e14\u0e15\u0e31\u0e27\u0e21\u0e32\u0e01\u0e31\u0e1a Git \u0e2d\u0e22\u0e39\u0e48\u0e41\u0e25\u0e49\u0e27\u0e44\u0e21\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e44\u0e1b\u0e14\u0e32\u0e27\u0e42\u0e2b\u0e25\u0e14\u0e2d\u0e30\u0e44\u0e23\u0e21\u0e32\u0e25\u0e07\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e41\u0e25\u0e30 Git hooks \u0e19\u0e31\u0e49\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e1f\u0e35\u0e40\u0e08\u0e2d\u0e23\u0e4c\u0e17\u0e35\u0e48\u0e08\u0e33\u0e17\u0e33\u0e07\u0e32\u0e19\u0e41\u0e1a\u0e1a local \u0e2b\u0e23\u0e37\u0e2d\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e02\u0e2d\u0e07\u0e04\u0e19\u0e46\u0e19\u0e31\u0e49\u0e19\u0e40\u0e17\u0e48\u0e32\u0e19\u0e31\u0e49\u0e19</p> <p>Note</p> <pre><code>$ git config --global core.hooksPath /path/to/my/centralized/hooks\n$ git config --local core.hooksPath /path/to/my/centralized/hooks\n</code></pre> <p>Let\u2019s have a look what kind of local hooks we have in our repository\u2019s <code>.git/hooks</code> folder :</p> <pre><code>.git/hooks\n  \u251c\u2500\u2500 applypatch-msg.sample\n  \u251c\u2500\u2500 commit-msg.sample\n  \u251c\u2500\u2500 post-update.sample\n  \u251c\u2500\u2500 pre-applypatch.sample\n  \u251c\u2500\u2500 pre-commit.sample\n  \u251c\u2500\u2500 prepare-commit-msg.sample\n  \u251c\u2500\u2500 pre-rebase.sample\n  \u2514\u2500\u2500 update.sample\n</code></pre>"},{"location":"tools/git/git-hooks/#post-receive","title":"Post Receive","text":"<p><code>git config receive.denycurrentbranch ignore</code></p> <p>post-receive \u0e08\u0e30\u0e17\u0e33\u0e2b\u0e19\u0e49\u0e32\u0e17\u0e35\u0e48\u0e04\u0e37\u0e2d \u0e40\u0e21\u0e37\u0e48\u0e2d push \u0e40\u0e02\u0e49\u0e32 origin master \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e44\u0e23 code \u0e08\u0e30\u0e16\u0e39\u0e01 update \u0e2d\u0e31\u0e15\u0e42\u0e19\u0e21\u0e31\u0e15\u0e34 \u0e41\u0e25\u0e30\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\u0e20\u0e32\u0e22\u0e43\u0e19 ./git/hooks/post-receive \u0e01\u0e47\u0e08\u0e30\u0e16\u0e39\u0e01\u0e23\u0e31\u0e19</p> <p>\u0e42\u0e14\u0e22\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\u0e43\u0e19 post-receive \u0e08\u0e30\u0e17\u0e33\u0e07\u0e32\u0e19\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e21\u0e35\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07 git push \u0e14\u0e31\u0e07\u0e19\u0e31\u0e49\u0e19\u0e2b\u0e32\u0e01\u0e40\u0e23\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e08\u0e30\u0e17\u0e33\u0e2d\u0e30\u0e44\u0e23\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01 push \u0e42\u0e04\u0e49\u0e14\u0e40\u0e2a\u0e23\u0e47\u0e08</p> Scenario 01: trigger jenkinsScenario 02Scenario 03 <p>\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e17\u0e35\u0e48\u0e21\u0e35\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07 job \u0e43\u0e19 Jenkins \u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e2a\u0e31\u0e48\u0e07\u0e43\u0e2b\u0e49 Jenkins \u0e23\u0e31\u0e19\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07 build project \u0e1c\u0e48\u0e32\u0e19\u0e25\u0e34\u0e07\u0e04\u0e4c\u0e44\u0e14\u0e49 \u0e0b\u0e36\u0e48\u0e07\u0e25\u0e34\u0e07\u0e04\u0e4c\u0e08\u0e30\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a \u0e25\u0e34\u0e07\u0e04\u0e4c\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2a\u0e31\u0e48\u0e07 build project \u0e43\u0e19 Jenkins http://jenkins-server/job/projectname/build</p> <pre><code>#!/bin/sh\ncurl http://jenkins-server/job/projectname/build\n</code></pre> <p>\u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e1a\u0e2d\u0e01\u0e27\u0e48\u0e32 \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e21\u0e35\u0e01\u0e32\u0e23 git push \u0e40\u0e02\u0e49\u0e32\u0e21\u0e32 \u0e43\u0e2b\u0e49\u0e2a\u0e48\u0e07 source code \u0e44\u0e1b\u0e22\u0e31\u0e07 /var/www/domain.com</p> <pre><code>#!/bin/sh\ngit --work-tree=/var/www/domain.com --git-dir=/var/repo/site.git checkout -f\n</code></pre> <pre><code>#!/bin/sh\ngit checkout -f\ntouch restart.txt\n</code></pre>"},{"location":"tools/git/git-hooks/#jira-with-git","title":"Jira with Git","text":"<p>Reference: Hook story ID from Jira</p> <p>Jira is software management tool. \u0e44\u0e21\u0e48\u0e27\u0e48\u0e32\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 Development life cycle \u0e41\u0e1a\u0e1a\u0e44\u0e2b\u0e19\u0e01\u0e47\u0e41\u0e25\u0e49\u0e27\u0e41\u0e15\u0e48 \u0e40\u0e0a\u0e48\u0e19 Agile, Scrum, Waterfall \u0e42\u0e14\u0e22\u0e41\u0e15\u0e48\u0e25\u0e30\u0e07\u0e32\u0e19\u0e17\u0e35\u0e48\u0e16\u0e39\u0e01\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e02\u0e36\u0e49\u0e19\u0e21\u0e32\u0e08\u0e30\u0e40\u0e23\u0e35\u0e22\u0e01\u0e27\u0e48\u0e32 Ticket</p> <p>\u0e16\u0e49\u0e32\u0e2a\u0e31\u0e07\u0e40\u0e01\u0e15\u0e17\u0e38\u0e01 ticket \u0e08\u0e30\u0e21\u0e35 auto-increment number \u0e43\u0e2b\u0e49\u0e40\u0e0a\u0e48\u0e19 JIRA-1 JIRA-2 JIRA-3 \u0e0b\u0e36\u0e48\u0e07\u0e43\u0e19\u0e1a\u0e17\u0e04\u0e27\u0e32\u0e21\u0e19\u0e35\u0e49\u0e08\u0e30\u0e02\u0e2d\u0e40\u0e23\u0e35\u0e22\u0e01\u0e2b\u0e21\u0e32\u0e22\u0e40\u0e25\u0e02\u0e02\u0e49\u0e32\u0e07\u0e15\u0e49\u0e19\u0e19\u0e35\u0e49\u0e27\u0e48\u0e32 \u201cStory-ID\u201d (\u0e1a\u0e32\u0e07\u0e17\u0e35\u0e48\u0e2d\u0e32\u0e08\u0e08\u0e30\u0e40\u0e23\u0e35\u0e22\u0e01 Ticket-ID, Jira-ID, etc.)</p> <p>\u0e43\u0e19\u0e40\u0e2b\u0e15\u0e38\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e08\u0e23\u0e34\u0e07\u0e2a\u0e21\u0e21\u0e15\u0e34\u0e27\u0e48\u0e32\u0e21\u0e35 branch \u0e17\u0e35\u0e48\u0e16\u0e39\u0e01\u0e17\u0e33\u0e07\u0e32\u0e19\u0e2d\u0e22\u0e39\u0e48\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e46\u0e01\u0e31\u0e19 4 branch \u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30 branch \u0e40\u0e01\u0e34\u0e14 commit \u0e02\u0e36\u0e49\u0e19\u0e1b\u0e23\u0e30\u0e21\u0e32\u0e13 5\u201320 commits \u0e16\u0e49\u0e32\u0e2a\u0e21\u0e21\u0e15\u0e34\u0e27\u0e48\u0e32 merge \u0e23\u0e27\u0e21\u0e01\u0e31\u0e19 \u0e08\u0e30\u0e40\u0e01\u0e34\u0e14\u0e1b\u0e31\u0e0d\u0e2b\u0e32 commit \u0e08\u0e32\u0e01\u0e17\u0e31\u0e49\u0e07 4 branch \u0e08\u0e30\u0e16\u0e39\u0e01\u0e19\u0e33\u0e21\u0e32\u0e40\u0e23\u0e35\u0e22\u0e07\u0e15\u0e32\u0e21\u0e25\u0e33\u0e14\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e40\u0e01\u0e34\u0e14\u0e02\u0e36\u0e49\u0e19 (timestamp) \u0e41\u0e25\u0e49\u0e27\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e40\u0e23\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e08\u0e30 trace \u0e01\u0e25\u0e31\u0e1a\u0e44\u0e1b\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e04\u0e37\u0e2d \u0e22\u0e32\u0e01\u0e41\u0e25\u0e30\u0e04\u0e48\u0e2d\u0e19\u0e02\u0e49\u0e32\u0e07\u0e40\u0e2a\u0e35\u0e22\u0e40\u0e27\u0e25\u0e32</p> <p>\u0e43\u0e19\u0e1a\u0e32\u0e07\u0e01\u0e32\u0e23\u0e17\u0e33\u0e07\u0e32\u0e19\u0e08\u0e36\u0e07\u0e19\u0e34\u0e22\u0e21\u0e17\u0e35\u0e48\u0e08\u0e30\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e2a\u0e48 <code>[STORY-ID] Commit message</code> Story-ID \u0e15\u0e48\u0e2d\u0e2b\u0e19\u0e49\u0e32\u0e0a\u0e37\u0e48\u0e2d\u0e04\u0e2d\u0e21\u0e21\u0e34\u0e17\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e07\u0e48\u0e32\u0e22\u0e15\u0e48\u0e2d\u0e01\u0e32\u0e23\u0e41\u0e01\u0e30\u0e23\u0e2d\u0e22\u0e22\u0e49\u0e2d\u0e19\u0e01\u0e25\u0e31\u0e1a</p> <p>\u0e0b\u0e36\u0e48\u0e07\u0e43\u0e19\u0e1a\u0e17\u0e04\u0e27\u0e32\u0e21\u0e19\u0e35\u0e49\u0e40\u0e23\u0e32\u0e08\u0e30\u0e40\u0e08\u0e32\u0e30\u0e25\u0e07\u0e44\u0e1b\u0e43\u0e0a\u0e49 hook \u0e17\u0e35\u0e48\u0e0a\u0e37\u0e48\u0e2d\u0e27\u0e48\u0e32 \u201cprepare-commit-msg\u201d \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19 hook \u0e17\u0e35\u0e48\u0e08\u0e30\u0e16\u0e39\u0e01\u0e40\u0e23\u0e35\u0e22\u0e01\u0e43\u0e0a\u0e49\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e2a\u0e31\u0e48\u0e07 commit -m \u0e2b\u0e23\u0e37\u0e2d\u0e43\u0e2a\u0e48 message \u0e19\u0e31\u0e48\u0e19\u0e40\u0e2d\u0e07 \u0e42\u0e14\u0e22\u0e17\u0e35\u0e48 hook \u0e19\u0e35\u0e49\u0e08\u0e30\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e19\u0e33 message \u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e43\u0e2a\u0e48\u0e02\u0e36\u0e49\u0e19\u0e21\u0e32 modify \u0e01\u0e48\u0e2d\u0e19\u0e17\u0e35\u0e48\u0e08\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e40\u0e02\u0e35\u0e22\u0e19\u0e25\u0e07\u0e44\u0e1b\u0e44\u0e14\u0e49</p>"},{"location":"tools/git/git-hooks/#create-script-in-githooksprepare-commit-msgsh","title":"Create script in <code>.git/hooks/prepare-commit-msg.sh</code>","text":"<p>we simply rename <code>prepare-commit-msg.sample</code> to <code>prepare-commit-msg</code>, paste the script listed below and ensure that the file is executable.</p>"},{"location":"tools/git/git-hooks/#example01","title":"Example01","text":"<pre><code>#!/bin/bash\n\n# This way you can customize which branches should be skipped when\n# prepending commit message.\nif [ -z \"$BRANCHES_TO_SKIP\" ]; then\n   BRANCHES_TO_SKIP=(master develop test)\nfi\nBRANCH_NAME=$(git symbolic-ref --short HEAD)\nBRANCH_NAME=\"${BRANCH_NAME##*/}\"\nBRANCH_EXCLUDED=$(printf \"%s\\n\" \"${BRANCHES_TO_SKIP[@]}\" | grep -c \"^$BRANCH_NAME$\")\nBRANCH_IN_COMMIT=$(grep -c \"\\[$BRANCH_NAME\\]\" $1)\nif [ -n \"$BRANCH_NAME\" ] &amp;&amp; ! [[ $BRANCH_EXCLUDED -eq 1 ]] &amp;&amp; ! [[ $BRANCH_IN_COMMIT -ge 1 ]]; then\n   sed -i.bak -e \"1s/^/[$BRANCH_NAME] /\" $1\nfi\n</code></pre> <p>NOTE: Reference code: https://gist.github.com/bartoszmajsak/1396344</p> <pre><code>#!/bin/bash\n\n# For instance with feature/add_new_feature_HEYT-653\n# $ git commit -m\"Fixed bug\"\n# will result with commit \"[HEYT-653] Fixed bug\"\n\n\n# Customize which branches should be skipped when prepending commit message.\nif [ -z \"$BRANCHES_TO_SKIP\" ]; then\n  BRANCHES_TO_SKIP=(master develop test)\nfi\n\nBRANCH_NAME=$(git symbolic-ref --short HEAD | grep -o '[A-Z]\\+-[0-9]\\+')\nBRANCH_NAME=\"${BRANCH_NAME##*/}\"\n\nBRANCH_EXCLUDED=$(printf \"%s\\n\" \"${BRANCHES_TO_SKIP[@]}\" | grep -c \"^$BRANCH_NAME$\")\nBRANCH_IN_COMMIT=$(grep -c \"\\[$BRANCH_NAME\\]\" $1)\n\nif [ -n \"$BRANCH_NAME\" ] &amp;&amp; ! [[ $BRANCH_EXCLUDED -eq 1 ]] &amp;&amp; ! [[ $BRANCH_IN_COMMIT -ge 1 ]]; then\n  sed -i.bak -e \"1s/^/[$BRANCH_NAME] /\" $1\nfi\n</code></pre> <p>NOTE:</p> <pre><code>BRANCH_NAME=$(git rev-parse --abbrev-ref HEAD 2&gt; /dev/null | grep -oE \"[A-Z]+-[0-9]+\")\nif [ -n \"$BRANCH_NAME\" ]; then\n    echo \"[$BRANCH_NAME] $(cat $1)\" &gt; $1\nfi\n</code></pre> <pre><code>#!/bin/bash\n\n# This way you can customize which branches should be skipped when\n# prepending commit message.\nif [ -z \"$BRANCHES_TO_SKIP\" ]; then\n  BRANCHES_TO_SKIP=(master develop test)\nfi\n\nBRANCH_NAME=$(git symbolic-ref --short HEAD)\nBRANCH_NAME=\"${BRANCH_NAME##*/}\"\n\nBRANCH_EXCLUDED=$(printf \"%s\\n\" \"${BRANCHES_TO_SKIP[@]}\" | grep -c \"^$BRANCH_NAME$\")\nBRANCH_IN_COMMIT=$(grep -c \"\\[$BRANCH_NAME\\]\" $1)\n\nif [ -n \"$BRANCH_NAME\" ] &amp;&amp; ! [[ $BRANCH_EXCLUDED -eq 1 ]] &amp;&amp; ! [[ $BRANCH_IN_COMMIT -ge 1 ]]; then\n  sed -i.bak -e \"1s/^/[$BRANCH_NAME] /\" $1\nfi\n</code></pre>"},{"location":"tools/git/git-hooks/#deployment","title":"Deployment","text":"<p>Reference: How to setup deployment with Git</p> <p>https://engineerball.com/blog/2014/05/05/%e0%b9%83%e0%b8%8a%e0%b9%89-git-hook-%e0%b9%80%e0%b8%9e%e0%b8%b7%e0%b9%88%e0%b8%ad-deploy-code.html</p> <p>https://www.imooh.com/git-hook-trigger-jenkins-build-job</p>"},{"location":"tools/git/git-hooks/#bump-version","title":"Bump-Version","text":"<p>The bump version is the script that auto update <code>CHANGELOG.md</code> file with all commit messages after the latest bump version or first initial commit.</p> <p>General digit of version naming, like <code>v1.2.3</code>, will mean <code>v${Major}.${Minor}.${Patch}.${Support}</code>, reference from semver</p> <p>Generate commit message:</p> <pre><code>$ git log --pretty=format:\"  - %s\" \"v$BASE_STRING\"...HEAD\n</code></pre> <p>Auto generate version (Only increment last number):</p> <pre><code>$ $(eval VERSION=$(`shell git describe --tags --abbrev=0 | awk -F. '{OFS=\".\"; $NF+=1; print $0}'`))\n$ git add .\n$ git commit -m \"$m\"\n$ git push origin master\n$ git tag -a $(VERSION) -m \"new release\"\n$ git push origin $(VERSION)\n</code></pre> <p>Examples of Bump-version script:</p> <ul> <li>BUMP-VERSION-shell</li> <li>BUMP-VERSION-shell-v2</li> </ul>"},{"location":"tools/git/git-scenarios/","title":"Git Scenarios","text":""},{"location":"tools/git/git-scenarios/#reset-removed-file","title":"Reset Removed File","text":"<pre><code>$ rm -r README.md\n$ git status\nOn branch master\nChanges not staged for commit:\n  (use \"git add/rm &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        deleted:    README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n</code></pre> Delete in WorkingDelete in StagedDelete in Local Repo <pre><code>$ git checkout README.md\nUpdated 1 path from the index\n\n$ git status\nOn branch master\nnothing to commit, working tree clean\n</code></pre> <p>Note</p> <p>Same as <code>git restore --worktree README.md</code></p> <pre><code>$ git add .\n$ git status\nOn branch master\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        deleted:    README.md\n</code></pre> <pre><code>$ git restore --source=HEAD --staged --worktree README.md\n$ git status\nOn branch master\nnothing to commit, working tree clean\n</code></pre> <p>Note</p> <p>If want to restore to before add to git will use <code>git restore --staged READMD.md</code></p> <pre><code>$ git add .\n$ git commit -m \"ACCIDENT COMMIT\"\n[master 134bf16] ACCIDENT COMMIT\n 1 file changed, 1 deletion(-)\n delete mode 100644 README.md\n\n$ git log\ncommit 5882d01f453beecfec3f252141a9f1bd0761fc35 (HEAD -&gt; master)\nAuthor: username &lt;username@email.com&gt;\nDate:   Sun Jan 01 00:00:00 1999 +0700\n\n    ACCIDENT COMMIT\n\ncommit 4e133da8b2b77cf9755095bf967d8be78a70c7b9\nAuthor: username &lt;username@email.com&gt;\nDate:   Sun Jan 01 00:00:00 1999 +0700\n\n    YOUR MESSAGE\n</code></pre> <code>--soft</code> option<code>--hard</code> option <pre><code>$ git reset --soft HEAD~1\n$ git status\nOn branch master\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        deleted:    README.md\n\nusername/myproject$ git log\ncommit 4e133da8b2b77cf9755095bf967d8be78a70c7b9 (HEAD -&gt; master)\nAuthor: username &lt;username@email.com&gt;\nDate:   Sun Jan 01 00:00:00 1999 +0700\n\n    YOUR MESSAGE\n</code></pre> <pre><code>$ git reset --hard HEAD~1\nHEAD is now at 4e133da YOUR MESSAGE\n\n$ git status\nOn branch master\nnothing to commit, working tree clean\n\nusername/myproject$ git log\ncommit 4e133da8b2b77cf9755095bf967d8be78a70c7b9 (HEAD -&gt; master)\nAuthor: username &lt;username@email.com&gt;\nDate:   Sun Jan 01 00:00:00 1999 +0700\n\n    YOUR MESSAGE\n</code></pre> <p>Note</p> <p><code>git remove --cached README.md</code></p>"},{"location":"tools/git/git-scenarios/#reset-main-branch","title":"Reset <code>main</code> Branch","text":"Deleting <code>.git</code>Deleting Branch <pre><code># Clone the project, e.g. `myproject` is my project repository\n$ git clone https://github/username/myproject.git\n$ cd myproject\n\n# Delete the `.git` folder\n$ git rm -rf .git\n\n# Now, re-initialize the repository\n$ git init\n$ git remote add origin https://github.com/heiswayi/myproject.git\n$ git remote -v\n\n# Add all the files and commit the changes\n$ git add --all\n$ git commit -am \"Initial commit\"\n\n# Force push update to the main branch of our project repository\n$ git push -f origin main\n</code></pre> <p>Deleting the <code>.git</code> folder may cause problems in our git repository. If we want to delete all of our commits history, but keep the code in its current state.</p> <pre><code># Check out to a temporary branch\n$ git checkout --orphan \"TEMP_BRANCH\"\n\n# Add all the files\n$ git add -A\n\n# Commit the changes\n$ git commit -am \"Initial commit\"\n\n# Delete the old branch\n$ git branch -D main\n\n# Rename the temporary branch, \"TEMP_BRANCH\" to main\n$ git branch -m main\n\n# Finally, force update to our repository\n$ git push -f origin main\n</code></pre>"},{"location":"tools/lang/go/","title":"GO","text":"<p>https://github.com/codebangkok/golang</p>"},{"location":"tools/lang/go/#go-linter","title":"GO Linter","text":"<p>https://github.com/pallat/uber-go-style-guide-th</p>"},{"location":"tools/lang/go/#package","title":"Package","text":"<p>https://github.com/avelino/awesome-go</p>"},{"location":"tools/lang/go/#learning","title":"Learning","text":"<p>https://levelup.gitconnected.com/learning-go-part-six-command-line-processing-and-config-files-a27d0b7acc0a</p>"},{"location":"tools/lang/go/go-command/","title":"GO Basic Command Line","text":"<p>Table of Content:</p>"},{"location":"tools/lang/go/go-command/#installation-and-setup","title":"Installation and Setup","text":"<ul> <li>Create <code>go.mod</code> file to your project</li> </ul> <pre><code>$ mkdir go-basic &amp;&amp; cd go-basic\n$ go mod init go-basic\n</code></pre> <ul> <li>Create go file and <code>main</code> package</li> </ul> <pre><code>package main\n\nfunc main() {\n    println(\"Hello World\")\n}\n</code></pre> <ul> <li>Import format</li> </ul> <pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Printf(\"Hello %v\\n\", \"World\")\n}\n</code></pre> <p>Warning: \\ The GO is separate end of line with <code>;</code> and fix the <code>{}</code> syntax.</p> <ul> <li>Test run main package</li> </ul> <pre><code>go run main.go\n</code></pre>"},{"location":"tools/lang/go/go-command/#declare-in-go","title":"Declare in GO","text":"<p><code>var</code> is mutable and <code>const</code> is immutable</p> <pre><code>var x int  // declared but not used\n</code></pre> <pre><code>var x int\n_ = x\n</code></pre> <pre><code>var x int = 10\nprint(x)\n</code></pre> <ul> <li>Short declaration</li> </ul> <pre><code>y := 10\n</code></pre> <p>Note: \\ All variable in GO have default value, in GO it call zero value.</p> <p>Note: \\ When to use short declaration</p> <pre><code>package main\n\nvar x int\n\nfunc main() {\n    y := 10\n}\n</code></pre>"},{"location":"tools/lang/go/go-command/#if-statement","title":"If Statement","text":"<pre><code>point := 50\nif point &gt;= 50 &amp;&amp; point &lt;= 100 {\n    println(\"Pass\")\n} else if point &gt;= 20 {\n    println(\"Not Pass\")\n}\n</code></pre>"},{"location":"tools/lang/go/go-command/#for-loop","title":"For Loop","text":"<pre><code>values := []int{1, 2, 3, 4, 5}\n\nfor i := 0; i &lt; len(values); i ++ {\n    println(values[i])\n}\n</code></pre> <pre><code>values := []int{10, 20, 30, 40}\n\nfor i, v := range values {\n    println(i, v)\n}\n</code></pre> <pre><code>0 10\n1 20\n2 30\n3 40\n</code></pre> <p>Note: \\ If you do not want to use index in for-each statement, you can use</p> <pre><code>for _, v := range values {\n    println(V)\n}\n</code></pre>"},{"location":"tools/lang/go/go-command/#while-loop","title":"While Loop","text":"<pre><code>values := []int{1, 2, 3, 4, 5}\n\ni := 0\nfor i &lt; len(values) {\n    println(values[i])\n    i++\n}\n</code></pre>"},{"location":"tools/lang/go/go-command/#array","title":"Array","text":"<pre><code>// array of int with size 3\nvar x [3]int = [3]int{1, 2, 3}\n\nx := [3]int{1, 2, 3}\n\nx := [...]int{1, 2, 3, 4}\n\n// Create slice array\nx := []int{1, 2, 3}\nx = append(x, 4)\n</code></pre>"},{"location":"tools/lang/go/go-command/#map","title":"Map","text":"<pre><code>var countries map[string]string\n\ncountries := map[string]string{}\ncountries[\"th\"] = \"Thailane\"\n\ncountry, ok := countries[\"jp\"]\nif !ok {\n    println(\"No value\")\n    return\n}\n</code></pre>"},{"location":"tools/lang/go/go-command/#function","title":"Function","text":"<p>The scope of function is in <code>package</code></p> <pre><code>func main() {\n    c, _ := add(10, 20)\n    println(c)\n}\n\nfunc add(a, b int) (int, string) {\n    return a + b, \"Hello\"\n}\n</code></pre> <pre><code>x := func(a, b int) int {\n    return a + b\n}\nvalue := x(10, 20)\nprintln(value)\n</code></pre> <pre><code>func sum(a ...int) int {\n    s := 0\n    for _, v := range a {\n        s += v\n    }\n    return s\n}\n\ns := sum(1, 2, 3, 4, 5)\nprintln(s)\n</code></pre>"},{"location":"tools/lang/go/go-command/#high-order-function","title":"High-order Function","text":"<pre><code>func cal(f func(int,int)int) {\n    sumn := f(50, 10)\n    println(sum)\n}\n\ncal(func(a, b int) int {\n    return a + b\n})\n</code></pre>"},{"location":"tools/lang/go/go-command/#package","title":"Package","text":"<pre><code>project\n    |-- customer\n    |   |- customer.go\n    |- go.mod\n    |- main.go\n</code></pre> <pre><code>// customer.go\npackage customer\n\nvar Name = \"Customer\"\n</code></pre> <pre><code>// main.go\nimport \"project/customer\"\n\nfunc main() {\n    println(customer.Name)\n}\n</code></pre> <p>Warning: \\ In GO, if you want to export anything from another package, you should use pascal case like, <code>func Sum()</code> or <code>var Name =</code></p>"},{"location":"tools/lang/go/go-command/#pointer","title":"Pointer","text":"<pre><code>var x, y int\nx = 10\ny = x\n\n// return pointer in memory\nprintln(&amp;x)\nprintln(&amp;y)\n</code></pre> <pre><code>0xc0000140e0\n0xc0000ae010\n</code></pre> <pre><code>var x int\nx = 10\n\nvar y *int\ny = &amp;x\n\n// return pointer in memory\nprintln(&amp;x)\nprintln(y)\n</code></pre> <pre><code>0xc000122008\n0xc000122008\n</code></pre> <p>Note: \\ If you want to use value of pointer <code>y</code>, you can use <code>println(*y)</code>.</p>"},{"location":"tools/lang/go/go-command/#struct","title":"Struct","text":"<p>Struct is a class without method in GO. Attribute in GO say with behavior.</p> <pre><code>type Person struct {\n    Name string\n    Age int\n}\n\nfunc main() {\n    x := Person{\"Tom\", 18}\n    y := Person{Name: \"Tom\", Age: 18}\n    z := Person{\n        Name: \"Tom\",\n        Age: 18,\n    }\n\n    println(x.Name)\n    println(y.Age)\n}\n</code></pre> <ul> <li>Create Extension Method</li> </ul> <pre><code>func Hello(p Person) string {\n    return \"Hello \" + p.Name\n}\n\nprintln(Hello(x))\n</code></pre> <pre><code>func (p Person) Hello() string {\n    return \"Hello \" + p.Name\n}\n\nprintln(Hello(x))\nprintln(x.Hello())\n</code></pre> <ul> <li>Create OOP in Struct</li> </ul> <pre><code>package customer\n\ntype Person struct {\n    name string\n    age int\n}\n\nfunc (p Person) GetName() string {\n    return p.name\n}\n\nfunc (p *Person) SetName() string {\n    p.name = name\n}\n\nfunc (p Person) GetAge() int {\n    return p.age\n}\n</code></pre> <pre><code>x := customer.Person{}\nx.SetName(\"Tom\")\nprintln(x.GetName())\n</code></pre> <pre><code>Tom\n</code></pre>"},{"location":"tools/lang/go/go-command/#references","title":"References","text":"<ul> <li>https://www.youtube.com/watch?v=JbIS97exQnQ</li> <li>https://github.com/avelino/awesome-go</li> </ul>"},{"location":"tools/lang/go/go-connect-kafka/","title":"GO Advance with Kafka","text":"<p>Table of Contents:</p> <ul> <li>Setup Kafka</li> <li>Create Simple Consumer with GO</li> <li>Create Simple Producer with GO</li> </ul>"},{"location":"tools/lang/go/go-connect-kafka/#setup-kafka","title":"Setup Kafka","text":"<pre><code>version: \"3.9\"\nservices:\n  zookeeper:\n    image: zookeeper\n    container_name: zookeeper\n    volumes:\n      - ./zookeeper:/data\n\n  kafka:\n    image: bitnami/kafka\n    container_name: kafka\n    ports:\n      - \"9092:9092\"\n    volumes:\n      - ./kafka:/bitnami/kafka/data\n    environment:\n      - ALLOW_PLAINTEXT_LISTENER=yes\n      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092\n      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181\n    depends_on:\n      - zookeeper\n</code></pre> <pre><code>$ docker compose up -d\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#test-send-some-data-to-kafka-server","title":"Test Send some data to Kafka Server","text":"<ul> <li>List topics on your Kafka server</li> </ul> <pre><code>$ kafka-topics --bootstrap-server=localhost:9092 --list\n</code></pre> <ul> <li>Create <code>demo</code> topic on your Kafka server</li> </ul> <pre><code>$ kafka-topics --bootstrap-server=localhost:9092 --topic=demo --create\n</code></pre> <ul> <li>Open consumer to <code>demo</code> topic</li> </ul> <pre><code>$ kafka-console-consumer --bootstrap-server=localhost:9092 --topic=demo\n</code></pre> <ul> <li>Send some data by producer to <code>demo</code> topic</li> </ul> <pre><code>$ kafka-console-producer --bootstrap-server=localhost:9092 --topic=demo\n&gt;hello world\n</code></pre> <pre><code>$ kafka-console-consumer ...\nhello world\n</code></pre> <ul> <li>Open consumer to <code>demo</code> topic with separate by groups</li> </ul> <pre><code>$ kafka-console-consumer --bootstrap-server=localhost:9092 \\\n  --topic=demo \\\n  --group=data\n</code></pre> <pre><code>$ kafka-console-consumer --bootstrap-server=localhost:9092 \\\n  --topic=demo \\\n  --group=log\n</code></pre> <ul> <li>Open consumer to <code>test</code>, and <code>demo</code> topics</li> </ul> <pre><code>$ kafka-console-consumer --bootstrap-server=localhost:9092 \\\n  --include=\"demo|test\" \\\n  --group=log\n</code></pre> <ul> <li>Send some data by producer to different topics</li> </ul> <pre><code>$ kafka-console-producer --bootstrap-server=localhost:9092 --topic=demo\n&gt;hello demo\n\n$ kafka-console-producer --bootstrap-server=localhost:9092 --topic=demo\n&gt;hello test\n</code></pre> <pre><code>$ kafka-console-consumer ... --include=\"demo|test\" ...\nhello demo\nhello test\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#create-simple-consumer-with-go","title":"Create Simple Consumer with GO","text":"<pre><code>go mod init consumer\ngo get github.com/Shopify/sarama\n</code></pre> <p>See the partition of <code>demo</code> topic</p> <pre><code>$ kafka-topics --bootstrap-server=localhost:9092 --topic=demo --describe\n</code></pre> <pre><code>package main\n\nfunc main() {\n    servers := []string{\"localhost:9092\"}\n\n    consumer, err := sarama.NewConsumer(servers, nil)\n    if err != nil {\n        panic(err)\n    }\n    defer consumer.Close()\n\n    partitionConsumer, err := consumer.ConsumePartition(\"demo\", 0, sarama.OffsetNewest)\n    if err != nil {\n        panic(err)\n    }\n    defer partitionConsumer.Close()\n\n    for {\n        select {\n            case err := &lt;- partitionConsumer.Errors():\n                fmt.Println(err)\n            case msg := &lt;- partitionConsumer.Messages():\n                fmt.Println(string(msg.Value))\n        }\n    }\n\n}\n</code></pre> <pre><code>$ go run .\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#test-receive-data-from-go-consumer","title":"Test receive data from GO Consumer","text":"<pre><code>$ kafka-console-producer --bootstrap-server=localhost:9092 --topic=demo\n&gt;hello Go Consumer\n</code></pre> <p>Response from Go Consumer will see:</p> <pre><code>$ go run .\nhello Go Consumer\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#create-simple-producer-with-go","title":"Create Simple Producer with GO","text":"<pre><code>go mod init producer\ngo get github.com/Shopify/sarama\n</code></pre> <pre><code>package main\n\nfunc main() {\n    servers := []string{\"localhost:9092\"}\n\n    producer, err := sarama.NewSyncProducer(servers, nil)\n    if err != nil {\n        panic(err)\n    }\n    defer producer.Close()\n\n    msg := sarama.ProducerMessage{\n        Topic: \"demo\",\n        Value: sarama.StringEncoder(\"Hello from Go Producer\"),\n    }\n\n    p, o, err := produser.SendMessage(&amp;msg)\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(\"partition=%v, offset=%v\", p, o)\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#test-send-data-from-go-producer","title":"Test send data from GO Producer","text":"<pre><code># producer\ngo run .\npartition=0, offset=4%\n</code></pre> <pre><code># consumer\ngo run .\nHello from Go Producer\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#create-events-with-go","title":"Create Events with GO","text":"<pre><code>go init events\n</code></pre> <pre><code>package events\n\nimport \"reflect\"\n\nvar Topics = []string{\n    reflect.TypeOf(OpenAccountEvent{}).Name(),\n    reflect.TypeOf(DepositFundEvent{}).Name(),\n    reflect.TypeOf(WithdrawFundEvent{}).Name(),\n    reflect.TypeOf(CloseAccountEvent{}).Name(),\n}\n\ntype Event interface {\n\n}\n\ntype OpenAccountEvent struct {\n    ID              string\n    AccountHolder   string\n    AccountType     int\n    OpeningBalance  float64\n}\n\ntype DepositFundEvent struct {\n    ID              string\n    Amount          float64\n}\n\ntype WithdrawFundEvent struct {\n    ID              string\n    Amount          float64\n}\n\ntype CloseAccountEvent struct {\n    ID              string\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#create-advance-consumer-with-go","title":"Create Advance Consumer with GO","text":"<pre><code>## config.yaml\nkafka:\n  servers:\n    - localhost:9092\n  group: accountConsumer\n\ndb:\n  driver: mysql\n  host: localhost\n  port: 3306\n  username: root\n  password: P@ssw0rd\n  database: demo\n</code></pre> <ul> <li>Import <code>events</code> module from local</li> </ul> <pre><code>// go.mod\n...\nreplace events =&gt; ../events\n...\n</code></pre> <pre><code>go get events\n</code></pre> <pre><code>// repositories/account.go\npackage repositories\n\n...\n\ntype BankAccount struct {\n    ID              string\n    AccountHolder   string\n    AccountType     int\n    Balance         float64\n}\n\ntype AccountRepository interface {\n    Save(bankAccount BankAccount) error\n    Delete(id string) error\n    FindAll() (bankAccounts []BankAccount, err error)\n    FindByID(id string) (bankAccount BankAccount, err error)\n}\n\ntype accountRepository struct {\n    db *gorm.DB\n}\n\nfunc NewAccountRepository(db *gorm.DB) AccountRepository {\n    db.Table(\"demo_banks\").AutoMigrate(&amp;BankAccount{})\n    return accountRepository{db}\n}\n\nfunc (obj accountRepository) Save(bankAccount BankAccount) error {\n    return obj.db.Table(\"demo_banks\").Save(bankAccount).Error\n}\n\nfunc (obj accountRepository) Save(id string) error {\n    return obj.db.Table(\"demo_banks\").Where(\"id=?\", id).Delete(&amp;BankAccount{}).Error\n}\n\nfunc (obj accountRepository) FindAll() (bankAccounts []BankAccount, err error) {\n    err = obj.db.Table(\"demo_banks\").Find(&amp;bankAccounts).Error\n    return bankAccounts, err\n}\n\nfunc (obj accountRepository) FindByID(id string) (bankAccount BankAccount, err error) {\n    err = obj.db.Table(\"demo_banks\").Where(\"id=?\", id).First(&amp;bankAccount).Error\n    return bankAccount, err\n}\n</code></pre> <pre><code>// services/account.go\npackage services\n\ntype EventHandler interface {\n    Handle(topic string, eventBytes []byte)\n}\n\ntype accountEventHandler struct {\n    accountRepo repositories.AccountRepository\n}\n\nfunc NewAccountEventHandler(accountRepo repositories.AccountRepository) EventHandler {\n    return accountEventHandler{accountRepo}\n}\n\nfunc (obj accountEventHandler) Handle(topic string, eventBytes []byte) {\n    switch topic {\n        case reflect.TypeOf(OpenAccountEvent{}).Name():\n            event := events.OpenAccountEvent{}\n            err := json.Unmarshal(eventBytes, event)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            bankAccount := repositories.BankAccount {\n                ID:             event.ID,\n                AccountHolder:  event.AccountHolder,\n                AccountType:    event.AccountType,\n                Balance:        event.OpenBalance,\n            }\n            err = obj.accountRepo.Save(bankAccount)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            log.Println(event)\n        case reflect.TypeOf(DepositFundEvent{}).Name():\n            event := events.DepositFundEvent{}\n            err := json.Unmarshal(eventBytes, event)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            bankAccount, err := obj.accountRepo.FindByID(event.ID)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            bankAccount.Balance += event.Amount\n            err = obj.accountRepo.Save(bankAccount)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            log.Println(event)\n        case reflect.TypeOf(WithdrawFundEvent{}).Name():\n            event := events.WithdrawFundEvent{}\n            err := json.Unmarshal(eventBytes, event)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            bankAccount, err := obj.accountRepo.FindByID(event.ID)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            bankAccount.Balance -= event.Amount\n            err = obj.accountRepo.Save(bankAccount)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            log.Println(event)\n        case reflect.TypeOf(CloseAccountEvent{}).Name():\n            event := events.CloseAccountEvent{}\n            err := json.Unmarshal(eventBytes, event)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            err = obj.accountRepo.Delete(event.ID)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            log.Println(event)\n        default:\n            log.Println(\"no event handler\")\n    }\n}\n</code></pre> <pre><code>// services/consumer.go\npackage services\n\ntype consumerHandler struct {\n    eventHandler EventHandler\n}\n\nfunc NewConsumerHandler(eventHandler EventHandler) sarama.ConsumerGroupHandler {\n    return consumerHandler{eventHandler}\n}\n\nfunc (obj consumerHandler) Setup(sarama.ConsumerGroupSession) error {\n    return nil\n}\n\nfunc (obj consumerHandler) Cleanup(sarama.ConsumerGroupSession) error {\n    return nil\n}\n\nfunc (obj consumerHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {\n    for msg := range claim.Messages() {\n        obj.eventHandler.Handle(msg.Topic, msg.Value)\n        session.MarkMessage(msg, \"\")\n    }\n\n    return nil\n}\n</code></pre> <pre><code>// main.go\npackage main\n\nfunc init() {\n    viper.SetConfigName(\"config\")\n    viper.SetConfigType(\"yaml\")\n    viper.AddConfigPath(\".\")\n    viper.AutomaticEnv()\n    viper.SetEnvKeyReplacer(strings.NewReplacer(\".\", \"_\"))\n    if err := viper.ReadConfig(); err != nil {\n        panic(err)\n    }\n}\n\nfunc initDatabase() *gorm.DB {\n    dsn := fmt.Sprintf(%v:%v@tcp(%v:%v)/%v,\n        viper.GetString(\"db.username\"),\n        viper.GetString(\"db.password\"),\n        viper.GetString(\"db.host\"),\n        viper.GetInt(\"db.port\"),\n        viper.GetString(\"db.database\"),\n    )\n\n    dial := mysql.Open(dsn)\n    db, err := gorm.Open(dial, &amp;gorm.Config{\n        Logger: logger.Default.LogMode(logger.Silent),\n    })\n    if err != nil {\n        panic(err)\n    }\n\n    return db\n}\n\nfunc main() {\n    consumer, err := sarama.NewConsumerGruop(viper.GetStringSlice(\"kafka.servers\"), viper.GetString(\"kafka.group\"), nil)\n    if err != nil {\n        panic(err)\n    }\n    defer consumer.Close()\n\n    db := initDatabase()\n    accountRepo := repositories.NewAccountRepository(db)\n    accountEventHandler := services.NewAccountEventHandler(accountRepo)\n    accountConsumerHandler := services.NewConsumerHandler(accountEventHandler)\n\n    fmt.Println(\"Account consumer started ...\")\n    for {\n        consumer.Consume(context.Backgroud(), event.Topics, accountConsumerHandler)\n    }\n}\n</code></pre> <pre><code>$ go run .\nAccount consumer started ...\n</code></pre> <ul> <li>List groups</li> </ul> <pre><code>$ kafka-consumer-groups --bootstrap-server=localhost:9092 --list\naccountConsumer\n...\n</code></pre> <ul> <li>Test receive data from manual producer</li> </ul> <pre><code>$ kafka-console-producer --bootstrap-server=localhost:9092 --topic=OpenAccountEvent\n&gt;{\"ID\": \"1\",\"AccountHolder\":\"Admin\",\"AccountType\":1,\"OpeningBalance\":1000}\n</code></pre> <pre><code>$ go run .\nAccount consumer started ...\n20**/**/01 00:00:00 &amp;{1 Admin 1 1000}\n</code></pre> <pre><code>$ kafka-console-consumer --bootstrap-server=localhost:9092 \\\n  --include=\"OpenAccountEvent|DepositFundEvent|WithdrawFundEvent|CloseAccountEvent\" \\\n  --group=log\n{\"ID\": \"1\",\"AccountHolder\":\"Admin\",\"AccountType\":1,\"OpeningBalance\":1000}\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#create-advance-consumer-with-go_1","title":"Create Advance Consumer with GO","text":"<pre><code>## config.yaml\nkafka:\n  servers:\n    - localhost:9092\n</code></pre> <ul> <li>Import <code>events</code> module from local</li> </ul> <pre><code>// go.mod\n...\nreplace events =&gt; ../events\n...\n</code></pre> <pre><code>// commands/command.go\npackage commands\n\ntype OpenAccountCommand struct {\n    AccountHolder   string\n    AccountType     int\n    OpeningBalance  float64\n}\n\ntype DepositFundCommand struct {\n    ID              string\n    Amount          float64\n}\n\ntype WithdrawFundCommand struct {\n    ID              string\n    Amount          float64\n}\n\ntype CloseAccountCommand struct {\n    ID              string\n}\n</code></pre> <pre><code>//services/producer.go\npackage services\n\ntype EventProducer interface {\n    Produce(event events.Event) error\n}\n\ntype eventProducer struct {\n    producer sarama.SyncProducer\n}\n\nfunc NewEventProducer(producer sarama.SyncProducer) EventProducer {\n    return eventProducer{producer}\n}\n\nfunc (obj eventProducer) Produce(event events.Event) error {\n    topic := reflect.TypeOf(event).Name()\n\n    value, err := json.Marshal(event)\n    if err != nil {\n        return err\n    }\n\n    msg := sarama.ProducerMessage{\n        Topic: topic,\n        Value: sarama.ByteEncoder(value),\n    }\n\n    _, _, err = obj.producer.SendMessage(&amp;msg)\n    if err != nil {\n        return err\n    }\n    return nil\n}\n</code></pre> <pre><code>$ go get github.com/google/uuid\n</code></pre> <pre><code>// services/account.go\npackage services\n\ntype AccountService interface {\n    OpenAccount(command commands.OpenAccountCommand) (id string, err error)\n    DepositFund(command commands.DepositFundCommand) error\n    WithdrawFund(command commands.WithdrawFundCommand) error\n    CloseAccount(command commands.CloseAccountCommand) error\n}\n\ntype accountService struct {\n    eventProducer EventProducer\n}\n\nfunc NewAccountService(eventProducer EventProducer) AccountService {\n    return accountService{eventProducer}\n}\n\nfunc (obj accountService) OpenAccount(command commands.OpenAccountCommand) (id string, err error) {\n\n    if command.AccountHolder == \"\" || command.AccountType == 0 || command.OpeningBalance == 0 {\n        return \"\", errors.New(\"bad request\")\n    }\n\n    event := events.OpenAccountEvent {\n        ID:             uuid.NewString(),\n        AccountHolder:  command.AccountHolder,\n        AccountType:    command.AccountType,\n        OpeningBalance: command.OpeningBalance,\n    }\n    log.Println(\"%#v\", event)\n    return event.ID, obj.eventProducer.Produce(event)\n}\n\nfunc (obj accountService) DepositFund(command commands.DepositFundCommand) error {\n    if command.ID == \"\" || command.Amount == 0 {\n        return errors.New(\"bad request\")\n    }\n\n    event := events.DepositFundEvent{\n        ID:     command.ID,\n        Amount  command.Amount,\n    }\n    log.Println(\"%#v\", event)\n    return obj.eventProducer.Produce(event)\n}\n\nfunc (obj accountService) WithdrawFund(command commands.WithdrawFundCommand) error {\n    if command.ID == \"\" || command.Amount == 0 {\n        return errors.New(\"bad request\")\n    }\n\n    event := events.WithdrawFundEvent{\n        ID:     command.ID,\n        Amount  command.Amount,\n    }\n    log.Println(\"%#v\", event)\n    return obj.eventProducer.Produce(event)\n}\n\nfunc (obj accountService) CloseAccount(command commands.CloseAccountCommand) error {\n    if command.ID == \"\" {\n        return errors.New(\"bad request\")\n    }\n\n    event := event.CloseAccountEvent{\n        ID: command.ID,\n    }\n    log.Println(\"%#v\", event)\n    return obj.eventProducer.Produce(event)\n}\n</code></pre> <pre><code>$ go get github.com/gofiber/fiber/v2\n</code></pre> <pre><code>// controllers/account.go\npackage controllers\n\ntype AccountController interface {\n    OpenAccount(c *fiber.Ctx) error\n    DepositAccount(c *fiber.Ctx) error\n    WithdrawAccount(c *fiber.Ctx) error\n    CloseAccount(c *fiber.Ctx) error\n}\n\ntype accountController struct {\n    accountService services.AccountService\n}\n\nfunc NewAccountController(accountService services.AccountService) AccountController {\n    return accountController{accountService}\n}\n\nfunc (obj accountController) OpenAccount(c *fiber.Ctx) error {\n    command := command.OpenAccountCommand{}\n\n    err := c.BodyParser(&amp;command)\n    if err != nil {\n        return err\n    }\n\n    id, err := obj.accountService.OpenAccount(command)\n    if err != nil {\n        return err\n    }\n\n    c.Status(fiber.StatusCreated)\n    return c.JSON(fiber.Map{\n        \"message\":  \"open account success\",\n        \"id\":       id,\n    })\n}\n\nfunc (obj accountController) DepositAccount(c *fiber.Ctx) error {\n    command := command.DepositFundCommand{}\n    err := c.BodyParser(&amp;command)\n    if err != nil {\n        return err\n    }\n\n    err := obj.accountService.DepositFund(command)\n    if err != nil {\n        return err\n    }\n    return c.JSON(fiber.Map{\n        \"message\": \"deposit fund success\",\n    })\n}\n\nfunc (obj accountController) WithdrawAccount(c *fiber.Ctx) error {\n    command := command.WithdrawFundCommand{}\n    err := c.BodyParser(&amp;command)\n    if err != nil {\n        return err\n    }\n\n    err := obj.accountService.WithdrawFund(command)\n    if err != nil {\n        return err\n    }\n    return c.JSON(fiber.Map{\n        \"message\": \"withdraw fund success\",\n    })\n}\n\nfunc (obj accountController) CloseAccount(c *fiber.Ctx) error {\n    command := command.CloseAccountCommand{}\n    err := c.BodyParser(&amp;command)\n    if err != nil {\n        return err\n    }\n\n    err := obj.accountService.CloseAccount(command)\n    if err != nil {\n        return err\n    }\n    return c.JSON(fiber.Map{\n        \"message\": \"close account success\",\n    })\n}\n</code></pre> <pre><code>// main.go\npackage main\n\ndef main() {\n    producer, err := sarama.NewSyncProducer(viper.GetStringSlice(\"kafka.servers\"), nil)\n    if err != nil {\n        panic(err)\n    }\n    defer producer.Close()\n\n    eventProducer := services.NewEventProducer(producer)\n    accountService := services.NewAccountService(eventProducer)\n    accountController := controllers.NewAccountController(accountService)\n\n    app := fiber.New()\n\n    app.Post(\"/openAccount\", accountController.OpenAccount)\n    app.Post(\"/depositFund\", accountController.DepositFund)\n    app.Post(\"/withdrawFund\", accountController.WithdrawFund)\n    app.Post(\"/closeAccount\", accountController.CloseAccount)\n\n    app.Liten(\":8000\")\n}\n</code></pre> <pre><code>$ curl -H 'content-type:application/json' localhost:8000/openaccount \\\n  -d '{\"AccountHolder\": \"Admin\", \"AccountType\": 1, \"OpeningBalance\": 1000}' \\\n  -i\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#references","title":"References","text":"<p>-</p>"},{"location":"tools/lang/go/go-connect-redis/","title":"GO Advance with Redis","text":"<p>Create project folder in your local,</p> <pre><code>goredis\n|--&gt; docker-compose.yml\n|--&gt; go.mod\n|--&gt; main.go\n</code></pre> <p>Table of Contents:</p> <ul> <li>Setup Redis</li> <li>[]</li> <li>Create API Server with GO</li> </ul>"},{"location":"tools/lang/go/go-connect-redis/#setup-redis","title":"Setup Redis","text":"<ul> <li>Install Redis CLI, use <code>redis-cli</code></li> <li>Install Redis Server from Docker Container</li> </ul> <pre><code>version: \"3.9\"\nservices:\n  redis:\n    image: redis\n    container_name: redis\n    ports:\n      - \"6379:6379\"\n</code></pre> <pre><code>docker-compose up\n</code></pre> <pre><code>$ redis-cli\n127.0.0.1:6379$ ping\nPONG\n127.0.0.1:6379$ set name value\nOK\n127.0.0.1:6379$ get name\n\"value\"\n</code></pre> <p>Note: \\ If you want to delete value in redis with life-cycle, you can use <code>ex</code> such as delete value in 5 seconds: <code>set name hello ex 5</code></p>"},{"location":"tools/lang/go/go-connect-redis/#persisted-data-from-redis","title":"Persisted Data from Redis","text":"<p>We want to config redis server to persisted data (snapshot data) when it has some problem take server down. First, we create data folder and file <code>config/redis.conf</code>:</p> <pre><code>goredis\n|--&gt; config\n|    |--&gt; redis.conf\n|--&gt; data\n|    |--&gt; redis\n|--&gt; ...\n</code></pre> <p>Copy configuration data with your version from redis official redis.io/topics/config document to this file.</p> <pre><code># Allow other network to connect redis\nbind 0.0.0.0\n\n# Disable Snapshot and use empty value\nsave \"\"\n\n# Enable Append-only mode\nappendonly yes\nappendfilename \"appendonly.aof\"\n</code></pre> <p>Edit the docker-compose file:</p> <pre><code># docker-compose.yml\nversion: \"3.9\"\nservices:\n  redis:\n    image: redis\n    container_name: redis\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - ./data/redis:/data\n      - ./config/redis.conf:/redis.conf\n    command: redis-server /redis.conf\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#create-api-server-with-go","title":"Create API Server with GO","text":"<p>We use <code>fiber</code> for API server.</p> <pre><code>go get github.com/gofiber/fiber/v2\n</code></pre> <pre><code>// main.go\npackage main\n\nimport github.com/gofiber/fiber/v2\n\nfunc main() {\n    app := fiber.New()\n    app.Get(\"/hello\", func(c *fiber.Ctx) error {\n        return c.SendString(\"Hello World\")\n    })\n    app.Listen(\":8000\")\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#load-test","title":"Load Test","text":"<p>We use <code>k6</code>.</p> <pre><code>goredis\n|--&gt; ...\n|--&gt; scripts\n|    |--&gt; test.js\n|--&gt; ...\n</code></pre> <pre><code># docker-compose.yml\nversion: '3.9'\nservices:\n\n    ...\n\n    k6:\n        image: loadimpact/k6\n        container_name: k6\n        volumes:\n            - ./scripts:/scripts\n</code></pre> <pre><code>import http from \"k6/http\";\n\nexport default function () {\n  http.get(\"http://host.docker.internal:8000/hello\");\n}\n</code></pre> <ul> <li>Load test with 5 users and 5 seconds</li> </ul> <pre><code>docker compose run --rm k6 run /scripts/test.js -u5 -d5s\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#edit-configuration-in-test-file","title":"Edit configuration in test file","text":"<pre><code>import http from 'k6/http'\n\nexport let: options = {\n    vus: 5,\n    duration: '5s'\n}\n\nexport default function() {\n    http.get('http://host.docker.internal:8000/hello')\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#create-timeseries-database-and-grafana","title":"Create TimeSeries Database and Grafana","text":"<p>We use <code>InfluxDB</code> for keep data from <code>k6</code> and use Grafana to visualize test dashboard. If we use InfluxDB version &gt;= 2.0.0, it will support build-in dashboard.</p> <pre><code># docker-compose.yml\nversion: '3.9'\nservices:\n\n    ...\n\n    k6:\n        ...\n        environment:\n            - K6_OUT=influxdb=http://influxdb:8086/k6\n        ...\n\n    influxdb:\n        image: influxdb:1.8.10  # Version 2.0.0 does not support for k6 yet\n        container_name: influxdb\n        environment:\n            - INFLUXDB_DB=k6\n            - INFLUXDB_HTTP_MAX_BODY_SIZE=0\n        ports:\n            - \"8086:8086\"\n        volumes:\n            - ./data/influxdb:/var/lib/influxdb\n\n    grafana:\n        image: grafana/grafana\n        container_name: grafana\n        environment:\n            - GF_AUTH_ANONYMOUS_ENABLED=true\n            - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin  # This is bad practice!!!\n        ports:\n            - \"3000:3000\"\n        volumes:\n            - ./data/grafana:/var/lib/grafana\n</code></pre> <pre><code>docker compose up influxdb grafana\n</code></pre> <p>After <code>Grafana</code> server run successful, we will go to <code>localhost:3000</code> and Add <code>InfluxDB</code> data source with these configurations:</p> <ul> <li>URL: http://influxdb:8086</li> <li>Database: k6</li> </ul> <p>Go to <code>grafana.com/grafana/dashboard</code> and search k6 dashboard. We will copy dashboard ID from official to Local Grafana server in import menu.</p>"},{"location":"tools/lang/go/go-connect-redis/#use-case","title":"Use-case","text":"<pre><code>goredis\n|--&gt; ...\n|--&gt; handlers\n|--&gt; repositories\n|    |--&gt; product_redis.go\n|    |--&gt; product_db.go\n|    |--&gt; product.go\n|--&gt; services\n|--&gt; go.mod\n|--&gt; main.go\n</code></pre> <pre><code>go get gorm.io/gorm\ngo get gorm.io/driver/mysql\ngo get github.com/go-redis/redis/v8\n</code></pre> <pre><code># docker-compose.yml\nversion: '3.9'\nservices:\n\n    ...\n\n    mariadb:\n        image: mariadb\n        container_name: mariadb\n        environment:\n            - MARIADB_ROOT_PASSWORD=P@ssw0rd\n            - MARIADB_DATABASE=product\n        ports:\n            - \"3306:3306\"\"\n        volumes:\n            - ./data/mariadb:/var/lib/mysql\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#repositories","title":"Repositories","text":"<pre><code>// repositories/product.go\npackage repositories\n\n...\n\ntype product struct {\n    ID          int\n    Name        string\n    Quantity    int\n}\n\ntype ProductRepository interface {\n    GetProducts() ([]product, error)\n}\n\nfunc mockData(db *gorm.DB) error {\n\n    var count int64\n    db.model(&amp;product{}).Count(&amp;count)\n    if count &gt; 0 {\n        return nil\n    }\n\n    seed := rand.NewSource(time.Now().UnixNano())\n    random := rand.New(seed)\n\n    products := []product{}\n    for i := 0; i &lt; 5000; i++ {\n        products = append(products, product{\n            Name:       fmt.Sprintf(\"Product%v\", i + 1),\n            Quantity:   random.Intn(100),\n        })\n    }\n    return db.Create(&amp;products).Error\n}\n</code></pre> <pre><code>// repositories/product_db.go\npackage repositories\n\n...\n\ntype productRepositoryDB struct {\n    db *gorm.DB\n}\n\nfunc NewProductRepositoryDB(db *gorm.DB) ProductRepository {\n    db.AutoMigrate(&amp;product{})\n    mockData(db)\n    return productRepositoryDB{db: db}\n}\n\nfunc (r productRepositoryDB) GetProducts() (products []product, err error) {\n    err = r.db.Order(\"quantity desc\").Limit(30).Find(&amp;products)\n    if err != nil {\n        return nil, err\n    }\n    return products, err\n}\n</code></pre> <pre><code>// repositories/product_redis.go\npackage repositories\n\n...\n\ntype productRepositoryRedis struct {\n    db *gorm.DB\n    redisClient *redis.Client\n}\n\nfunc NewProductRepositoryDB(db *gorm.DB, redisClient *redis.Client) ProductRepository {\n    db.AutoMigrate(&amp;product{})\n    mockData(db)\n    return productRepositoryRedis{db, redisClient}\n}\n\nfunc (r productRepositoryRedis) GetProducts() (products []product, err error) {\n\n    key := \"repository::GetProcusts\"\n\n    // Chack data in Redis\n    productsJson, err := r.redisClient.Get(context.Background(), key).Result()\n    if err == nil {\n        err = json.Unmarshal([]byte(productsJson, &amp;products)\n        if err == nil {\n            return products, nil\n        }\n    }\n\n    err = r.db.Order(\"quantity desc\").Limit(30).Find(&amp;products)\n    if err != nil {\n        return nil, err\n    }\n\n    // Import to Redis\n    data, err := json.Marshal(products)\n    if err != nil {\n        return nil, err\n    }\n    err = r.redisClient.Set(context.Background(), key, string(data), time.Second * 10).Err()\n    if err != nil {\n        return nil, err\n    }\n    return products, err\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#services","title":"Services","text":"<pre><code>// services/catalog.go\npackage services\n\ntype Product struct {\n    ID       int    `json:\"id\"`\n    Name     string `json:\"name\"`\n    Quantity int    `json:\"quantity\"`\n}\n\ntype CatalogService interface {\n    GetProducts() ([]Product, error)\n}\n</code></pre> <pre><code>// services/catalog_service.go\npackage services\n\n...\n\ntype catalogService struct {\n    productRepo repositories.ProductRepository\n}\n\nfunc NewCatalogService(productRepo repositories.ProductRepository) CatalogService {\n    return catalogService{productRepo}\n}\n\nfunc (s catalogService) GetProducts() (products []Product, err error) {\n    productsDB, err := s.productRepo.GetProducts()\n    if err != nil {\n        return nil, err\n    }\n\n    for _, p := range productsDB {\n        products = append(products, Product{\n            ID:         p.ID,\n            Name:       p.Name,\n            Quantity:   p.Quantity,\n        })\n    }\n    return products, nil\n}\n</code></pre> <pre><code>// services/catalog_redis.go\npackage services\n\n...\n\ntype catalogServiceRedis struct {\n    productRepo repositories.ProductRepository\n    redisClient *redis.Client\n}\n\nfunc NewCatalogServiceRedis(productRepo repositories.ProductRepository, redisClient *redis.Client) CatalogService {\n    return catalogServiceRedis{productRepo, redisClient}\n}\n\nfunc (s catalogServiceRedis) GetProducts() (products []Product, err error) {\n    key := \"services::GetProcusts\"\n\n    // Chack data in Redis\n    if productsJson, err := s.redisClient.Get(context.Background(), key).Result(); err == nil {\n        if json.Unmarshal([]byte(productsJson, &amp;products) == nil {\n            return products, nil\n        }\n    }\n\n    // Repository\n    productsDB, err := s.productRepo.GetProducts()\n    if err != nil {\n        return nil, err\n    }\n\n    for _, p := range productsDB {\n        products = append(products, Product{\n            ID:         p.ID,\n            Name:       p.Name,\n            Quantity:   p.Quantity,\n        })\n    }\n\n    // Import to Redis\n    if data, err := json.Marshal(products); err == nil {\n        s.redisClient.Set(context.Background(), key, string(data), time.Second * 10)\n    }\n    return products, nil\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#handler","title":"Handler","text":"<pre><code>// handlers/catalog.go\npackage handlers\n\n...\n\ntype CatalogHandler interface {\n    GetProducts(c *fiber.Ctx) error\n}\n</code></pre> <pre><code>// handlers/catalog.handlers.go\npackage handlers\n\n...\n\ntype catalogHandler struct {\n    catalogSrv services.CatalogService\n}\n\nfunc NewCatalogHandler(catalogSrv services.CatalogService) CatalogHandler {\n    return catalogHandler{catalogSrv}\n}\n\nfunc (h catalogHandler) GetProducts(c *fiber.Ctx) error {\n    products, err := h.catalogSrv.GetProducts()\n    if err != nil {\n        return err\n    }\n    response := fiber.Map{\n        \"status\": \"ok\",\n        \"products\": products,\n    }\n    return c.JSON(response)\n}\n</code></pre> <pre><code>// handlers/catalog_redis.go\n\npackage handlers\n\n...\n\ntype catalogHandlerRedis struct {\n    catalogSrv services.CatalogService\n    redisClinet *redis.Client\n}\n\nfunc NewCatalogHandlerRedis(catalogSrv services.CatalogService, redisClient *redis.Client) catalogHandlerRedis {\n    return catalogHandlerRedis{catalogSrv, redisClient}\n}\n\nfunc (h catalogHandlerRedis) GetProducts(c *fiber.Ctx) error {\n    key := \"handler::GetProcusts\"\n\n    // Chack data in Redis\n    if responseJson, err := s.redisClient.Get(context.Background(), key).Result(); err == nil {\n        c.Set(\"Content-Type\", \"applicatin/json\")\n        return c.SendString(responseJson)\n    }\n\n    products, err := h.catalogSrv.GetProducts()\n    if err != nil {\n        return err\n    }\n    response := fiber.Map{\n        \"status\": \"ok\",\n        \"products\": products,\n    }\n\n    // Import to Redis\n    if data, err := json.Marshal(response); err == nil {\n        h.redisClient.Set(context.Background(), key, string(data), time.Second * 10)\n    }\n\n    return c.JSON(response)\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#database","title":"Database","text":"<pre><code>// main.go\n\n...\n\nfunc main() {\n    db := initDatabase()\n    redisClient := initRedis()\n\n    productRepo := repositories.NewProductRepositoryDB(db)\n    // // Or,\n    // productRepo := repositories.NewProductRepositoryRedis(db, redisClient)\n    // products, err := productRepo.GetProducts()\n\n    productService := services.NewCatalogService(productRepo)\n    // // Or,\n    // productService := services.NewCatalogServiceRedis(productRepo, redisClient)\n    // products, err := productService.GetProducts()\n\n    productHandler := handler.NewCatalogHandler(productService)\n    // Or,\n    productHandler := handler.NewCatalogHandlerRedis(productService, redisClient)\n\n    app := fiber.New()\n    app.Get(\"/products\", productHandler.GetProducts)\n    app.Listen(\":8000\")\n}\n\nfunc initDatabase() *gorm.DB {\n    dial := mysql.Open(\"root:P@ssw0rd@tcp(localhost:3306)/product\")\n    db, err := gorm.Open(dial, &amp;gorm.Config{})\n    if err != nil {\n        panic(err)\n    }\n    return db\n}\n\nfunc initRedis() *redis.Client {\n    return redis.NewClient(&amp;redis.Option{\n        Addr: \"localhost:6379\"\n    })\n}\n</code></pre> <pre><code>curl localhost:8000/products | jq\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#references","title":"References","text":"<p>-</p>"},{"location":"tools/lang/go/go-database/","title":"GO Connect to Database","text":"<p>Table of Contents:</p> <ul> <li>[Start to Connect Database]</li> <li>[Create Return Function]</li> <li>[Create CRUD]</li> <li>[Transaction]</li> </ul>"},{"location":"tools/lang/go/go-database/#start-to-connect-database","title":"Start to Connect Database","text":"<pre><code>go get -u github.com/denisenkom/go-mssqldb\n</code></pre> <pre><code>package main\n\nimport {\n    \"database/sql\"\n\n    // import driver that use in /sql package\n    _ \"github.com/denisenkom/go-mssqldb\"\n}\n\nfunc main() {\n    db, err := sql.Open(\"sqlserver\", \"sqlserver://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:1443/&lt;database&gt;\")\n    if err != nil {\n        panic(err)\n    }\n\n    err = db.Ping()\n    if err != nil {\n        panic(err)\n    }\n\n    query := \"select id, name from cover\"\n    row, err := db.Query(query)\n    if err != nil {\n        panic(err)\n    }\n    defer rows.Close()\n\n    for rows.Next() {\n        id := 0\n        name := \"\"\n        err = rows.Scan(&amp;id, &amp;name)\n        if err := nil {\n            panic(err)\n        }\n        println(id, name)\n    }\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#create-struct-for-receive-data","title":"Create Struct for receive data","text":"<pre><code>type Cover struct {\n    Id int\n    Name string\n}\n</code></pre> <pre><code>covers := []Cover{}\n\nfor rows.Next() {\n    cover := Cover{}\n    err = rows.Scan(&amp;cover.Id, &amp;cover.Name)\n    if err != nil {\n        panic(err)\n    }\n    covers = append(covers, cover)\n}\n\nfmt.Printf(\"%#v\", covers)\n</code></pre> <pre><code>[]main.Cover{main.Cover{Id:1, Name:\"cover-lion\"}, main.Cover{Id:1, Name:\"cover-zebra\"}}%\n</code></pre>"},{"location":"tools/lang/go/go-database/#create-return-function","title":"Create Return Function","text":"<pre><code>var db *sql.DB\n\nfunc main() {\n    var err error\n    db, err = sql.Open(\"sqlserver\", \"sqlserver://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:1443/&lt;database&gt;\")\n    if err != nil {\n        panic(err)\n    }\n\n    covers, err := GetCovers()\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n\n    for _, cover := range covers {\n        fmt.Println(cover)\n    }\n}\n\nfunc GetCovers() ([]Cover, error) {\n    err = db.Ping()\n    if err != nil {\n        return nil, err\n    }\n\n    query := \"select id, name from cover\"\n    row, err := db.Query(query)\n    if err != nil {\n        return nil, err\n    }\n    defer rows.Close()\n\n    covers := []Cover{}\n    for rows.Next() {\n        cover := Cover{}\n        err = rows.Scan(&amp;cover.Id, &amp;cover.Name)\n        if err != nil {\n            return nil, err\n        }\n        covers = append(covers, cover)\n    }\n    return covers, nil\n}\n</code></pre> <p>Note: \\ You should not create error handler process in your err.</p>"},{"location":"tools/lang/go/go-database/#create-crud","title":"Create CRUD","text":""},{"location":"tools/lang/go/go-database/#read","title":"Read","text":"<pre><code>func GetCover(id int) (*Cover, error) {\n    err := db.Ping()\n    if err != nil {\n        return nil, err\n    }\n\n    // Use @id for SQL Server\n    query := \"select id, name from cover where id=@id\"\n    row := db.QueryRow(query, sql.Named(\"id\", id))\n\n    cover := Cover{}\n    err = row.Scan(&amp;cover.Id, &amp;cover.Name)\n    if err != nil {\n        return nil, err\n    }\n    return &amp;cover, nil\n}\n</code></pre> <pre><code>cover, err := GetCover(1)\nif err != nil {\n    panic(err)\n}\nprintln(cover)\n</code></pre> <pre><code>&amp;{1 cover-lion}\n</code></pre> <p>Note: \\ If you use MySQL, the above query row syntax will change to</p> <pre><code>query := \"select id, name from cover where id=?\"\nrow := db.QueryRow(query, id)\n</code></pre>"},{"location":"tools/lang/go/go-database/#create","title":"Create","text":"<pre><code>func AddCover(cover Cover) error {\n    query := \"insert into cover (id, name) values (?, ?)\"\n    result, err := db.Exec(query, cover.Id, cover.Name)\n    if err != nil {\n        return err\n    }\n    affected, err := result.RowAffected()\n    if err != nil {\n        return err\n    }\n    if affected &lt;= 0 {\n        return errors.New(\"cannot insert to cover table\")\n    }\n    return nil\n}\n</code></pre> <pre><code>cover := Cover{9, \"cover-Tom\"}\nerr = AddCover(cover)\nif err != nil {\n    panic(err)\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#update","title":"Update","text":"<pre><code>func UpdateCover(cover Cover) error {\n    query := \"update cover set name=? where id=?\"\n    result, err := db.Exec(query, cover.Name, cover.Id)\n    if err != nil {\n        return err\n    }\n    affected, err := result.RowAffected()\n    if err != nil {\n        return err\n    }\n    if affected &lt;= 0 {\n        return errors.New(\"cannot update to cover table\")\n    }\n    return nil\n}\n</code></pre> <pre><code>cover := Cover{9, \"cover-Sara\"}\nerr = UpdateCover(cover)\nif err != nil {\n    panic(err)\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#delete","title":"Delete","text":"<pre><code>func DeleteCover(id int) error {\n    query := \"delete from cover where id=?\"\n    result, err := db.Exec(query, id)\n    if err != nil {\n        return err\n    }\n    affected, err := result.RowAffected()\n    if err != nil {\n        return err\n    }\n    if affected &lt;= 0 {\n        return errors.New(\"cannot delete to cover table\")\n    }\n    return nil\n}\n</code></pre> <pre><code>err = DeleteCover(9)\nif err != nil {\n    panic(err)\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#sqlx","title":"SQLX","text":"<pre><code>go get github.com/jmoiron/sqlx\n</code></pre> <pre><code>package main\n\nimport {\n    _ \"github.com/go-sql-driver/mysql\"\n    \"github.com/jmoiron/sqlx\"\n}\n\nvar db *sqlx.DB\n\nfunc main() {\n    var err error\n    db, err = sqlx.Open(\"mysql\", \"root:&lt;password&gt;@tcp(&lt;host&gt;)/&lt;database&gt;\")\n    if err != {\n        panic(err)\n    }\n}\n\nfunc GetCoversX() ([]Cover, error) {\n    query := \"select id, name from cover\"\n    covers := []Cover{}\n    err = db.Select(&amp;covers, query)\n    if err != nil {\n        return nil, err\n    }\n    return covers, nil\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#change-read","title":"Change Read","text":"<pre><code>func GetCoverX(id int) (*Cover, error) {\n    query := \"select id, name from cover where id=?\"\n    cover := Cover{}\n    err = db.Get(&amp;cover, query, id)\n    if err != nil {\n        return nil, err\n    }\n    return &amp;cover, nil\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#transaction","title":"Transaction","text":"<pre><code>tx, err := db.Begin()\nif err != nil {\n    return err\n}\n\nquery := \"...\"\nresult, err := tx.Exec(query, ...)\n\n...\n\naffected, err := result.RowsAffected()\nif err != nil {\n    tx.Rollback()\n    return err\n}\n\n...\n\nerr = tx.Commit()\nif err != nil {\n    return err\n}\nreturn nil\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/","title":"GO Hexagonal Architecture","text":"<p>Update: <code>2023-05-04</code> | Tag: <code>GO</code> <code>Architecture</code> <code>RestAPI</code></p> <p>Hexagonal Architecture or Ports &amp; Adapters is an architectural pattern that guilds you in structuring a component/service/application and managing dependencies.</p> <p>Warning: \\ This is not a best practice!</p> <p>Layer of Architecture:</p> <pre><code>Presentation --&gt; Business --&gt; Database\n</code></pre> <p>Table of Contents:</p> <ul> <li>Get Started</li> <li></li> </ul>"},{"location":"tools/lang/go/go-hexagonal-architecture/#get-started","title":"Get Started","text":"<pre><code>bank\n|--&gt; repository\n|    |--&gt; customer_db.go\n|    |--&gt; customer_mock.go\n|    |--&gt; customer.go\n|--&gt; service\n|    |--&gt; customer.go\n|--&gt; go.mod\n|--&gt; main.go\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#create-customer-ports","title":"Create Customer Ports","text":"<pre><code>// repository/customer.go\npackage repository\n\ntype Customer struct {\n    CustomerID  int     `db:\"customer_id\"`\n    Name        string  `db:\"name\"`\n    DateOfBirth string  `db:\"date_of_birth\"`  // or time.Time\n    City        string  `db:\"city\"`\n    ZipCode     string  `db:\"zipcode\"`\n    Status      int     `db:\"status\"`\n}\n\ntype CustomerRepository interface {\n    GetAll() ([]Customer, error)\n    GetById(int) (*Customer, error)\n}\n</code></pre> <pre><code>// service/customer.go\npackage service\n\ntype CustomerResponse struct {\n    CustomerID  int     `json:\"customer_id\"`  // If you want xml, `xml:\"...\"`\n    Name        string  `json:\"name\"`\n    Status      string  `json:\"status\"`\n}\n\ntype CustomerService interface {\n    GetCustomers() ([]CustomerResponse, error)\n    GetCustomer(int) (*CustomerResponse, error)\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#create-database-adapters","title":"Create Database Adapters","text":"<pre><code>go get github.com/jmoiron/sqlx\n</code></pre> <pre><code>// repository/customer_db.go\npackage repository\n\nimport \"github.com/jmoiron/sqlx\"\n\ntype customerRepositoryDB struct {\n    db *sqlx.DB\n}\n\nfunc NewCustomerRepositoryDB(db *sqlx.DB) customerRepositoryDB {\n    return customerRepositoryDB{db: db}\n}\n\nfunc (r customerRepositoryDB) GetAll() ([]Customer, error) {\n    customers := []Customer{}\n    query := \"select ... from customers\"\n    err := r.db.Select(&amp;customers, query)\n    if err != nil {\n        return nil, err\n    }\n    return customers, nil\n}\n\nfunc (r customerRepositoryDB) GetById(id int) (*Customer, error) {\n    customer := Customer{}\n    query := \"select ... from customers where customer_id=?\"\n    err := r.db.Get(&amp;customer, query, id)\n    if err != nil {\n        return nil, err\n    }\n    return &amp;customer, nil\n}\n</code></pre> <p>Note: \\ If you want to implement create function, you can use:</p> <pre><code>query = \"insert into ... values(?, ?, ?)\"\nresult, err := r.db.Exec(\n    query,\n    &lt;obj&gt;.Param01,\n    &lt;obj&gt;.Param02,\n    &lt;obj&gt;.Param03,\n)\nid, err := result.LastInsertId()\nif err != nil {\n    return nil, err\n}\n&lt;obj&gt;.ID = int(id)\nretunr &amp;&lt;obj&gt;, nil\n</code></pre> <pre><code>go get -u github.com/go-sql-driver/mysql\n</code></pre> <pre><code>// main.go\npackage main\n\nimport (\n    _ \"github.com/go-sql-driver/mysql\"\n    \"github.com/jmoiron/sqlx\"\n)\n\nfunc main() {\n    db, err := sqlx.Open(\"mysql\", \"root:&lt;password&gt;@tcp(&lt;host&gt;:&lt;port&gt;)/&lt;database&gt;\"\n    if err != nil {\n        panic(err)\n    }\n\n    customerRepository := repository.NewCustomerRepositoryDB(db)\n    _ = customerRepository\n\n    custoers, err := customerRepository.GetAll()\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(customers)\n\n    customer, err := customerRepository.GetById(2000)\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(customer)  // &amp;{200 Steve ...}\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#create-service-adapters","title":"Create Service Adapters","text":"<pre><code>// service/customer_service.go\npackage service\n\ntype customerService struct {\n    custRepo repository.CustomerRepository  // ref interface only\n}\n\nfunc NewCustomerService(custRepo repository.CustomerRepository) customerService {\n    return customerService{custRepo: custRepo}\n}\n\nfunc (s customerService) GerCustomers() ([]CustomerResponse, error) {\n    customers, err := s.custRepo.GetAll()\n    if err != nil {\n        log.Println(err)\n        return nil, err\n    }\n    custResponses := []CustomerResponse{}\n    for _, customer := range customers {\n        custResponse := CustomerResponse{\n            CustomerID: customer.CustomerID,\n            Name:       customer.Name,\n            Status:     customer.Status,\n        }\n        custResponses = append(custResponses, custResponse)\n    }\n    return custResponses, nil\n}\n\nfunc (s customerService) GerCustomer(id int) (*CustomerResponse, error) {\n    customer, err := s.custRepo.GetById(id)\n    if err != nil {\n\n        if err == sql.ErrNoRows {\n            return nil, errors.New(\"customer not found\")\n        }\n\n        log.Println(err)\n        return nil, err\n    }\n    custResponse := CustomerResponse{\n        CustomerID: customer.CustomerID,\n        Name:       customer.Name,\n        Status:     customer.Status,\n    }\n    return &amp;custResponse, nil\n}\n</code></pre> <pre><code>// main.go\npackage main\n\nimport (\n    \"back/repository\"\n    \"back/service\"\n\n    _ \"github.com/go-sql-driver/mysql\"\n    \"github.com/jmoiron/sqlx\"\n)\n\nfunc main() {\n    db, err := sqlx.Open(\"mysql\", \"root:&lt;password&gt;@tcp(&lt;host&gt;:&lt;port&gt;)/&lt;database&gt;\"\n    if err != nil {\n        panic(err)\n    }\n\n    customerRepository := repository.NewCustomerRepositoryDB(db)\n    customerService := service.NewCustomerService(customerRepository)\n\n    customers, err := customerService.GetCustomers()\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(customers)\n\n    customer, err := customerService.GetCustomer(2000)\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(customer)  // &amp;{200 Steve ...}\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#create-customer-handler","title":"Create Customer Handler","text":"<pre><code>bank\n|--&gt; handler\n|    |--&gt; customer.go\n|--&gt; repository\n|    |--&gt; customer_db.go\n|    |--&gt; customer.go\n|--&gt; service\n|    |--&gt; customer.go\n|--&gt; go.mod\n|--&gt; main.go\n</code></pre> <pre><code>go get -u github.com/gorilla/mux\n</code></pre> <pre><code>package handler\n\nimport (\n    \"bank/service\"\n    \"net/http\"\n)\n\ntype customerHandler struct {\n    custSrv service.CustomerService\n}\n\nfunc NewCustomerHandler(custSrv service.CustomerService) customerHandler {\n    return customerHandler{custSrv: custSrv}\n}\n\nfunc (h customerHandler) GetCustomers(w http.ResponseWriter, r *http.Request) {\n    customers, err := h.custSrv.GetCustomers()\n    if err != nil {\n        w.WritHeader(http.StatusInternalServerError)\n        fmt.Fprintln(w, err)\n        return\n    }\n\n    w.Header().Set(\"content-type\", \"application/json\")\n    json.NewEncoder(w).Encode(customers)\n}\n\nfunc (h customerHandler) GetCustomer(w http.ResponseWriter, r *http.Request) {\n    customerID, _ := strconv.Atoi(mux.Vars(r)[\"customerID\"])\n    customer, err := h.custSrv.GetCustomer(customerID)\n    if err != nil {\n        w.WritHeader(http.StatusInternalServerError)\n        fmt.Fprintln(w, err)\n        return\n    }\n    w.Header().Set(\"content-type\", \"application/json\")\n    json.NewEncoder(w).Encode(customer)\n}\n</code></pre> <pre><code>// main.go\npackage main\n\nimport (\n    \"back/repository\"\n    \"back/service\"\n    \"back/handler\"\n    \"net/http\"\n\n    _ \"github.com/go-sql-driver/mysql\"\n    \"github.com/jmoiron/sqlx\"\n)\n\nfunc main() {\n    db, err := sqlx.Open(\"mysql\", \"root:&lt;password&gt;@tcp(&lt;host&gt;:&lt;port&gt;)/&lt;database&gt;\"\n    if err != nil {\n        panic(err)\n    }\n\n    customerRepository := repository.NewCustomerRepositoryDB(db)\n    customerService := service.NewCustomerService(customerRepository)\n    customerHandler := handler.NewCustomerHandler(customerService)\n\n    router := mux.NewRouter()\n\n    router.HandleFunc(\"/customers\", customerHandler.GetCustomers).Methods(http.MethodGet)\n    router.HandleFunc(\"/customers/{customerID:[0-9]+}\", customerHandler.GetCustomer).Methods(http.MethodGet)\n\n    http.ListenAndServe(\":8000\", router)\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#create-mock-repository","title":"Create Mock Repository","text":"<pre><code>bank\n|--&gt; repository\n|    |--&gt; ...\n|    |--&gt; customer_mock.go\n|    |--&gt; ...\n|--&gt; ...\n</code></pre> <pre><code>// repository/customer_mock.go\npackage repository\n\ntype customerRepositoryMock struct {\n    customers []Customer\n}\n\nfunc NewCustomerRepositoryMock() CustomerRepositoryMock {\n    customers := []Customer{\n        {CustomerID: 1001, Name: \"Sara\", ...},\n        {CustomerID: 1002, Name: \"Tom\", ...},\n    }\n    return CustomerRepositoryMock{customers: customers}\n}\n\nfunc (r customerRepositoryMock) GetAll() ([]Customer, error) {\n    return r.cusomers, nil\n}\n\nfunc (r customerRepositoryMock) GetById(id int) (*Customer, error) {\n    for _, customer := range r.cusomers {\n        if customer.CustomerID == id {\n            return &amp;customer, nil\n        }\n    }\n    return nil, errors.New(\"customer not found\")\n}\n</code></pre> <pre><code>...\ncustomerRepository := repository.NewCustomerRepositoryDB(db)\n---&gt; Change to ...\ncustomerRepository := repository.NewCustomerRepositoryMock()\n...\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#configuration","title":"Configuration","text":"<pre><code>go get github.com/spf13/viper\n</code></pre> <pre><code>bank\n|--&gt; repository\n|    |--&gt; ...\n|--&gt; ...\n|--&gt; config.yaml\n|--&gt; main.go\n</code></pre> <pre><code>// main.go\n\nfunc main() {\n    initConfig()\n\n    dsn := fmt.Sprintf(\"%v:%v@tcp(%v:%v)/%v?parseTime=true\",\n        viper.GetString(\"db.username\"),\n        viper.GetString(\"db.password\"),\n        viper.GetString(\"db.host\"),\n        viper.GetInt(\"db.port\"),\n        viper.GetString(\"db.database\"),\n    )\n    db, err := sqlx.Open(viper.GetString(\"db.driver\"), dns)\n    if err != nil {\n        panic(err)\n    }\n\n    ...\n\n    http.ListenAddServe(fmt.Sprintf(\":%v\", viper.GetInt(\"app.port\")), router)\n}\n\n\nfunc initConfig() {\n    viper.SetConfigName(\"config\")\n    viper.SetConfigType(\"yaml\")\n    viper.AddConfigPath(\".\")\n\n    // Read Environment variable after and override value\n    viper.AutonaticEnv()\n    viper.SetEnvKeyReplacer(string.NewReplacer(\".\", \"_\"))\n\n    err := viper.ReadInConfig()\n    if err != nil {\n        panic(err)\n    }\n}\n</code></pre> <pre><code>app:\n  port: 8000\n\ndb:\n  driver: \"mysql\"\n  host: \"13.00.000.00\"\n  port: 3306\n  username: root\n  password: P@ssw0rd\n  database: banking\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#change-database-setup","title":"Change Database Setup","text":"<pre><code>// main.go\n\ndef main() {\n    ...\n    db := initDatabase()\n    ...\n}\n\ndef initDatabase() *sqlxDB {\n    dsn := fmt.Sprintf(\"%v:%v@tcp(%v:%v)/%v?parseTime=true\",\n        viper.GetString(\"db.username\"),\n        viper.GetString(\"db.password\"),\n        viper.GetString(\"db.host\"),\n        viper.GetInt(\"db.port\"),\n        viper.GetString(\"db.database\"),\n    )\n    db, err := sqlx.Open(viper.GetString(\"db.driver\"), dns)\n    if err != nil {\n        panic(err)\n    }\n\n    // Database configuration\n    db.SetConnMaxLifetime(3 * time.Minute)\n    db.SetMaxOpenConns(10)\n    db.SetMaxIdleConns(10)\n\n    return db\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#timezone","title":"TimeZone","text":"<pre><code>// main.go\n\nfunc main() {\n    initTimeZone()\n}\n\nfunc initTimeZone() {\n    ict, err := time.LoadLocation(\"Asia/Bangkok\")\n    if err != nil {\n        panic(err)\n    }\n\n    time.Local = ict\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#logging","title":"Logging","text":"<pre><code>go get -u go.uber.org/zap\n</code></pre> <pre><code>bank\n|--&gt; logs\n|    |--&gt; logs.go\n|--&gt; repository\n|    |--&gt; ...\n|--&gt; ...\n|--&gt; main.go\n</code></pre> <pre><code>// logs/logs.go\npackage logs\n\nimport \"go.uber.org/zap\"\n\nvar log *zap.Logger\n\nfunc init() {\n    var err error\n\n    log, _ = zap.NewProduction()  // production use json, but development use console\n\n    // Or, ddit Production configuration\n    config := zap.NewProductionConfig()\n    config.EncoderConfig.TimeKey = \"timestamp\"\n    config.EncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder\n\n    // If you want to close strack trace,\n    config.EncoderConfig.StacktraceKey = \"\"\n\n    log, err = config.Build(zap.AddCallerSkip(1))  // Skip caller 1 step\n    if err != nil {\n        panic(err)\n    }\n}\n\nfunc Info(message string, fields ...zap.Field) {\n    log.Info(message, fields...)\n}\n\nfunc Debug(message string, fields ...zap.Field) {\n    log.Debug(message, fields...)\n}\n\nfunc Error(message interface{}, fields ...zap.Field) {\n    switch y := message.(type) {\n        case error:\n            log.Error(v.Error(), fields...)\n        case string:\n            log.Error(v, fields...)\n    }\n}\n</code></pre> <p>Note: \\ In GO, it has init function that will run before main function. You can use it by <code>func init() { ... }</code></p> <pre><code>// main.go\n\nfunc main() {\n    ...\n    logs.Log.Info(\"Banking service started Info Level log\")\n    ...\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#change-log-in-service","title":"Change Log in Service","text":"<pre><code>...\nlog.Println(err)\n---&gt; Change to ...\nlogs.Error(err.Error())  // Or, logs.Error(err)\n...\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#error","title":"Error","text":"<pre><code>bank\n|--&gt; errs\n|    |--&gt; errs.go\n|--&gt; ...\n|--&gt; repository\n|    |--&gt; ...\n|--&gt; ...\n|--&gt; main.go\n</code></pre> <pre><code>// errs/errs.go\npackage errs\n\ntype AppError struct {\n    Code    int\n    Message string\n}\n\nfunc (e AppError) Error() string {\n    return e.Message\n}\n\nfunc NewNotFoundError(message string) error {\n    return AppError{\n        Code:    http.StatusNotFound,\n        Message: message,\n    }\n}\n\nfunc NewUnexpectedError(message string) error {\n    return AppError{\n        Code:    http.StatusInternalServerError,\n        Message: \"unexpected error\",\n    }\n}\n</code></pre> <pre><code>// service/customer_service.go\n\nfunc (s customerService) GetCustomer(id int) ... {\n    ...\n    if err != nil {\n        if err == sql.ErrNoRows {\n            return nil, errs.NewNotFoundError(\"customer not found\")\n        }\n        logs.Error(err)\n        return nil, errs.NewUnexpectedError()\n    }\n    ...\n}\n</code></pre> <pre><code>// handler/customer.go\n\nfunc (h customerHandler) GetCustomer(w http.ResponseWriter, ...) {\n    ...\n    if err != nil {\n\n        appErr, ok := err.(errs.AppError)\n        if ok {\n            w.WriteHeader(appErr.Code)\n            fmt.Fprintln(w, appErr.Message)\n            return\n        }\n\n        w.WritHeader(http.StatusInternalServerError)\n        fmt.Fprintln(w, err)\n        return\n    }\n    ...\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#change-code-to-helper","title":"Change Code to helper","text":"<pre><code>bank\n|--&gt; errs\n|    |--&gt; errs.go\n|--&gt; handler\n|    |--&gt; customer.go\n|    |--&gt; handler.go\n|--&gt; ...\n|--&gt; main.go\n</code></pre> <pre><code>// handler/handler.go\npackage handler\n\nimport \"net/http\"\n\nfunc handleError(w http.ResponseWriter, err error) {\n    switch e := err.(type) {\n        case errs.AppError:\n            w.WriteHeader(e.Code)\n            fmt.Fprintln(w, e)\n        case error:\n            w.WriteHeader(http.StatusInternalServerError)\n            fmt.Fprintln(w, e)\n    }\n}\n</code></pre> <pre><code>...\nappErr, ok := err.(errs.AppError)\nif ok {\n    w.WriteHeader(appErr.Code)\n    fmt.Fprintln(w, appErr.Message)\n    return\n}\nw.WritHeader(http.StatusInternalServerError)\nfmt.Fprintln(w, err)\n\n---&gt; Change to ...\n\nhandleError(w, err)\n...\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#add-more","title":"Add More","text":""},{"location":"tools/lang/go/go-hexagonal-architecture/#account","title":"Account","text":"<pre><code>// reposirory/account.go\n\ntype Account struct {\n    AccountID       int     `json:\"account_id\"`\n    CustomerID      int     `json:\"customer_id\"`\n    OpeningDate     string  `json:\"opening_date\"`\n    AccountType     string  `json:\"account_type\"`\n    Amount          float64 `json:\"amount\"`\n    Status          int     `json:\"status\"`\n}\n\ntype AccountRepository interface {\n    Create(Account) (*Account, error)\n    GetAll(int) ([]Account, error)\n}\n</code></pre> <pre><code>// service/account.go\npackage serice\n\ntype NewAccountRequest struct {\n    AccountType     string  `json:\"account_type\"`\n    Amount          float64 `json:\"amount\"`\n}\n\ntype NewAccountResponse struct {\n    AccountID       int     `json:\"account_id\"`\n    OpeningDate     string  `json:\"opening_date\"`\n    AccountType     string  `json:\"account_type\"`\n    Amount          float64 `json:\"amount\"`\n    Status          int     `json:\"status\"`\n}\n\ntype AccountService interface {\n    NewAccount(int, NewAccountRequest) (*NewAccountResponse, error)\n    GetAccounts(int) ([]NewAccountResponse, error)\n}\n</code></pre> <pre><code>// service/account_service.go\n\n...\nfunc (s accountService) NewAccount(customerID int, request NewAccountRequest) (*AccountResponse, error) {\n    // Validation\n    if request.Amount &lt; 5000 {\n        return nil, errs.NewValidationError(\"amount at least 5,000\")\n    }\n    if string.ToLower(requset.AccountType) != \"saving\" &amp;&amp; string.ToLower(requset.AccountType) != \"checking\" {\n        return nil, errs.NewValidationError(\"account type should be saving or checking only\")\n    }\n    account := repository.Account{\n        CustomerID:     customerID,\n        OpeningDate:    time.New().Format(\"2006-1-2 15:00:00\"),\n        AccountType:    requset.AccountType,\n        Amount:         requset.Amount,\n        Status:         1,\n    }\n\n    newAcc, err := s.accRepo.Create(account)\n    if err != nil {\n        logs.Error(err)\n        return nil, errs.NewUnexpectedError()\n    }\n    response := AccountResponse{\n        AccountID:  newAcc.AccountID,\n        ...\n    }\n    return response, nil\n}\n...\n</code></pre> <pre><code>// handler/account.go\n\n...\n\nfunc (h accountHandler) NewAccount(w http.ResponseWriter, r *http.Request) {\n    customerID, _ := strconv.Atoi(mux.Vars(r)[\"customerID\"])\n    if r.Header.Get(\"content-type\") != \"application/json\" {\n        handlerError(w, errs.NewValidationError(\"request body incorrect format\"))\n        return\n    }\n\n    request := service.NewAccoutRequest{}\n    err := json.NewDecoder(r.Body).Decode(&amp;request)\n    if err != nil {\n        handlerError(w, errs.NewValidationError(\"request body incorrect format\"))\n        return\n    }\n\n    response, err := h.accSrv.NewAccount(customerID, request)\n    if err != nil {\n        handlerError(w, err)\n        return\n    }\n\n    w.WriteHeader(http.StatusCreated)\n    w.Header().Set(\"content-type\", \"application/json\")\n    json.NewEncoder(w).Encode(response)\n}\n\n...\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#references","title":"References","text":"<ul> <li>https://www.youtube.com/watch?v=k3JZI-sQs2k</li> </ul>"},{"location":"tools/lang/go/go-unittest/","title":"GO Unittest","text":"<p>Update: <code>2023-05-04</code> | Tag: <code>GO</code> <code>Unittest</code></p>"},{"location":"tools/lang/go/go-unittest/#test","title":"Test","text":"<pre><code>// services/grade_test.go\npackage services_test\n\nimport \"testing\"\n\nfunc TestCheckGrade(t *testing.T) {\n    // test function for CheckGrade(80)\n    grade := services.CheckGrade(80)\n    expected := \"A\"\n\n    if grade != expected {\n        t.Errorf(\"get %v expected %v\", grade, expected)\n    }\n}\n</code></pre> <pre><code>// services/grade.go\npackage services\n\nfunc CheckGrade(score int) string {\n    switch {\n        case score &gt;= 80:\n            return \"A\"\n        case score &gt;= 70:\n            return \"B\"\n        case score &gt;= 60:\n            return \"C\"\n        case score &gt;= 50:\n            return \"D\"\n        default:\n            return \"F\"\n    }\n}\n</code></pre> <pre><code>go test &lt;module-name&gt;/serices -v\n\ngo test &lt;module-name&gt;/serices -v -run=TestCheckGrade\n\ngo test ./... -v\n</code></pre> <pre><code># Get test coverage\ngo test &lt;module-name&gt;/services -cover\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#create-sub-test","title":"Create Sub-test","text":"<pre><code>// services/grade_test.go\n\nfunc TestCheckGrade(t *testing.T) {\n    t.Run(\"A\", func(t *testing.T) {\n        grade := services.CheckGrade(80)\n        expected := \"A\"\n\n        if grade != expected {\n            t.Errorf(\"get %v expected %v\", grade, expected)\n        }\n    })\n\n    t.Run(\"B\", func(t *testing.T) {\n        grade := services.CheckGrade(70)\n        expected := \"B\"\n\n        if grade != expected {\n            t.Errorf(\"get %v expected %v\", grade, expected)\n        }\n    })\n}\n</code></pre> <pre><code>go test &lt;module-name&gt;/services -run=\"TestCheckGrade/A\" -v\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#change-to-be-good-coverage","title":"Change to be Good Coverage","text":"<pre><code>// services/grade_test.go\n\nfunc TestCheckGrade(t *testing.T) {\n    type testCase struct {\n        name     string\n        score    int\n        expected string\n    }\n\n    cases := []testCase{\n        {name: \"a\", score: 80, expected: \"A\"},\n        {name: \"b\", score: 70, expected: \"B\"},\n        {name: \"c\", score: 60, expected: \"C\"},\n        {name: \"d\", score: 50, expected: \"D\"},\n        {name: \"f\", score: 0, expected: \"F\"},\n    }\n\n    for _, c := range cases {\n        t.Run(c.name, func(t *testing.T) {\n            grade := services.CheckGrade(c.score)\n            if grade != c.expected {\n                t.Errorf(\"get %v expected %v\", grade, expected)\n            }\n        })\n    }\n\n}\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#benchmark","title":"Benchmark","text":"<pre><code>// services/grade_test.go\n\nfunc BenchmarkCheckGrade(b *testing.B) {\n    for i := 0; i &lt; b.N; i ++ {\n        services.CheckGrade(80)\n    }\n}\n</code></pre> <pre><code>go test &lt;module-name&gt;/services -bench=.\n\n# For check memory benchmark\ngo test &lt;module-name&gt;/services -bench=. -benchmem\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#example","title":"Example","text":"<pre><code>go get golang.org/x/tools/cmd/godoc\n</code></pre> <pre><code>// services/grade_test.go\n\nfunc ExampleCheckGrade() {\n    grade := services.CheckGrade(80)\n    fmt.Println(grade)\n    // Output: A\n}\n</code></pre> <pre><code>godoc -http=:8000\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#mock","title":"Mock","text":"<pre><code>go get github.com/stretchr/testify\n</code></pre> <pre><code>// main.go\npackage main\n\nfunc main() {\n    c := CustomerRepositoryMock{}\n    c.On(\"GetCustomer\", 1).Return(\"Tom\", 18, nil)\n    c.On(\"GetCustomer\", 2).Return(\"\", 0, errors.New(\"not found\"))\n\n    // Try to use\n    name, age, err := c.GetCustomer(1)\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n    fmt.Println(name, age)\n}\n\ntype CustomerRepository interface {\n    GetCustomer(id int) (name string, age int, err error)\n}\n\ntype CustomerRepositoryMock struct {\n    mock.Mock\n}\n\nfunc (m *CustomerRepositoryMock) GetCustomer(id int) (name string, age int, err error) {\n    args := m.Called(id)\n    return args.String(0), args.Int(1), args.Error(2)\n}\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#implement-mock","title":"Implement Mock","text":"<pre><code>// repositories/promotion_mock.go\npackage repositories\n\ntype promotionRepositoryMock struct {\n    mock.Mock\n}\n\nfunc NewPromotionRepositoryMock() *promotionRepositoryMock{\n    return &amp;promotionRepositoryMock{}\n}\n\nfunc (m *promotionRepositoryMock) GetPromotion() (Promotion, error) {\n    args := m.Called()\n    // Cast type of first value to Promotion\n    return args.Get(0).(Promotion), args.Error(1)\n}\n</code></pre> <pre><code>// services/promotion_test.go\npackage services_test\n\nimport (\n    \"&lt;module-name&gt;/repositories\"\n    \"&lt;module-name&gt;/services\"\n    \"testing\"\n)\n\nfunc TestPromotionCalculateDiscount(t *testing.T) {\n\n    // Arrage\n    promoRepo := repositories.NewPromotionRepositoryMock()\n    promoReop.On(\"GetPromotion\").Return(repositories.Promotion{\n        ID:              1,\n        PurchaseMin:     100,\n        DiscountPercent: 20,\n    }, nil)\n\n\n    promoService := services.NewPromotionService(promoRepo)\n\n    // Act\n    discount, _ := promoService.CalculateDiscount(100)\n    expected := 80\n\n    // Assert\n    assert.Equal(t, expected, discount)\n}\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#change-code","title":"Change Code","text":"<pre><code>// services/promotion_test.go\npackage services_test\n\nfunc TestPromotionCalculateDiscount(t *testing.T) {\n\n    type testCase struct {\n        name            string\n        purchaseMin     int\n        discountPercent int\n        amount          int\n        expected        int\n    }\n\n    cases := []testCase{\n        {name: \"applied 100\", purchaseMin: 100, discountPercent: 20, amount: 100, expected: 80},\n        {name: \"applied 200\", purchaseMin: 100, discountPercent: 20, amount: 200, expected: 160},\n        {name: \"applied 300\", purchaseMin: 100, discountPercent: 20, amount: 300, expected: 240},\n        {name: \"not applied 50\", purchaseMin: 100, discountPercent: 20, amount: 50, expected: 50},\n    }\n\n    for _, c := range cases {\n        t.Run(c.name, func(t *testing.T) {\n            // Arrage\n            promoRepo := repositories.NewPromotionRepositoryMock()\n            promoReop.On(\"GetPromotion\").Return(repositories.Promotion{\n                ID:              1,\n                PurchaseMin:     c.purchaseMin,\n                DiscountPercent: c.discountPercent,\n            }, nil)\n\n\n            promoService := services.NewPromotionService(promoRepo)\n\n            // Act\n            discount, _ := promoService.CalculateDiscount(c.amount)\n            expected := c.expected\n\n            // Assert\n            assert.Equal(t, expected, discount)\n        })\n    }\n\n    t.Run(\"purchase amount zero\", func(t *testing.T) {\n        promoRepo := repositories.NewPromotionRepositoryMock()\n        promoReop.On(\"GetPromotion\").Return(repositories.Promotion{\n            ID:              1,\n            PurchaseMin:     100,\n            DiscountPercent: 20,\n        }, nil)\n\n        promoService := services.NewPromotionService(promoRepo)\n\n        // Act\n        _, err := promoService.CalculateDiscount(0)\n\n        // Assert\n        assert.ErrorIs(t, err, services.ErrZeroAmount)\n        promoRepo.AssertNotCalled(t, \"GetPromotion\")\n    })\n\n    t.Run(\"repository error\", func(t *testing.T) {\n        promoRepo := repositories.NewPromotionRepositoryMock()\n        promoReop.On(\"GetPromotion\").Return(repositories.Promotion{}, errors.New(\"\"))\n\n        promoService := services.NewPromotionService(promoRepo)\n\n        // Act\n        _, err := promoService.CalculateDiscount(100)\n\n        // Assert\n        assert.ErrorIs(t, err, services.ErrRepository)\n    })\n}\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#tags","title":"Tags","text":"<pre><code>// handlers/promotion_test.go\n//go:build unit\n\n...\n</code></pre> <pre><code>go test &lt;module-name&gt;/handlers -v -tags=unit\n\ngo test &lt;module-name&gt;/handlers -v -tags=integation,unit\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#references","title":"References","text":"<ul> <li>https://youtu.be/Wd3O6GcA20w</li> </ul>"},{"location":"tools/lang/powershell/","title":"PowerShell","text":""},{"location":"tools/lang/powershell/win-bat-file/","title":"PowerShell: Batch File","text":"<p>As others have already said, parameters passed through the command line can be accessed in batch files with the notation <code>%1</code> to <code>%9</code>. There are also two other tokens that you can use:</p> <ul> <li><code>%0</code> is the executable (batch file) name as specified in the command line.</li> <li><code>%*</code> is all parameters specified in the command line -- this is very useful if   you want to forward the parameters to another program.</li> </ul> <p>There are also lots of important techniques to be aware of in addition to simply how to access the parameters.</p>"},{"location":"tools/lang/powershell/win-bat-file/#checking-if-a-parameter-was-passed","title":"Checking if a parameter was passed","text":"<p>This is done with constructs like <code>IF \"%~1\"==\"\"</code>, which is <code>true</code> if and only if no arguments were passed at all. Note the tilde character which causes any surrounding quotes to be removed from the value of <code>%1</code>; without a tilde you will get unexpected results if that value includes double quotes, including the possibility of syntax errors.</p>"},{"location":"tools/lang/powershell/win-bat-file/#handling-more-than-9-arguments","title":"Handling more than 9 arguments","text":"<p>If you need to access more than 9 arguments you have to use the command <code>SHIFT</code>. This command shifts the values of all arguments one place, so that <code>%0</code> takes the value of <code>%1</code>, <code>%1</code> takes the value of <code>%2</code>, etc. <code>%9</code> takes the value of the tenth argument (if one is present), which was not available through any variable before calling <code>SHIFT</code> (enter command <code>SHIFT /?</code> for more options).</p> <p><code>SHIFT</code> is also useful when you want to easily process parameters without requiring that they are presented in a specific order. For example, a script may recognize the flags <code>-a</code> and <code>-b</code> in any order. A good way to parse the command line in such cases is</p> <pre><code>:parse\n    IF \"%~1\"==\"\" GOTO endparse\n    IF \"%~1\"==\"-a\" REM do something\n    IF \"%~1\"==\"-b\" REM do something else\n    SHIFT\n    GOTO parse\n\n:endparse\n    REM ready for action!\n</code></pre>"},{"location":"tools/lang/powershell/win-bat-file/#substitution-of-batch-parameters","title":"Substitution of batch parameters","text":"<p>For parameters that represent file names the shell provides lots of functionality related to working with files that is not accessible in any other way. This functionality is accessed with constructs that begin with <code>%~</code>.</p> <p>For example, to get the size of the file passed in as an argument use</p> <pre><code>ECHO %~z1\n</code></pre> <p>To get the path of the directory where the batch file was launched from you can use</p> <pre><code>ECHO %~dp0\n</code></pre> <p>Note: \\ You can view the full range of these capabilities by typing <code>CALL /?</code> in the command prompt.</p>"},{"location":"tools/lang/powershell/win-bat-file/#full-example","title":"Full Example","text":"<pre><code>@echo off\ngoto :init\n\n:usage\n    echo USAGE:\n    echo   %__bat_filename% [flags] \"release argument\"\n    echo.\n    echo.  -h, --help           shows this help\n    echo.  -p, --port value     specifies a port number value\n    goto :eof\n\n:missing_args\n    call :usage\n    echo.\n    echo ****\n    echo MISSING RELEASE ARGUMENT !!!\n    goto :eof\n\n:port\n    echo Port does set from argument and port changes from 8000 to %__port% ...\n    goto :eof\n\n:version\n    if \"%~1\"==\"full\" call :usage &amp; goto :eof\n    echo %__version%\n    goto :eof\n\n:init\n    set \"__name=%~n0\"\n    set \"__port=8000\"\n\n    set \"__bat_filepath=%~0\"\n    set \"__bat_path=%~dp0\"\n    set \"__bat_filename=%~nx0\"\n\n    set \"__version=0.1.0\"\n    set \"__release=\"\n\n:parse\n    if \"%~1\"==\"\"                goto :validate\n\n    if /i \"%~1\"==\"-h\"           call :usage \"%~2\" &amp; goto :end\n    if /i \"%~1\"==\"--help\"       call :usage \"%~2\" &amp; goto :end\n\n    if /i \"%~1\"==\"-v\"           call :version      &amp; goto :end\n    if /i \"%~1\"==\"--version\"    call :version full &amp; goto :end\n\n    if /i \"%~1\"==\"-p\"           set \"__port=%~2\" &amp; shift &amp; shift &amp; call :port &amp; goto :parse\n    if /i \"%~1\"==\"--port\"       set \"__port=%~2\" &amp; shift &amp; shift &amp; call :port &amp; goto :parse\n\n    if not defined __release    set \"__release=%~1\" &amp; shift &amp; goto :parse\n\n    shift\n    goto :parse\n\n:validate\n    if not defined __release call :missing_args &amp; goto :end\n\n:main\n    echo INFO: Start running server with release \"%__release%\" ...\n    call .\\venv\\Scripts\\activate\n    call uvicorn main:app --port %__port%\n\n:end\n    echo.\n    echo End and Clean Up\n    call :cleanup\n    exit /B\n\n:cleanup\n    REM The cleanup function is only really necessary if you\n    REM are _not_ using SETLOCAL.\n\n    set \"__name=\"\n    set \"__port=\"\n\n    set \"__bat_filepath=\"\n    set \"__bat_path=\"\n    set \"__bat_filename=\"\n\n    set \"__release=\"\n    set \"__version=\"\n\n    goto :eof\n</code></pre> <p>Note: \\ <code>setlocal</code> allows you to set environmental variables (that are usually global) only seen inside your batch file and automatically cleans them up when the <code>endlocal</code> call is executed or the batch file ends.</p> <p>The most frequent use of SETLOCAL is to turn on command extensions and allow delayed expansion of variables: <code>setlocal enableextensions enabledelayedexpansion</code></p>"},{"location":"tools/lang/powershell/win-bat-file/#references","title":"References","text":"<ul> <li>https://stackoverflow.com/questions/14286457/using-parameters-in-batch-files-at-windows-command-line</li> <li>https://stackoverflow.com/questions/3973824/windows-bat-file-optional-argument-parsing</li> </ul>"},{"location":"tools/lang/powershell/win-bat-restapi/","title":"PowerShell: RestAPI","text":""},{"location":"tools/lang/powershell/win-bat-restapi/#webrequest","title":"WebRequest","text":"<p>Create Function to get the response code:</p> <pre><code>function Get-UrlStatusCode([string] $Url)\n{ try\n    { (Invoke-WebRequest -Uri $Url -UseBasicParsing -DisableKeepAlive).StatusCode }\n  catch [Net.WebException]\n    { [int]$_.Exception.Response.StatusCode }\n}\n</code></pre> <pre><code>$r = Get-UrlStatusCode \"http://localhost:8000/health\"\nif ($r -eq 200) {\n    echo \"The application can run normally.\"\n} else {\n    echo \"Failed to request to health check endpoint.\"\n}\n</code></pre>"},{"location":"tools/lang/powershell/win-bat-restapi/#restmethod","title":"RestMethod","text":"<pre><code>$Body = @{\n    Cook = \"Barbara\"\n    Meal = \"Pizza\"\n}\n\n$Header = @{\n    \"authorization\" = \"Bearer $token\"\n}\n\n$Parameters = @{\n    Method      = \"POST\"\n    Uri         =  \"https://4besday4.azurewebsites.net/api/AddMeal\"\n    Body        = ($Body | ConvertTo-Json)\n    ContentType = \"application/json\"\n    Headers     = $Header\n}\nInvoke-RestMethod @Parameters\n</code></pre> <p>Note</p> <pre><code>$User = \"GitHubUserName\"\n$Token = \"tokenhere\"\n$base64AuthInfo = [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes(\"$($User):$($Token)\"))\n$Header = @{\n    Authorization = \"Basic $base64AuthInfo\"\n}\n</code></pre>"},{"location":"tools/lang/python/","title":"Python","text":""},{"location":"tools/lang/python/#best-practice","title":"Best Practice","text":"<p>https://medium.com/@Sabrina-Carpenter/10-advanced-code-performance-tips-for-senior-python-developers-47e752e95333</p>"},{"location":"tools/lang/python/#create-package","title":"Create Package","text":"<ul> <li>Advanced Project Structuring \u2014 Python</li> <li>https://medium.com/towards-data-engineering/pip-install-your-de-packages-from-github-fbfeed553555</li> </ul>"},{"location":"tools/lang/python/py-data-structure-for-data-engineer/","title":"Python: Data Structure for Data Engineer","text":"<p>https://medium.com/@SaiParvathaneni/data-structures-for-data-engineers-linked-lists-singly-linked-list-6ad66fd0bd62</p>"},{"location":"tools/lang/python/py-dependency-wheel/","title":"Dependencies Wheel","text":""},{"location":"tools/lang/python/py-dependency-wheel/#use-case-deploy-to-on-premise","title":"Use-case: Deploy to On-premise","text":"<p>This will use when you want to compress all dependencies from internet-able machine to non-internet-able machine</p> <ul> <li>Pack the dependencies to <code>.whl</code> files to wheels folder</li> </ul> <pre><code>pip wheel -w wheels -r requirements.txt\n</code></pre> <ul> <li>Install <code>.whl</code> files in wheel folder</li> </ul> <pre><code>pip install --no-index --find-links=wheels/ -r requirements.txt\n</code></pre>"},{"location":"tools/lang/python/py-dependency-wheel/#what-is-a-python-wheel","title":"What Is a Python Wheel?","text":"<p>A Python <code>.whl</code> file is essentially a ZIP (<code>.zip</code>) archive with a specially crafted filename that tells installers what Python versions and platforms the wheel will support.</p> <p>A wheel is a type of built distribution. In this case, built means that the wheel comes in a ready-to-install format and allows you to skip the build stage required with source distributions.</p> <p>Note</p> <p>It\u2019s worth mentioning that despite the use of the term built, a wheel doesn't contain <code>.pyc</code> files, or compiled Python bytecode.</p> <p>A wheel filename is broken down into parts separated by hyphens:</p> <pre><code>{dist}-{version}(-{build})?-{python}-{abi}-{platform}.whl\n</code></pre>"},{"location":"tools/lang/python/py-dependency-wheel/#references","title":"References","text":"<ul> <li>https://realpython.com/python-wheels/</li> </ul>"},{"location":"tools/lang/python/py-rust-migrate/","title":"Migrate Rust in Python","text":""},{"location":"tools/lang/python/py-rust-migrate/#references","title":"References","text":"<ul> <li>This Is How You Make Your Python Functions 5000% Faster With Rust (Yes, You Read It Right!)</li> </ul>"},{"location":"tools/lang/python/py-sync-multi-processes/","title":"Synchronizing Multi-Processes","text":"<pre><code>import multiprocessing\nimport time\n\n\ndef increase_counter(counter, lock):\n    for _ in range(20):\n        lock.acquire()\n        counter.value() += 10\n        lock.release()\n        time.sleep(0.1)\n\n\ndef decrease_counter(counter, lock):\n    for _ in range(20):\n        with lock:\n            counter.value() -= 10\n        time.sleep(0.1)\n\ncounter = multiprocessing.Value('i', 0)\nlock = multiprocessing.Lock()\n\nincrease = []\ndecrease = []\n\nfor _ in range(5):\n    p = multiprocessing.Process(target=increase_counter, args=(counter, lock, ))\n    p.start()\n    increase.append(p)\n\nfor _ in range(5):\n    p = multiprocessing.Process(target=decrease_counter, args=(counter, lock, ))\n    p.start()\n    decrease.append(p)\n\nfor p in increase:\n    p.join()\n\nfor p in decrease:\n    p.join()\n\nprint(counter)\n</code></pre>"},{"location":"tools/lang/python/py-with-rust/","title":"Python with Rust","text":"<p>Speed Up Your Python Programs with Rust</p>"},{"location":"tools/lang/python/functional/py-func-monad/","title":"Python Functional Programing: PyMonad","text":"<pre><code>pip install pymonad\n</code></pre>"},{"location":"tools/lang/python/functional/py-func-toolz/","title":"Python Functional Programing: Toolz","text":"<pre><code>pip install toolz\n</code></pre>"},{"location":"tools/lang/python/libs/py-cookiecutter/","title":"Cookiecutter","text":"<p>Cookiecutter</p> <p>Quote</p> <p>The name \"cookiecutter\" comes from the concept of a cookie cutter, a tool used to cut cookie dough into a specific shape. In the same way, Cookiecutter allows developers to quickly create new projects with a predefined structure and configuration based on a template.</p> <pre><code>pip install cookiecutter\n</code></pre>"},{"location":"tools/lang/python/libs/py-cookiecutter/#references","title":"References","text":"<ul> <li>Faster Data Experimentation With \"cookiecutter\"</li> </ul>"},{"location":"tools/lang/python/libs/py-joblib/","title":"Python: Joblib Package","text":""},{"location":"tools/lang/python/libs/py-joblib/#installation","title":"Installation","text":"<pre><code>pip install joblib\n</code></pre>"},{"location":"tools/lang/python/libs/py-joblib/#parallel","title":"Parallel","text":""},{"location":"tools/lang/python/libs/py-joblib/#functions","title":"Functions","text":"<pre><code>import os\nimport uuid\nimport requests\nimport pandas as pd\nfrom colorthief import ColorThief\nfrom joblib import Parallel, delayed\n\ndata = pd.read_csv(\"dress.csv\")\n\n\ndef extract_img_colors(url: str):\n    unique_id = uuid.uuid4()\n    with open(f\"{unique_id}.jpg\", \"wb\") as f:\n        f.write(requests.get(url).content)\n    color_thief = ColorThief(f\"{unique_id}.jpg\")\n    palette = color_thief.get_palette(color_count=2)\n    os.remove(f\"{unique_id}.jpg\")\n    return palette[0], palette[1]\n\ncolors = Parallel(n_jobs=-1)(\n    delayed(extract_img_colors)(url)\n    for url in data['image_url'].values[:100]\n)\n</code></pre>"},{"location":"tools/lang/python/libs/py-joblib/#classes","title":"Classes","text":"<pre><code>from joblib import Parallel, delayed\n\n\nclass A:\n\n    def __init__(self, x):\n        self.x = x\n\n    def square(self):\n        return self.x ** 2\n\n\nruns = [A(x) for x in range(20)]\nwith Parallel(n_jobs=6, verbose=5) as parallel:\n    delayed_func = [delayed(lambda x: x.square())(run) for run in runs]\n    output = parallel(delayed_func)\n</code></pre>"},{"location":"tools/lang/python/libs/py-joblib/#references","title":"References","text":"<ul> <li>Python Package: <code>joblib</code></li> </ul>"},{"location":"tools/lang/python/libs/py-pre-commit/","title":"Pre-Commit","text":"<pre><code>pip install pre-commit\n</code></pre>"},{"location":"tools/lang/python/libs/py-pre-commit/#create-custom-pre-commit","title":"Create Custom Pre-Commit","text":"<p>Custom pre-commit hooks for safer code changes</p>"},{"location":"tools/lang/python/libs/py-pydantic/","title":"Python: Pydantic","text":"<p>https://medium.com/towards-data-engineering/pydantic-for-data-engineers-5064ee2e4489</p>"},{"location":"tools/lang/python/libs/py-pytest/","title":"Pytest","text":"<ul> <li>https://towardsdatascience.com/a-guide-to-data-pipeline-testing-with-python-a85e3d37d361</li> </ul>"},{"location":"tools/lang/python/updates/py-py312/","title":"Python 3.12","text":"<ul> <li>5 Handy Python 3.12 New Features That Improve Your Coding Experience</li> </ul>"},{"location":"tools/lang/rust/","title":"Rust","text":"<p>Rust was created by Graydon Hoare in 2006, who was working at Mozilla by the time. It began as a personal project, driven by the desire to create a more secure and efficient language for system-level programming.</p> <p>In 2009, Mozilla began sponsoring the project. The goal was to create a language that could power the next generation of web applications and services, particularly in performance-critical components of Firefox and its layout engine, Servo.</p> <p>Rust drew inspiration from several existing languages. Its syntax has similarities to C++ but also incorporates features from languages like Erlang and Haskell, especially in terms of its approach to concurrency and memory safety.</p> <p>In general terms</p> <p>Rust brings to the table a unique blend of features, primarily focusing on performance, in a similar way to C++, but with modern language features that make development safer.</p>"},{"location":"tools/lang/rust/#safety","title":"Safety","text":"<p>The key features that define safety in rust are:</p> <ul> <li>Memory management (Ownership System)</li> <li>Borrowing and references</li> <li>Lifetimes</li> <li>Error handling</li> </ul>"},{"location":"tools/lang/rust/#ownership-memory-management","title":"Ownership - Memory management","text":"<p>Ownership is a set of rules that govern how a program manages memory. It refers to how different parts of the system\u2019s memory are allocated and controlled.</p> <p>Imagine having an assistant who periodically checks your desk, removing items you\u2019re done with so your workspace remains uncluttered and efficient.</p> <p>All programs have to manage the way they use a computer\u2019s memory while running. Some languages have garbage collection that regularly looks for no longer-used memory as the program runs; in other languages, the programmer must explicitly allocate and free the memory.</p> <p>Ownership in Rust:</p> <p>Rust manages memory through a system of ownership with a set of rules that the compiler checks. If any of the rules are violated, the program won\u2019t compile.</p> <p>Ownership Rules:</p> <ul> <li>Each value in Rust has an owner.</li> <li>There can only be one owner at a time.</li> <li>When the owner goes out of scope, the value will be dropped.</li> </ul> <p>Read More:  Rust Ownership - Explained for Beginners</p> <p>None of the features of ownership will slow down a Rust program while it\u2019s running.</p> <p>This ensures that Rust programs not only benefit from enhanced memory safety and efficiency but also maintain optimal runtime performance, as the ownership model imposes no runtime overhead.</p>"},{"location":"tools/lang/rust/#borrowing-and-references","title":"Borrowing and References","text":"<p>Borrowing and references are fundamental concepts in Rust that work hand-in-hand with the ownership system to ensure memory safety and data race protection without the overhead of garbage collection.</p>"},{"location":"tools/lang/rust/#lifetimes","title":"Lifetimes","text":""},{"location":"tools/lang/rust/#error-handling","title":"Error handling","text":""},{"location":"tools/lang/rust/#speed","title":"Speed","text":""},{"location":"tools/lang/rust/#concurrency","title":"Concurrency","text":""},{"location":"tools/lang/rust/#references","title":"References","text":"<ul> <li> Rust -- A New Titan in Data Science</li> </ul>"},{"location":"tools/lang/rust/rust-cli-app/","title":"CLI Application","text":""},{"location":"tools/lang/rust/rust-cli-app/#getting-started","title":"Getting Started","text":"<p>If you haven\u2019t already, install Rust on your computer. After that, open a terminal and navigate to the directory you want to put your application code into.</p> <p>Start by running <code>cargo new grrs</code> in the directory you store your programming projects in. If you look at the newly created <code>grrs</code> directory, you\u2019ll find a typical setup for a Rust project:</p> <ul> <li>A <code>Cargo.toml</code> file that contains metadata for our project, incl.   a list of dependencies/external libraries we use.</li> <li>A <code>src/main.rs</code> file that is the entry point for our (main) binary.</li> </ul> <p>If you can execute <code>cargo run</code> in the <code>grrs</code> directory and get a <code>\"Hello World\"</code>, you\u2019re all set up.</p> <pre><code>$ cargo new grrs\n     Created binary (application) `grrs` package\n$ cd grrs/\n$ cargo run\n   Compiling grrs v0.1.0 (/Users/pascal/code/grrs)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.70s\n     Running `target/debug/grrs`\nHello, world!\n</code></pre>"},{"location":"tools/lang/rust/rust-cli-app/#references","title":"References","text":"<ul> <li>Command line apps in Rust</li> <li>Rust: Command-line apps</li> </ul>"},{"location":"tools/lang/rust/rust-from-python/","title":"Rust: Python to Rust","text":"<p>How Rust and Python Manage Memory</p> <p>https://towardsdatascience.com/python-to-rust-everything-you-must-know-about-virtual-environments-c1cd0e529835</p>"},{"location":"tools/lang/rust/rust-learning/","title":"Start Learning","text":"<ul> <li>Day 1 of learning Rust : The basics</li> </ul>"},{"location":"tools/lang/scala/","title":"Scala","text":"<p>https://medium.com/odds-team</p>"},{"location":"tools/lang/scala/#installation","title":"Installation","text":"<p>We should to install 2 components for running scalar on local:</p> <ol> <li>Java JDK</li> <li>Scalar</li> </ol> <pre><code>project-directory\n|\n|---&gt; build.sbt\n|---&gt; MyMain.scalar\n</code></pre> <pre><code>sbt run\n</code></pre>"},{"location":"tools/lang/scala/#main-object","title":"Main Object","text":"<pre><code>object MainObject {\n    def main(args: Array[String]) = {\n        ...\n    }\n}\n</code></pre>"},{"location":"tools/lang/scala/scala-advance-feature/","title":"Scala Advance Feature","text":""},{"location":"tools/lang/scala/scala-advance-feature/#lazy-evaluation","title":"Lazy Evaluation","text":"<pre><code>class LazyEval {\n    lazy val l = {\n        println(\"lazy\")\n        9\n    }\n}\n\nval x = new LazyEval;  // Does not print anything\nprintln(y.l);\n</code></pre> <pre><code>lazy\n9\n</code></pre> <ul> <li>Method Lazy</li> </ul> <pre><code>def methodNormal(n: Int) {\n    println(\"Normal\");\n    println(n);\n}\n\ndef methodLazy(n: =&gt; Int) {\n    println(\"Lazy\");\n    println(n);\n}\n\nval add = (a: Int, b: Int) =&gt; {\n    print(\"Add\");\n    a + b;\n}\n\nmethodNormal(add(5, 6));\nmethodLazy(add(5, 6));\n</code></pre> <pre><code>Add\nNormal\n11\nLazy\nAdd\n11\n</code></pre>"},{"location":"tools/lang/scala/scala-advance-feature/#multi-threading","title":"Multi-threading","text":"<pre><code>class ThreadExample extends Thread {\n    override def run() {\n        for (i &lt;- 0 to 3) {\n            println(this.getName() + \":\" + this.getPriority() + \" - \" + i);\n            Thread.sleep(500);\n        }\n    }\n}\n\nvar t1 = new ThreadExample();\nvar t2 = new ThreadExample();\nvar t3 = new ThreadExample();\nt1.start();\nt1.join();\nt1.setName(\"First Thread:1\");\nt2.setName(\"Second Thread:10\");\nt1.setPriority(Thread.MIN_PRIORITY);\nt2.setPriority(Thread.MAX_PRIORITY);\nt1.start();\nt2.start();\n</code></pre> <pre><code>0\n1\n2\n3\nFirst Thread:1 - 0\nSecond Thread:10 - 0\nSecond Thread:10 - 1\nFirst Thread:1 - 1\nSecond Thread:10 - 2\nFirst Thread:1 - 2\nSecond Thread:10 - 3\nFirst Thread:1 - 3\n</code></pre> <ul> <li>Thread Multitasking</li> </ul> <pre><code>class ThreadExample() extends Thread {\n    override def run() {\n        for (i &lt;- 0 to 5) {\n            println(i);\n            Thread.sleep(500);\n        }\n    }\n    def task() {\n        for (i &lt;- 0 to 5) {\n            println(i);\n            Thread.sleep(200);\n        }\n    }\n}\n\nvar t1 = new ThreadExample();\nt1.start();\nt1.task();\n</code></pre> <pre><code>0\n0\n1\n2\n1\n3\n4\n2\n5\n3\n4\n5\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/","title":"Scalar Basic Command Line","text":"<p>Table of Contents:</p> <ul> <li>While Loop</li> <li>For Loop</li> <li>Pattern Matching</li> <li>Break</li> <li>Function</li> <li>Higher-Order Function</li> <li>String</li> <li>Option</li> <li>Exception</li> </ul>"},{"location":"tools/lang/scala/scala-basic-command/#while-loop","title":"While Loop","text":"<ul> <li>Method 01: Common while loop</li> </ul> <pre><code>var x = 0;\nwhile (x &lt; 10) {\n    println(\"x = \" + x );\n    x += 1;\n}\n</code></pre> <ul> <li>Method 02: Switch while loop to the end of doing (do-while)</li> </ul> <pre><code>var x = 0;\ndo {\n    println(\"x = \" + x );\n    x += 1;\n} while (x &lt; 10);\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#for-loop","title":"For Loop","text":"<pre><code>for (i &lt;- 1 to 5) {\n    println(\"i using to \" + i);\n}\n</code></pre> <p>Note: \\ We can use <code>1.to(5)</code>, <code>1 until 6</code>, or <code>1.until(6)</code> for represent range of number from 1 to 5.\\ Optional, if you want to step this range, you can use <code>by</code> like <code>for(i&lt;-1 to 10 by 2)</code>.</p> <ul> <li>Loop</li> </ul> <pre><code>for (i &lt;- 1 to 9; j &lt;- 1 to 3) {\n    println(\"i using until \" + i + \" \" + j);\n}\n</code></pre> <pre><code>i using until 1 1\ni using until 1 2\ni using until 1 3\ni using until 2 1\n...\n</code></pre> <ul> <li>Using <code>List</code></li> </ul> <pre><code>val lst = List(1, 2, 3, 78, 9);';\nfor (i &lt;- lst; if i &lt; 6) {\n    println(\"i using Filters \" + i);\n}\n</code></pre> <pre><code>lst.foreach(print)\n// 123789\n</code></pre> <pre><code>lst.foreach((element:Int) =&gt; print(element + \" \"))\n// 1 2 3 78 9\n</code></pre> <ul> <li>Pass for loop into val</li> </ul> <pre><code>val result = for {i &lt;- lst; if i &lt; 6} yield {\n    i * i\n}\nprintln(\"result = \" + result);\n</code></pre> <pre><code>result = List(1, 4, 9)\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#break","title":"Break","text":"<pre><code>import scala.util.control.Breaks._\n\n// Breakable method to avoid exception\nbreakable {\n    for (i &lt;- 1 to 10 by 2) {\n        if (i == 7)\n            break\n        else\n            println(\"i using to \" + i)\n    }\n}\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#pattern-matching","title":"Pattern Matching","text":"<pre><code>def search(a: Any): Any = a match {\n    case 1  =&gt; println(\"One\");\n    case \"Two\" =&gt; println(\"Two\");\n    case \"Hello\" =&gt; println(\"Hello\");\n    case _ =&gt; println(\"No\");\n}\n</code></pre> <pre><code>val i = 7;\ni match {\n    case 1 | 3 | 5 | 7 | 9 =&gt; println(\"odd\");\n    case 2 | 4 | 6 | 8 =&gt; println(\"even\");\n}\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#function","title":"Function","text":"<pre><code>def add(x: Int, y: Int): Int = {\n    return x + y;\n}\n</code></pre> <pre><code>def subtract(x: Int, y: Int): Int = {\n    x - y;\n}\n</code></pre> <pre><code>def multiply(x: Int, y: Int): Int = x * y;\n</code></pre> <pre><code>def devide(x: Int, y: Int) = x / y;\n</code></pre> <p>Note: \\ You can ignore to write <code>return</code> in last line of function body that will auto use the last line to return.</p> <pre><code>object Math {\n    def square(x: Int) = x * x;\n}\n\n// Use method function of object with space seperator\nprintln(Math square 3);\n</code></pre> <ul> <li>Recursion Function</li> </ul> <pre><code>// We can set default value to `y` parameter\ndef plusRecursion(x: Int, y: Int = 40): Int = {\n    if (y == 0)\n        0\n    else\n        x + plusRecursion(x, y - 1)\n}\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#higher-order-function","title":"Higher-Order Function","text":"<ul> <li>Passing a Function as Parameter</li> </ul> <pre><code>def math(x: Double, y: Double, f: (Double, Double) =&gt; Double): Double = f(x, y);\n</code></pre> <pre><code>val result = math(50, 20, (x, y) =&gt; x max y);\n</code></pre> <pre><code>def math(x: Double, y: Double, z: Double, f: (Double, Double) =&gt; Double): Double = f(f(x, y), z);\n</code></pre> <pre><code>val result = math(50, 20, 10, (x, y) =&gt; x + y);\n</code></pre> <p>Note: \\ You can use wild-card like <code>math(50, 20, 10, _ + _);</code></p> <ul> <li>Function Composition</li> </ul> <pre><code>def addTwo(x: Int): Int = x + 2;\ndef multiplyTwo(x: Int): Int = x * 2;\n</code></pre> <pre><code>val result = multiplyTwo(addTwo(10));\n</code></pre> <ul> <li>Anonymous (lambda) Function</li> </ul> <pre><code>// Anonymous function by using =&gt; (rocket)\nval result = (x: Int, y: Int) =&gt; x + y;\n</code></pre> <pre><code>// Anonymous function by using _ (underscore) wild-card\nval result = (_: Int) + (_: Int)\n</code></pre> <ul> <li>Multiline Expression</li> </ul> <pre><code>def add(x: Int, y: Int) = {\n    x +\n    y\n}\n</code></pre> <pre><code>def add(x: Int, y: Int) = {\n    (x\n    + y)\n}\n</code></pre> <ul> <li>Partially Applied Function</li> </ul> <pre><code>val sum(x: Int, y: Int, z: Int) =&gt; x + y + z;\n</code></pre> <pre><code>val f = sum(10, 20, _: Int);\nval result = f(200);\n</code></pre> <ul> <li>Closures</li> </ul> <pre><code>var number = 10;\nval add = (x: Int) =&gt; x + number;\n\ndef main(args: Array[String]) {\n    number = 100;\n    println(add(20)); // stdout will show 120\n}\n</code></pre> <pre><code>val add = (x: Int) =&gt; {\n    number = x + number;\n    number;\n}\n\ndef main(args: Array[String]) {\n    number = 100;\n    println(add(20)); // stdout will show 120\n    println(number); // stdout will show 120\n}\n</code></pre> <ul> <li>Function Currying</li> </ul> <pre><code>def add(x: Int)(y: Int) = {\n    x + y;\n}\n</code></pre> <pre><code>val result = add(10)(10);\n</code></pre> <pre><code>val add40 = add(40)_;\nval result = add40(100);\n</code></pre> <p>Note: \\ You can write with oneline like <code>def add (x: Int) = (y: Int) =&gt; x + y;</code>, and Change to use with <code>val add20 = add(20)</code></p> <ul> <li>Nested Function</li> </ul> <pre><code>def add(x: Int, y: Int, z: Int) = {\n    def addTwo(a: Int, b: Int) = {\n        a + b;\n    }\n    addTwo(x, addTwo(y, z));\n}\n</code></pre> <ul> <li>Function with Variable Length Parameters</li> </ul> <pre><code>def addAll(args: Int*) = {\n    var sum = 0;\n    for (a &lt;- args) sum += a;\n    sum;\n</code></pre> <pre><code>val result = addAll(1, 2, 3, 4, 5);\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#string","title":"String","text":"<pre><code>val str1: String = \"Hello World\";\nval str2: String = \" Max\";\n</code></pre> <pre><code>str1.length();  // 11\nstr1.concat(str2);  // Hello World Max\nstr1 + str2;  // Hello World Max\nstr1.equals(str2);  // false\nstr1.compareTo(str2);  // -7\nstr1.substring(0, 5);  // Hello\n</code></pre> <ul> <li>String Interpolation</li> </ul> <pre><code>var s1 = \"Scala string example\";\nvar version = 2.12;\n\nprintln(f\"This is $s1%s, scala version is $version%2.2f\");  // This is Scala string example, scala version is 2.12\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#option","title":"Option","text":"<pre><code>val lst = List(1, 2, 3);\nval map = Map(1 -&gt; \"Tom\", 2 -&gt; \"Sara\");\nval opt: Option[Int] = None;\n</code></pre> <pre><code>lst.find(_ &gt; 3);  // None\nlst.find(_ &gt; 2);  // Some(3)\nmap.get(1);  // Some(Tom)\nmap.get(3);  // None\n</code></pre> <pre><code>lst.find(_ &gt; 2).get;  // 3\nmap.get(1).get;  // Tom\nmap.get(3).getOrElse(\"No name found\");  // No name found\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#exception","title":"Exception","text":"<ul> <li>Try Catch</li> </ul> <pre><code>class ExceptionExample {\n    def divide(a: Int, b: Int) = {\n        try {\n            a / b;\n            var arr = Array(1,2);\n            arr(10);\n        } catch {\n            case e: ArithmeticException =&gt; println(e)\n            case ex: Throwable =&gt; println(\"found a unknown exception: \" + ex)\n        } finally {\n            println(\"Finally block always executes\")\n        }\n        println(\"Rest of the code is executing...\");\n    }\n}\n\nvar e = new ExceptionExample();\ne.divide(100, 0);\ne.divide(100,10)\n</code></pre> <pre><code>java.lang.ArithmeticException: / by zero\nFinally block always executes\nRest of the code is executing...\nfound a unknown exception: java.lang.ArrayIndexOutOfBoundsException: 10\nFinally block always executes\nRest of the code is executing...\n</code></pre> <ul> <li>Throw keyword</li> </ul> <pre><code>class ExceptionExample {\n    def validate(age: Int) = {\n        if (age &lt; 18)\n            throw new ArithmeticException(\"You are not eligible\")\n        else println(\"You are eligible\")\n    }\n}\n\nvar e = new ExceptionExample();\ne.validate(10);\n</code></pre> <pre><code>java.lang.ArithmeticException: You are not eligible\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/","title":"Scala Collection","text":"<p>Scala provides rich set of collection library. It contains classes and traits to collect data. These collections can be mutable or immutable. You can use them according to your requirement. <code>Scala.collection.mutable</code> package contains all the mutable collections. You can add, remove and update data while using this package.</p> <p></p> <p><code>Scala.collection.immutable</code> contains all the immutable collections. It does not allow you to modify data. Scala imports this package by default. If you want mutable collection, you must import <code>Scala.collection.mutable</code> package in your code.</p> <p>Note: \\ Traversable is a trait and used to traverse collection elements. It is a base trait for all scala collections. It implements the methods which are common to all collections.</p> Method Description def head: A It returns the first element of collection. def init: Traversable[A] It returns all elements except last one. def isEmpty: Boolean It checks whether the collection is empty or not. It returns either true or false. def last: A It returns the last element of this collection. def max: A It returns the largest element of this collection. def min: A It returns smallest element of this collection def size: Int It is used to get size of this traversable and returns a number of elements present in this traversable. def sum: A It returns sum of all elements of this collection. def tail: Traversable[A] It returns all elements except first. def toArray: Array[A] It converts this collection to an array. def toList: List[A] It converts this collection to a list. def toSeq: Seq[A] It converts this collection to a sequence. def toSet[B &gt;: A]: immutable.Set[B] It converts this collection to a set. <p>Note: \\ Iterable is a next trait from the top of the hierarchy and a base trait for iterable collections. It extends traversable trait and provides important methods to concrete classes.</p> <p>Table of Contents:</p> <ul> <li>Array</li> <li>Seq</li> <li>List</li> <li>Vector</li> <li>Queue</li> <li>Stream</li> <li>Set</li> <li>HashSet</li> <li>BitSet</li> <li>ListSet</li> <li>Map</li> <li>HashMap</li> <li>ListMap</li> <li>Tuple</li> <li>Method of Collections</li> </ul>"},{"location":"tools/lang/scala/scala-collection/#array","title":"Array","text":"<p>Array is a collection of mutable values. It is an index based data structure which starts from 0 index to n-1 where n is length of array.</p> <p>Scala arrays can be generic. It means, you can have an <code>Array[T]</code>, where <code>T</code> is a type parameter or abstract type. Scala arrays are compatible with Scala sequences - you can pass an <code>Array[T]</code> where a <code>Seq[T]</code> is required. It also supports all the sequence operations.</p>"},{"location":"tools/lang/scala/scala-collection/#single-dimensional-array","title":"Single Dimensional Array","text":"<p>Syntax for Single Dimensional Array:</p> <pre><code>var arrayName : Array[arrayType] = new Array[arrayType](arraySize);\nvar arrayName = new Array[arrayType](arraySize);\nvar arrayName : Array[arrayType] = new Array(arraySize);\nvar arrayName = Array(element1, element2 ... elementN);\n</code></pre> <pre><code>val arr = Array(1, 2, 3, 4, 5);\nfor (x &lt;- arr) {\n    println(x);\n}\n</code></pre> <pre><code>1\n2\n3\n4\n5\n</code></pre> <p>Note: You can iterate an array by using foreach Loop like, <code>arr.foreach((x: Int) =&gt; println(x))</code></p> <ul> <li>Array Function</li> </ul> <pre><code>import Array._\n\nval arr1 = Array(1, 2, 3, 4, 5);\nval arr2 = Array(6, 7, 8);\nval result = concat(arr1, arr2);  // Array(1, 2, 3, 4, 5, 6, 7, 8)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#multidimensional-array","title":"Multidimensional Array","text":"<p>Multidimensional Array Syntax:</p> <pre><code>var arrayName = Array.ofDim[ArrayType](NoOfRows,NoOfColumns);\nvar arrayName = Array(Array(element...), Array(element...), ...);\n</code></pre> <pre><code>var arr = Array.ofDim[Int](2,2);\narr(1)(0) = 15;\nfor( i &lt;- 0 to 1){\n    for( j &lt;- 0 to 1){\n        print(\" \" + arr(i)(j))\n    }\n    println();\n}\n</code></pre> <pre><code>0 0\n15 0\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#seq","title":"Seq","text":"<p>Seq is a trait which represents indexed sequences that are guaranteed immutable. You can access elements by using their indexes. It maintains insertion order of elements.</p> <p>Sequences support a number of methods to find occurrences of elements or subsequences. It returns a list.</p> <p>Commonly used Methods of Seq:</p> Method Description def containsA1 &gt;: A: Boolean Check whether the given element present in this sequence. def copyToArray(xs: Array[A], start: Int, len: Int): Unit It copies the seq elements to an array. def endsWithB: Boolean It tests whether this sequence ends with the given sequence or not. def head: A It selects the first element of this seq collection. def indexOf(elem: A): Int It finds index of first occurrence of a value in this immutable sequence. def isEmpty: Boolean It tests whether this sequence is empty or not. def lastIndexOf(elem: A): Int It finds index of last occurrence of a value in this immutable sequence. def reverse: Seq[A] It returns new sequence with elements in reversed order. <pre><code>import scala.collection.immutable.Seq\n\nvar seq: Seq[Int] = Seq(52, 85, 1, 8, 3, 2, 7);\nseq.reverse;  // List(7, 2, 3, 8, 1, 85, 52)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#list","title":"List","text":"<p>List is used to store ordered elements. It extends LinearSeq trait. It is a class for immutable linked lists. This class is good for last-in-first-out (LIFO), stack-like access patterns.</p> <p>is a next trait from the top of the hierarchy and a base trait for iterable collections. It extends traversable trait and provides important methods to concrete classes.</p> <pre><code>val lst: List[Int] = List(1, 8, 5, 6, 9, 58, 23, 15, 4);\nval lst2: List[Int] = List(88, 100);\nval lstNil: List[Int] = 1 :: 5 :: 9 :: Nil;  // List(1, 5, 9)\nval lstTwo: List[Int] = List.fill(5)(2);  // List(2, 2, 2, 2, 2)\n</code></pre> <pre><code>val lst3 = lst ++ lst2;  // List(1, 8, 5, 6, 9, 58, 23, 15, 4, 88, 100)\nval lst4 = lst3.sorted;  // List(1, 4, 5, 6, 8, 9, 15, 23, 58, 88, 100)\nval lst5 = lst3.reverse;  // List(100, 88, 4, 15, 23, 58, 9, 6, 5, 8, 1)\n</code></pre> <pre><code>var result = 0 :: lst;  // List(0, 1, 8, 5, 6, 9, 58, 23, 15, 4);\nvar result = lst.max;  // 58\n</code></pre> <pre><code>var sum: Int = 0;\nval result = lst2.foreach(sum += _);  // 188\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#vector","title":"Vector","text":"<p>Vector is a general-purpose, immutable data structure. It provides random access of elements. It is good for large collection of elements.</p> <p>It extends an abstract class AbstractSeq and IndexedSeq trait.</p> <pre><code>import scala.collection.immutable.Vector\n\nvar vector: Vector[Int] = Vector(5, 8, 3, 6, 9, 4);\nvar vector2 = Vector.empty;\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#queue","title":"Queue","text":"<p>Queue implements a data structure that allows inserting and retrieving elements in a first-in-first-out (FIFO) manner.</p> <p>In scala, Queue is implemented as a pair of lists. One is used to insert the elements and second to contain deleted elements. Elements are added to the first list and removed from the second list.</p> <pre><code>import scala.collection.immutable._\n\nvar queue: Queue[Int] = Queue(1, 5, 6, 2, 3, 9, 5, 2, 5);\n</code></pre> <pre><code>queue.front;  // 1\nqueue.enqueue(100);  // Queue(1, 5, 6, 2, 3, 9, 5, 2, 5, 100)\nqueue.dequeue;  // (1,Queue(5, 6, 2, 3, 9, 5, 2, 5))\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#stream","title":"Stream","text":"<p>Stream is a lazy list. It evaluates elements only when they are required. This is a feature of scala. Scala supports lazy computation. It increases performance of your program.</p> <pre><code>val stream = 100 #:: 200 #:: 85 #:: Stream.empty;  // Stream(100, ?)\nval stream2 = (1 to 10).toStream;  // Stream(1, ?)\n</code></pre> <pre><code>stream2.head;  // 1\nstream2.take(10);  // Stream(1, ?)\nstream.map{_*2};  // Stream(200, ?)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#set","title":"Set","text":"<p>It is used to store unique elements in the set. It does not maintain any order for storing elements. You can apply various operations on them. It is defined in the <code>Scala.collection.immutable</code> package.</p> <p>Scala Set Syntax:</p> <pre><code>val variableName: Set[Type] = Set(element1, element2,... elementN);\nval variableName = Set(element1, element2,... elementN);\n</code></pre> <pre><code>import scala.collection.immutable._\n\nval alphabet = Set(\"C\",\"F\",\"H\",\"G\");\nval alphabet2 = Set(\"A\",\"B\", \"D\",\"E\");\n</code></pre> <pre><code>var result = alphabet ++ alphabet2;  // Set(E, F, G, H, A, B, C, D);\nvar result = alphabet.size;  // 4\nvar result = alphabet.contains(\"G\");  // true\n</code></pre> <pre><code>alphabet += \"R\";  // Set(E, F, G, H, A, B, C, D, R);\nalphabet += \"A\";  // Set(E, F, G, H, A, B, C, D, R);\nalphabet -= \"C\";  // Set(E, F, G, H, A, B, D, R);\n</code></pre> <p>Note: \\ We can use <code>variable(something)</code> instead <code>variable.contains(something)</code>.</p> <p>Note: \\ If you want to create set that able to sort, you can use <code>scala.collection.immutable.SortedSet</code>.</p>"},{"location":"tools/lang/scala/scala-collection/#hashset","title":"HashSet","text":"<p>HashSet is a sealed class. It extends AbstractSet and immutable Set trait. It uses hash code to store elements. It neither maintains insertion order nor sorts the elements.</p> <pre><code>import scala.collection.immutable.HashSet\n\nvar hashset = HashSet(4, 2, 8, 0, 6, 3, 45);\nhashset.foreach((element: Int) =&gt; println(element));\n</code></pre> <pre><code>0\n6\n2\n45\n3\n8\n4\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#bitset","title":"BitSet","text":"<p>Bitsets are sets of non-negative integers which are represented as variable-size arrays of bits packed into 64-bit words. The memory footprint of a bitset is determined by the largest number stored in it. It extends Set trait.</p> <pre><code>import scala.collection.immutable.BitSet\n\nvar numbers = BitSet(1, 5, 8, 6, 9, 0);\nnumbers.foreach((element: Int) =&gt; println(element));\n</code></pre> <pre><code>0\n1\n5\n6\n8\n9\n</code></pre> <pre><code>numbers += 20;  // BitSet(0 1 5 6 8 9 20)\nnumbers -= 0;  // BitSet(1 5 6 8 9 20)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#listset","title":"ListSet","text":"<p>In scala, ListSet class implements immutable sets using a list-based data structure. Elements are stored internally in reversed insertion order, which means the newest element is at the head of the list. It maintains insertion order.</p> <p>This collection is suitable only for a small number of elements. You can create empty <code>ListSet</code> either by calling the constructor or by applying the function <code>ListSet.empty</code>. Its iterate and traversal methods visit elements in the same order in which they were first inserted.</p> <pre><code>import scala.collection.immutable.ListSet\n\nvar listset = ListSet(4, 2, 8, 0, 6, 3, 45);\nlistset.foreach((element: Int) =&gt; println(element));\n</code></pre> <pre><code>4\n2\n8\n0\n6\n3\n45\n</code></pre> <pre><code>var listset: ListSet[String] = new ListSet();\nvar listset2: ListSet[String] = ListSet.empty;\n\nlistset += \"India\";  // ListSet(India)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#map","title":"Map","text":"<p>Map is used to store elements. It stores elements in pairs of key and values. In scala, you can create map by using two ways either by using comma separated pairs or by using rocket operator.</p> <pre><code>val map: Map[String, String] = Map((\"A\", \"Apple\"), (\"B\", \"Ball\"));\nval map2 = Map(\"D\" -&gt; \"Dog\");\n</code></pre> <pre><code>var result = map + (\"C\" -&gt; \"Cat\");  // Map(A -&gt; Apple, B -&gt; Ball, C -&gt; Cat)\nvar result = map - (\"B\");  // Map(A -&gt; Apple)\nvar result = map ++ map2;  // Map(A -&gt; Apple, B -&gt; Ball, D -&gt; Dog)\n</code></pre> <pre><code>map.keys;  // Set(A, B, C)\nmap.values;  // MapLike.DefaultValuesIterable(Apple, Ball, Cat)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#hashmap","title":"HashMap","text":"<p>HashMap is used to store element. It use hash code to store elements and return a map.</p> <pre><code>import scala.collection.immutable.HashMap\n\nvar hashMap = new HashMap();\nvar hashMap2 = HashMap(\"A\" -&gt; \"Apple\", \"B\" -&gt; \"Ball\", \"C\" -&gt; \"Cat\");\n\nhashMap2.foreach {\n    case (key, value) =&gt; println(key + \" -&gt; \" + value)\n}\n</code></pre> <pre><code>A -&gt; Apple\nB -&gt; Ball\nC -&gt; Cat\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#tuple","title":"Tuple","text":"<p>A tuple is a collection of elements in ordered form. If there is no element present, it is called empty tuple. You can use tuple to store any type of data. You can store similar type or mix type data also. You can use it to return multiple values from a function.</p> <pre><code>val tp = (1, 2.5, \"Apple\");\nval tp2 = 1 -&gt; \"Tom\";  // (1, Tom)\n</code></pre> <pre><code>tp.productIterator.foreach(println)\n</code></pre> <pre><code>1\n2.5\nApple\n</code></pre> <pre><code>tp._1;  // 1\ntp._2;  // 2.5\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#listmap","title":"ListMap","text":"<p>This class implements immutable maps by using a list-based data structure. It maintains insertion order and returns <code>ListMap</code>. This collection is suitable for small elements.</p> <pre><code>import scala.collection.immutable._\n\n\nvar listMap = ListMap(\"Rice\" -&gt; \"100\", \"Wheat\" -&gt; \"50\", \"Gram\" -&gt; \"500\");\nvar emptyListMap = new ListMap();\nvar emptyListMap2 = ListMap.empty;\n</code></pre> <pre><code>var newListMap = listMap + (\"Pulses\" -&gt; \"550\");\nnewListMap.foreach {\n    case (key, value) =&gt; println(key + \" -&gt; \" + value)\n}\n</code></pre> <pre><code>Rice -&gt; 100\nWheat -&gt; 50\nGram -&gt; 500\nPulses -&gt; 550\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#method-of-collections","title":"Method of Collections","text":""},{"location":"tools/lang/scala/scala-collection/#map-filter","title":"Map &amp; Filter","text":"<pre><code>val lst = List(1, 2, 3, 4, 5);\nvar result = lst.map(x =&gt; x * 2);  // List(2, 4, 6, 8, 10)\nvar result = lst.map(x =&gt; List(x, x + 1));  // List(List(1, 2), List(2, 3), List(3, 4), List(4, 5), List(5, 6))\nvar result = lst.flatMap(x =&gt; List(x, x + 1));  // List(1, 2, 2, 3, 3, 4, 4, 5, 5, 6)\nvar result = lst.fillter(x =&gt; x % 2 == 0);  // List(2, 4)\n</code></pre> <pre><code>val map = Map(1 -&gt; \"Apple\", 2 -&gt; \"Ball\");\nvar result = map.map(x =&gt; \"hi\" + x);  // List(hi(1,Apple), hi(2,Ball))\nvar result = map.mapValues(x =&gt; \"hi\" + x);  // Map(1 -&gt; hi Apple, 2 -&gt; hi Ball)\n</code></pre> <pre><code>\"Hello\".map(_.toUpper);  // HELLO\n</code></pre> <pre><code>List(List(1, 2, 3), List(3, 4)).flatten;  // List(1, 2, 3, 3, 4)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#reduce-fold-or-scan","title":"Reduce &amp; Fold or Scan","text":"<pre><code>val lst = List(1, 2, 3, 10, 13);\nvar result = lst.reduceLeft(_ + _);  // 29\nvar result = lst.foldLeft(0)(_ + _);  // 29\nvar result = lst.foldLeft(100)(_ + _);  // 129\nvar result = lst.scanLeft(100)(_ + _);  // List(100, 101, 103, 106, 116, 129)\n\nval lst2 = List(\"A\", \"B\", \"C\");\nvar result = lst2.reduceLeft(_ + _);  // ABC\nvar result = lst2.foldLeft(\"z\")(_ + _);  // zABC\nvar result = lst2.scanLeft(\"z\")(_ + _);  // List(z, zA, zAB, zABC)\n</code></pre> <pre><code>var result = lst.reduceLeft((x, y) =&gt; {println(x + \" , \" + y); x + y;});\n</code></pre> <pre><code>1, 2\n3, 3\n6, 10\n16, 13\n29\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#references","title":"References","text":"<ul> <li>https://www.youtube.com/watch?v=OfngvXKNkpM</li> <li>https://www.javatpoint.com/</li> </ul>"},{"location":"tools/lang/scala/scala-oop-concepts/","title":"Scala OOPs Concepts","text":"<p>Table of Contents:</p> <ul> <li>Object and Class</li> <li>[Singleton Object]</li> <li>[Companion Object]</li> </ul>"},{"location":"tools/lang/scala/scala-oop-concepts/#object-and-class","title":"Object and Class","text":"<p>Unlike java, scala is a pure object-oriented programming language. It allows us to create object and class so that you can develop object-oriented applications.</p> <ul> <li>Object</li> </ul> <p>Object is a real world entity. It contains state and behavior. Laptop, car,   cell phone are the real world objects.</p> <p>Object typically has two characteristics:</p> <ul> <li>State: data values of an object are known as its state.</li> <li>Behavior: functionality that an object performs is known as its behavior.</li> </ul> <p>Note: \\ Object in scala is an instance of class. It is also known as runtime entity.</p> <ul> <li>Class</li> </ul> <p>Class is a template or a blueprint. It is also known as collection of objects   of similar type.</p> <p>In scala, a class can contain:</p> <ul> <li>Data member</li> <li>Member method</li> <li>Constructor</li> <li>Block</li> <li>Nested class</li> <li>Super class information etc.</li> </ul> <p>You must initialize all instance variables in the class. There is no default   scope. If you don't specify access scope, it is public. There must be an object   in which main method is defined. It provides starting point for your program.   Here, we have created an example of class.</p> <pre><code>class Student {\n    var id: Int = 0;\n    var name: String = null;\n}\n\nvar s = new Student();\ns.id = 10;\nprintln(s.id + \" \" + s.name);  // 10 null\n</code></pre> <pre><code>// Primary constructor with `val` by default\nclass Student(id: Int, name: String) {\n    def this() {\n        this(99, \"Default\");\n    }\n\n    def this(id: Int) {\n        this(id, \"Default);\n    }\n\n    def show() {\n        println(id + \" \" + name)\n    }\n}\n\nvar s = new Student(100, \"Martin\");\ns.show();  // 100 Martin\n\nvar s = new Student();  // Student(99, Default)\nvar s = new Student(200);  // Student(200, Default)\n</code></pre> <pre><code>class User(private var name: String, val age: Int) {\n    def printName{ println(name) }\n}\n\nvar u = new User(\"Max\", 28);\nu.printName;  // Max\n</code></pre> <pre><code>// Anonymous object\nclass Arithmetic {\n    def add(a: Int, b: Int) {\n        var add = a + b;\n        println(\"sum = \" + add);\n    }\n}\n\nnew Arithmetic().add(10, 10);  // Sum = 20\n</code></pre> <p>Note: \\ In scala, <code>this</code> is a keyword and used to refer current object. You can call instance variables, methods, constructors by using <code>this</code> keyword.</p> <p>Note: \\ Access modifier is used to define accessibility of data and our code to the outside world. Scala provides only three types of access modifiers, which are given below:</p> <ul> <li>No modifier</li> <li>Protected: <code>protected var ...</code></li> <li>Private: <code>private var ...</code></li> </ul>"},{"location":"tools/lang/scala/scala-oop-concepts/#singleton-object","title":"Singleton Object","text":"<p>Singleton object is an object which is declared by using object keyword instead by class. No object is required to call methods declared inside singleton object.</p> <p>In scala, there is no static concept. So scala creates a singleton object to provide entry point for your program execution.</p> <p>If you don't create singleton object, your code will compile successfully but will not produce any output. Methods declared inside Singleton Object are accessible globally. A singleton object can extend classes and traits.</p> <pre><code>object SingletonObject{\n    def hello(){\n        println(\"Hello, This is Singleton Object\")\n    }\n}\n\nSingletonObject.hello();  // Hello, This is Singleton Object\n</code></pre>"},{"location":"tools/lang/scala/scala-oop-concepts/#companion-object","title":"Companion Object","text":"<p>In scala, when you have a class with same name as singleton object, it is called companion class and the singleton object is called companion object.</p> <p>The companion class and its companion object both must be defined in the same source file.</p> <pre><code>class ComapanionClass{\n    def hello(){\n        println(\"Hello, this is Companion Class.\")\n    }\n}\n\nnew ComapanionClass().hello();  // Hello, this is Companion Class.\n</code></pre>"},{"location":"tools/lang/scala/scala-oop-concepts/#inheritance","title":"Inheritance","text":"<p>Inheritance is an object-oriented concept which is used to re-usability of code. You can achieve inheritance by using extends keyword. To achieve inheritance a class must extend to other class. A class which is extended called super or parent class. a class which extends class is called derived or base class.</p> <p>Inheritance Syntax:</p> <pre><code>class SubClassName extends SuperClassName() {\n  /* Write your code\n  *  methods and fields etc.\n  */\n}\n</code></pre> <p>Types of Inheritance in Scala:</p> <p>Scala supports various types of inheritance including single, multilevel, multiple, and hybrid. You can use single, multilevel and hierarchical in your class. Multiple and hybrid can only be achieved by using traits. Here, we are representing all types of inheritance by using pictorial form.</p> <p></p> <pre><code>class Employee {\n    val salary: Float = 10000;\n    def title : String = \"Salary: \" + salary;\n}\n\nclass Programmer(name: String) extends Employee {\n    override val salary: Float = 15000;\n    var bonus: Int = 5000;  // this cannot override a mutable variable\n    override def title: String = name + \" Salary: \" + salary + \", Bonus: \" + bonus;\n}\n\nvar p = new Programmer(\"Jax\");\np.title;  // Jax Salary: 15000, Bonus: 5000\n</code></pre>"},{"location":"tools/lang/scala/scala-oop-concepts/#final","title":"Final","text":"<p>Final is a keyword, which is used to prevent inheritance of super class members into derived class. You can declare final variables, methods and classes also.</p> <pre><code>class Vehicle{\n    final val speed: Int = 60;\n}\n\nclass Bike extends Vehicle{\n    override val speed: Int = 100;\n}\n</code></pre> <pre><code>Error - value speed cannot override final member\n</code></pre> <p>Note: \\ The <code>final</code> can use to method and class like <code>final def ...</code> and <code>final class ...</code>.</p>"},{"location":"tools/lang/scala/scala-oop-concepts/#case-classes","title":"Case Classes","text":"<p>Scala case classes are just regular classes which are immutable by default and decomposable through pattern matching.</p> <p>It uses equal method to compare instance structurally. It does not use new keyword to instantiate object.</p> <p>Warning: \\ All the parameters listed in the case class are public and immutable by default.</p> <pre><code>case class Task(id: Int, title: String, var state: Int = 1) {\n    def next() { if (state == 3) 3 else state += 1 }\n}\n\nval buyBanana = Task(id = 0, title = \"Buy Banana\");  // Task(0, Buy Banana, 3)\n</code></pre>"},{"location":"tools/lang/scala/scala-oop-concepts/#abstract-class","title":"Abstract Class","text":"<p>A class which is declared with abstract keyword is known as abstract class. An abstract class can have abstract methods and non-abstract methods as well. Abstract class is used to achieve abstraction. Abstraction is a process in which we hide complex implementation details and show only functionality to the user.</p> <p>In scala, we can achieve abstraction by using abstract class and trait. We have discussed these in detail here.</p> <pre><code>abstract class Bike(name: String) {\n    var b: Int = 20;\n\n    // Abstract method\n    def run()\n\n    // Non-abstract method\n    def performance() {\n        println(\"Performance awesome\")\n    }\n}\n\nclass Hero extends Bike {\n    def run() {\n        println(\"running fine...\")\n    }\n}\n</code></pre>"},{"location":"tools/lang/scala/scala-oop-concepts/#trait","title":"Trait","text":"<p>A trait is like an interface with a partial implementation. In scala, trait is a collection of abstract and non-abstract methods. You can create trait that can have all abstract methods or some abstract and some non-abstract methods.</p> <p>A variable that is declared either by using <code>val</code> or <code>var</code> keyword in a trait get internally implemented in the class that implements the <code>trait</code>. Any variable which is declared by using <code>val</code> or <code>var</code> but not initialized is considered abstract.</p> <p>Traits are compiled into Java interfaces with corresponding implementation classes that hold any methods implemented in the traits.</p> <pre><code>trait Printable {\n    def print()\n}\n\nclass A4 extends Printable{\n  def print() {\n        println(\"Hello I am A4\")\n    }\n}\n</code></pre> <ul> <li>Implementing Multiple Traits in a Class</li> </ul> <pre><code>trait Printable {\n    def print()\n\n    // Non-abstract method\n    def title() {\n        println(\"This is show method\")\n    }\n}\n\ntrait Showable {\n    def show()\n}\n\nclass A6 extends Printable with Showable {\n    def print() {\n        println(\"This is printable\")\n    }\n    def show() {\n        println(\"This is showable\");\n    }\n}\n</code></pre> <p>Note: \\ On above example, <code>Printable</code> can be abstract class but <code>Showable</code> can not. That mean <code>A6</code> need to inherited only one abstract class and <code>Showable</code> should be trait. \\ <code>class ClassName extends AbstractClass with FirstTrait with SecondTrait { ... }</code></p>"},{"location":"tools/lang/scala/scala-oop-concepts/#trait-mixins","title":"Trait Mixins","text":"<p>In scala, trait mixins means you can extend any number of traits with a class or abstract class. You can extend only traits or combination of traits and class or traits and abstract class.</p> <p>It is necessary to maintain order of mixins otherwise compiler throws an error.</p> <pre><code>trait Print {\n    def print()\n}\n\nabstract class PrintA4 {\n    def printA4()\n}\n\nclass A6 extends PrintA4 with Print {\n    // Trait print\n    def print() {\n        println(\"print sheet\");\n    }\n    // Abstract class printA4\n    def printA4() {\n        println(\"Print A4 Sheet\");\n    }\n}\n\nclass A6After extends PrintA4 {\n    // Trait print\n    def print() {\n        println(\"print sheet\");\n    }\n    // Abstract class printA4\n    def printA4() {\n        println(\"Print A4 Sheet\");\n    }\n}\n\nvar a = new A6();\nvar a = new A6After() with Print;\n</code></pre>"},{"location":"tools/lang/shell/","title":"Shell","text":"<p>Is the basic programming language that use for communicate with OS of any machine, like Linux, Windows, or Mac.</p>"},{"location":"tools/lang/shell/ubt-sh-common-file/","title":"App","text":"<ul> <li>https://stackoverflow.com/questions/3534280/how-can-i-pass-a-file-argument-to-my-bash-script-using-a-terminal-command-in-lin</li> </ul>"},{"location":"tools/lang/sql/","title":"SQL","text":"<p>Mastering SQL for Data Engineers: A Comprehensive Learning Roadmap</p>"},{"location":"tools/lang/sql/sql-optimizing-sql-queries/","title":"SQL: Optimizing SQL Queries","text":"<p>https://medium.com/@vishalbarvaliya/optimizing-sql-queries-25-techniques-for-efficient-database-queries-b31ce7559079</p>"},{"location":"tools/open_table/","title":"File Formats","text":"<p>In the realm of big data, choosing the right file format is crucial for efficient data storage, retrieval, and processing. Four prominent file formats that have gained traction in recent years are Apache Iceberg, Apache Hudi, Parquet, and Delta Lake. Each format offers unique features and benefits tailored to different use cases. In this article, we\u2019ll delve into these file formats, exploring their characteristics, advantages, and code examples to illustrate their usage.</p>"},{"location":"tools/open_table/#apache-iceberg","title":"Apache Iceberg","text":"<p>Apache Iceberg is a table format designed for massive-scale data platforms. It offers features like schema evolution, data versioning, and time travel capabilities. Iceberg organizes data into tables and provides ACID transactions for data modifications.</p> <p>Features and Benefits:</p> Schema Evolution <p>Iceberg allows schema changes without interrupting concurrent reads and writes.</p> Data Versioning <p>It maintains a history of data changes, facilitating easy rollback or time-travel queries.</p> ACID Transactions <p>Iceberg ensures data consistency with atomicity, consistency, isolation, and durability.</p> <pre><code>from pyiceberg import HadoopTables\n\n# Assuming `hadoopConf`, `schema`, and `tableIdentifier` are defined elsewhere\niceberg_table = HadoopTables(hadoopConf).create(schema, tableIdentifier)\n</code></pre>"},{"location":"tools/open_table/#apache-hudi","title":"Apache Hudi","text":"<p>Apache Hudi (Hadoop Upserts Deletes and Incrementals) is a data lake storage format that supports record-level insert, update, and delete operations. It provides efficient ingestion and query performance with features like columnar storage and indexing.</p> <p>Features and Benefits:</p> Upserts and Deletes <p>Hudi enables efficient upserts and deletes, making it suitable for use cases requiring real-time data updates.</p> Incremental Processing <p>It supports incremental data processing, reducing the computational overhead for large datasets.</p> Query Performance <p>Hudi optimizes query performance through columnar storage and indexing mechanisms.</p> <pre><code># Python example to write data using Hudi\n(\n    hudi_df\n        .write\n        .format(\"org.apache.hudi\")\n        .option(\"hoodie.datasource.write.operation\", \"upsert\")\n        .save(\"/path/to/hudi_table\")\n)\n</code></pre>"},{"location":"tools/open_table/#parquet","title":"Parquet","text":"<p>Apache Parquet is a columnar storage format optimized for big data processing frameworks like Apache Hadoop and Apache Spark. It offers efficient compression and encoding techniques, making it suitable for analytical workloads.</p> <p>Features and Benefits:</p> Columnar Storage <p>Parquet organizes data by column, enabling efficient query execution by reading only the necessary columns.</p> Compression <p>It provides built-in compression algorithms like Snappy and Gzip, reducing storage costs and improving query performance.</p> Predicate Pushdown <p>Parquet supports predicate pushdown, filtering data at the storage level before retrieval, further enhancing query performance.</p> <pre><code>// Scala example to read Parquet file using Apache Spark\nval parquetDF = spark.read.parquet(\"/path/to/parquet_file\")\n</code></pre>"},{"location":"tools/open_table/#delta-lake","title":"Delta Lake","text":"<p>Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data lakes. It provides features like data versioning, schema enforcement, and time travel capabilities.</p> <p>Features and Benefits:</p> ACID Transactions <p>Delta Lake ensures atomicity, consistency, isolation, and durability for data modifications, enhancing data integrity.</p> Schema Enforcement <p>It enforces schema evolution and validation, preventing data inconsistencies.</p> Time Travel <p>Delta Lake enables querying data at different versions or timestamps, facilitating historical analysis and debugging.</p> <pre><code>query = (\n    myDF.writeStream\n        .format(\"delta\")\n        .partitionBy(\"dt\")\n        .outputMode(\"append\")\n        .trigger(processingTime='60 seconds')\n        .option(\"checkpointLocation\", checkpointLocation)\n        .option(\"path\",refine_loc)\n        .option(\"overwriteSchema\", \"true\")\n        .option(\"mergeSchema\", \"true\")\n        .start()\n)\n</code></pre>"},{"location":"tools/open_table/#references","title":"References","text":"<ul> <li>A Comprehensive Guide to Modern File Formats in Data Engineering 2024!</li> </ul>"},{"location":"tools/open_table/deltalake/","title":"Delta","text":"<p>Quote</p> <p>Delta Lake is open source software that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling. A Delta Lake table is a table created and managed using Delta Lake technology.</p> <p>Delta Lake is the optimized storage layer that provides the foundation for storing data and tables in the Databricks Lakehouse Platform. Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale.</p>"},{"location":"tools/open_table/deltalake/#compare-with-parquet","title":"Compare with Parquet","text":"<p>Read More</p>"},{"location":"tools/open_table/deltalake/#references","title":"References","text":"<ul> <li>Microsoft - Azure Databricks: Delta</li> </ul>"},{"location":"tools/open_table/deltalake/delta-lake-deletion-vector/","title":"DeltaLake: Deletion Vector","text":""},{"location":"tools/open_table/deltalake/delta-lake-deletion-vector/#references","title":"References","text":"<ul> <li>https://medium.com/@rahulsoni4/delta-lake-deletion-vectors-solution-to-the-immutability-challenges-5d1658992761</li> </ul>"},{"location":"tools/open_table/deltalake/delta-lake-liquid-clustering/","title":"DataLake: Liquid Clustering","text":"<p>https://pub.towardsai.net/start-using-liquid-clustering-instead-of-partitioning-for-delta-tables-in-databricks-973cd196f4dd https://levelup.gitconnected.com/delta-lake-liquid-clustering-a-visual-explanation-b9d8782a9f33</p>"},{"location":"tools/open_table/deltalake/delta-lake-merge/","title":"Merge","text":""},{"location":"tools/open_table/deltalake/delta-lake-merge/#references","title":"References","text":"<ul> <li>Merge/Upsert In Delta Lake</li> </ul>"},{"location":"tools/open_table/deltalake/delta-lake-partition-zorder-cluster/","title":"DeltaLake: Partition, Z-order, and Cluster","text":"<p>https://towardsdatascience.com/delta-lake-partitioning-z-order-and-liquid-clustering-944030ff1828</p>"},{"location":"tools/open_table/deltalake/delta-lake-universal-format/","title":"DeltaLake: Universal Format","text":"<p>https://levelup.gitconnected.com/delta-lake-universal-format-a-first-look-9dfa28b68b72</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-auto-schema-evolution/","title":"Pyspark Delta lake Automatic Schema Evolution","text":"<p>Update: <code>2023-04-27</code> | Tag: <code>Python</code> <code>Spark</code> <code>Delta Lake</code> <code>Schema</code></p> <p>Schema evolution is a critical aspect of managing data over time. It is very common for data sources to evolve and adapt to new business requirements, which might mean adding or removing fields from an existing data schema. As a data consumer, it is imperative a quick and agile adaption to the new characteristics of the data sources and automatic schema evolution allows us to seamlessly adapt to these changes.</p> <p>In this post, we will cover automatic schema evolution in Delta while using the people10m public dataset that is available on Databricks Community Edition. We\u2019ll test adding and removing fields in several scenarios.</p> <p>Table of Contents:</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-auto-schema-evolution/#setup","title":"Setup","text":"<p>Automatic schema evolution can be enabled in two ways, depending on our workload. If we are doing blind appends, all we need to do is to enable <code>mergeSchema</code> option:</p> <pre><code>(\n    df.write // writeStream\n        .mode(\"append\")\n        .option(\"mergeSchema\", \"true\")\n        .save(\"delta_table\")\n)\n</code></pre> <p>If we use a merge strategy for inserting data we need to enable <code>spark.databricks.delta.schema.autoMerge.enabled</code> by setting it to true.</p> <pre><code>spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")\n</code></pre>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-auto-schema-evolution/#evolving-schemas","title":"Evolving Schemas","text":"<p>Disclaimer: All the updates that we will be doing to the schema are just examples and are not meant to make much sense.</p> <pre><code>import org.apache.spark.sql.types._\n\nval schema = StructType(\n  StructField(\"id\", IntegerType) ::\n    StructField(\"firstName\", StringType) ::\n    StructField(\"middleName\", StringType) ::\n    StructField(\"lastName\", StringType) ::\n    StructField(\"gender\", StringType) ::\n    StructField(\"birthDate\", TimestampType) ::\n    StructField(\"ssn\", StringType) ::\n    StructField(\"salary\", IntegerType) ::\n    Nil\n)\n</code></pre> <pre><code>import io.delta.tables._\nimport scala.collection.JavaConverters._\n\nval sourceDataFrame = spark.createDataFrame(\n  Seq(\n    Row.fromSeq(\n      // records\n    )).asJava, schema)\n\nval targetDeltaTable = DeltaTable.forPath(spark, \"/delta/people-10m.delta\")\n\ntargetDeltaTable.alias(\"t\").merge(\n    sourceDataFrame.alias(\"s\"),\n    \"t.id = s.id\")\n  .whenMatched().updateAll()\n  .whenNotMatched().insertAll()\n  .execute()\n</code></pre> <ul> <li>Adding a field:</li> </ul> <p>We\u2019ll add a new field to our current schema called <code>nickName</code> and update   Pennie\u2019s nickName (id number 1).</p> <pre><code>...\nStructField(\"nickName\", StringType) ::\n...\n</code></pre> <p></p> <p>As we can see a new field as added and Pennie can now be called by her new   favorite nickname! Notice how all the other records' value was automatically   filled with <code>null</code>.</p> <ul> <li>Removing a field:</li> </ul> <p>With the addition of nicknames, everyone started thinking about how no one uses   their middle name, so they decided to remove it.</p> <p>We\u2019re going to update Quyen\u2019s nickname as well but as a result of the source   deleting the field, her middle name won\u2019t be present. What should happen to the   table?</p> <p></p> <p>If you guessed nothing, you were right. Every current target table record remains   the same, only new records will have <code>middleName</code> as <code>null</code>.</p> <p>To showcase this we\u2019re going to insert a new id (0).</p> <p></p> <ul> <li>Renaming a column:</li> </ul> <p>Renaming a column is the same as removing a column and adding another with a   new name. If you wish to rename a column in place please refer to   Delta Column Name Mapping.</p> <p>I won\u2019t dig further into this topic as even though it is a schema evolution,   it is not automatic. Have in mind that this feature is irreversible, once you   turn it on you aren\u2019t able to turn it off.</p> <ul> <li>Changing a column type/order:</li> </ul> <p>Changing a column type or column order is also not part of automatic schema evolution.</p> <ul> <li>Adding/Removing a field in a struct:</li> </ul> <p>Let\u2019s imagine that we have added an employee history struct that includes the   <code>startDate</code> and <code>endDate</code> to track when the employee started and left the job.</p> <p></p> <p>For a more complete history, we now wish to include the <code>title</code> in order to track   the employee\u2019s career in the company.</p> <pre><code>val schema = StructType(\n  StructField(\"id\", IntegerType) ::\n  StructField(\"firstName\", StringType) ::\n  StructField(\"lastName\", StringType) ::\n  StructField(\"gender\", StringType) ::\n  StructField(\"birthDate\", TimestampType) ::\n  StructField(\"ssn\", StringType) ::\n  StructField(\"salary\", IntegerType) ::\n  StructField(\"nickName\", StringType) ::\n  StructField(\n    \"employmentHistory\",\n    StructType(\n      StructField(\"title\", StringType) :: //newly added\n        StructField(\"startDate\", TimestampType) ::\n        StructField(\"endDate\", TimestampType) ::\n        Nil\n    )) ::\n  Nil\n)\n</code></pre> <p></p> <p>As we can see, adding a field to a struct is also not an issue. If we try to   remove the newly added field it will also work. Adding and removing fields   inside a struct works the same way as if it is performed on the root.</p> <ul> <li>Adding/Removing a field in an array of structs:</li> </ul> <p>Now we are getting more complex. In this case, we\u2019ll be adding a new field to   a struct that is inside an array. Imagine we now have an array of equipment that   currently belongs to an employee:</p> <p></p> <pre><code>val schema = StructType(\n  StructField(\"id\", IntegerType) ::\n    StructField(\"firstName\", StringType) ::\n    StructField(\"lastName\", StringType) ::\n    StructField(\"gender\", StringType) ::\n    StructField(\"birthDate\", TimestampType) ::\n    StructField(\"ssn\", StringType) ::\n    StructField(\"salary\", IntegerType) ::\n    StructField(\"nickName\", StringType) ::\n    StructField(\n      \"employmentDetails\",\n      StructType(\n        StructField(\"title\", StringType) ::\n          StructField(\"startDate\", DateType) ::\n          StructField(\"endDate\", DateType) ::\n          Nil\n      )) ::\n    StructField(\n      \"equipment\", ArrayType(\n        StructType(\n          StructField(\"brand\", StringType) ::\n            StructField(\"model\", StringType) ::\n            StructField(\"serial_num\", StringType) :: //newly added\n            Nil\n        )\n      )) ::\n    Nil\n)\n</code></pre> <p></p> <p>As we can see, this also works as expected. The table schema is updated, new   records have the respective <code>serial_num</code> and older records <code>serial_num</code> are filled   with null values.</p> <p>If we remove the newly added field again it works as expected.</p> <ul> <li>Adding/Removing a field in a map of structs:</li> </ul> <p>Now it's time to test the same but inside a map. We have added a new column   called <code>connections</code> that will be responsible for holding the hierarchy for each   employee.</p> <p></p> <p>To simulate an update we\u2019ll be adding a new column called <code>title</code> to the struct   inside the <code>connections</code> column.</p> <pre><code>val schema = StructType(\n  StructField(\"id\", IntegerType) ::\n    StructField(\"firstName\", StringType) ::\n    StructField(\"lastName\", StringType) ::\n    StructField(\"gender\", StringType) ::\n    StructField(\"birthDate\", TimestampType) ::\n    StructField(\"ssn\", StringType) ::\n    StructField(\"salary\", IntegerType) ::\n    StructField(\"nickName\", StringType) ::\n    StructField(\n      \"employmentDetails\",\n      StructType(\n        StructField(\"title\", StringType) ::\n          StructField(\"startDate\", DateType) ::\n          StructField(\"endDate\", DateType) ::\n          Nil\n      )) ::\n    StructField(\n      \"equipment\", ArrayType(\n        StructType(\n          StructField(\"brand\", StringType) ::\n            StructField(\"model\", StringType) ::\n            StructField(\"serial_num\", StringType) ::\n            Nil\n        )\n      )) ::\n    StructField(\n      \"connections\", MapType(\n        StringType, StructType(\n          StructField(\"id\", IntegerType) ::\n            StructField(\"name\", StringType) ::\n            StructField(\"title\", StringType) :: //newly added\n            Nil\n        )\n      )) ::\n    Nil\n)\n</code></pre> <p></p> <p>This time, removing the field that returns an <code>AnalysisException</code> which means   that MapType conversions are not well-supported.</p> <pre><code>AnalysisException: cannot resolve 's.connections' due to data type mismatch:\ncannot cast map&lt;string,struct&lt;id:int,title:string&gt;&gt; to map&lt;string,struct&lt;id:int,name:string,title:string&gt;&gt;;\n</code></pre> <p>After a brief investigation, I found that it is due to castIfNeeded function not   supporting MapTypes yet. I have opened a bug and will try to work on a fix for   this issue.</p> <p>Edit: https://github.com/delta-io/delta/pull/1645</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-auto-schema-evolution/#conclusion","title":"Conclusion","text":"<p>In this article, we went through the addition and removal of fields in several scenarios. We concluded that automatic schema evolution in Delta is very complete and supports most of the complex scenarios. By allowing these scenarios we can avoid having to manually intervene to update our schemas when data sources evolve. This is especially useful when consuming hundreds of data sources.</p> <p>As a bonus, we also found a missing case that is not supported in MapTypes which is a great opportunity to give back to the community for such an awesome open-source project.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-auto-schema-evolution/#references","title":"References","text":"<ul> <li>https://towardsdatascience.com/delta-lake-automatic-schema-evolution-11d32bd1aa99</li> </ul>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-best-practice/","title":"Pyspark Delta the Best Practice","text":""},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-best-practice/#references","title":"References","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/databricks/delta/best-practices</li> </ul>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-handling-concurrent-write/","title":"Pyspark DetaLake: Handling Concurrent Writes","text":"<p>https://itnext.io/handling-concurrent-writes-in-databricks-01a81ed33aea</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-image-files/","title":"Pyspark Delta lake Image Files","text":"<p>Update: <code>2023-05-16</code> | Tag: <code>Python</code> <code>Spark</code> <code>Delta Lake</code> <code>Image Files</code></p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-image-files/#limitations","title":"Limitations","text":"<p>If you have very large images, use delta table containing path to the file rather than ingesting the image in binary file format.</p> <p>Warning: \\ Binary file format can support up to 2 GB per image. In general, if your image files are larger than 512 MB, we recommend storing the path to the file in the table rather than storing the content/data of image file itself.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-keeping-it-fast-and-clean/","title":"Pyspark Delta lake keeping it Fast and Clean","text":"<p>Update: <code>2023-04-27</code> | Tag: <code>Python</code> <code>Spark</code> <code>Delta Lake</code> <code>Optimization</code></p> <p>Keeping Delta tables fast and clean is important for maintaining the efficiency of data pipelines. Delta tables can grow very large over time, leading to slow query performance and increased storage costs. However, there are several operations and trade-offs that can positively influence the speed of the tables.</p> <p>In this content, we\u2019ll use the people10m public dataset that is available on Databricks Community Edition, to showcase the best practices on how to keep the tables fast and clean using Delta operations while explaining what is happening behind the scenes.</p> <p></p> <p>Table of Contents:</p> <ul> <li>Analyzing the delta log</li> </ul>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-keeping-it-fast-and-clean/#analyzing-the-delta-log","title":"Analyzing the delta log","text":"<p>If we check the contents of the log we can see a JSON file that describes the first transaction that was written when Databricks created this Delta table.</p> <pre><code>%fs\nhead dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta/_delta_log/00000000000000000000.json\n</code></pre> <p>From analysis, we can see that this transaction includes several actions:</p> <ul> <li>Commit info:</li> </ul> <pre><code>{\n  \"commitInfo\": {\n    \"timestamp\": 1602173340340,\n    \"userId\": \"360903564160648\",\n    \"userName\": \"stephanie.bodoff@databricks.com\",\n    \"operation\": \"WRITE\",\n    \"operationParameters\": {\n      \"mode\": \"ErrorIfExists\",\n      \"partitionBy\": \"[]\"\n    },\n    \"notebook\": {\n      \"notebookId\": \"1607762315395537\"\n    },\n    \"clusterId\": \"1008-160338-oil232\",\n    \"isolationLevel\": \"WriteSerializable\",\n    \"isBlindAppend\": true,\n    \"operationMetrics\": {\n      \"numFiles\": \"8\",\n      \"numOutputBytes\": \"221245652\",\n      \"numOutputRows\": \"10000000\"\n    }\n  }\n}\n</code></pre> <p><code>commitInfo</code> contains all the information regarding the commit: which operation   was made, by whom, where, and at what time. The <code>perationMetrics</code> field shows   that 8 files were written with a total of 1000000 records.</p> <ul> <li>Protocol:</li> </ul> <pre><code>{\n  \"protocol\": {\n    \"minReaderVersion\": 1,\n    \"minWriterVersion\": 2\n  }\n}\n</code></pre> <p>The <code>protocol</code> action is used to increase the version of the Delta protocol that   is required to read or write a given table. This allows excluding readers/writers   that are on an old protocol and would miss the necessary features to correctly   interpret the transaction log.</p> <ul> <li>Metadata:</li> </ul> <pre><code>{\n  \"metaData\": {\n    \"id\": \"ee2db204-0e38-4962-92b0-83e5570d7cd5\",\n    \"format\": {\n      \"provider\": \"parquet\",\n      \"options\": {}\n    },\n    \"schemaString\": \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"firstName\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"middleName\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"lastName\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"birthDate\\\",\\\"type\\\":\\\"timestamp\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"ssn\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"salary\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n    \"partitionColumns\": [],\n    \"configuration\": {},\n    \"createdTime\": 1602173313568\n  }\n}\n</code></pre> <p>The <code>metadata</code> action contains all the table metadata. It is required on the first   action of a table as it contains its own definition. Subsequent modifications   to the table metadata will originate a new action.</p> <ul> <li>Add:</li> </ul> <pre><code>{\n    \"add\": {\n        \"path\": \"part-00000-373539c8-e620-43e4-82e0-5eba22bb3b77-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 27825521,\n        \"modificationTime\": 1602173334000,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1249744,\\\"minValues\\\":{\\\"id\\\":3766824,\\\"firstName\\\":\\\"Aaron\\\",\\\"middleName\\\":\\\"Aaron\\\",\\\"lastName\\\":\\\"A'Barrow\\\",\\\"gender\\\":\\\"F\\\",\\\"birthDate\\\":\\\"1951-12-31T05:00:00.000Z\\\",\\\"ssn\\\":\\\"666-10-1008\\\",\\\"salary\\\":-20858},\\\"maxValues\\\":{\\\"id\\\":5016567,\\\"firstName\\\":\\\"Zulma\\\",\\\"middleName\\\":\\\"Zulma\\\",\\\"lastName\\\":\\\"Zywicki\\\",\\\"gender\\\":\\\"M\\\",\\\"birthDate\\\":\\\"2000-01-30T05:00:00.000Z\\\",\\\"ssn\\\":\\\"999-98-9985\\\",\\\"salary\\\":180841},\\\"nullCount\\\":{\\\"id\\\":0,\\\"firstName\\\":0,\\\"middleName\\\":0,\\\"lastName\\\":0,\\\"gender\\\":0,\\\"birthDate\\\":0,\\\"ssn\\\":0,\\\"salary\\\":0}}\"\n    }\n}\n{\n    \"add\": {\n        \"path\": \"part-00001-943ebb93-8446-4a6c-99f7-1ca12ec2511b-c000.snappy.parquet\",\n        \"partitionValues\": {},\n        \"size\": 27781558,\n        \"modificationTime\": 1602173334000,\n        \"dataChange\": true,\n        \"stats\": \"{\\\"numRecords\\\":1249537,\\\"minValues\\\":{\\\"id\\\":1267751,\\\"firstName\\\":\\\"Abbey\\\",\\\"middleName\\\":\\\"Abbey\\\",\\\"lastName\\\":\\\"A'Barrow\\\",\\\"gender\\\":\\\"F\\\",\\\"birthDate\\\":\\\"1951-12-31T05:00:00.000Z\\\",\\\"ssn\\\":\\\"666-10-1005\\\",\\\"salary\\\":-20925},\\\"maxValues\\\":{\\\"id\\\":2517287,\\\"firstName\\\":\\\"Zulma\\\",\\\"middleName\\\":\\\"Zulma\\\",\\\"lastName\\\":\\\"Zywicki\\\",\\\"gender\\\":\\\"F\\\",\\\"birthDate\\\":\\\"2000-01-30T05:00:00.000Z\\\",\\\"ssn\\\":\\\"999-98-9981\\\",\\\"salary\\\":165757},\\\"nullCount\\\":{\\\"id\\\":0,\\\"firstName\\\":0,\\\"middleName\\\":0,\\\"lastName\\\":0,\\\"gender\\\":0,\\\"birthDate\\\":0,\\\"ssn\\\":0,\\\"salary\\\":0}}\"\n    }\n}\n...\n</code></pre> <p>The <code>add</code> action, as the name suggests, is used to modify the data in a table by   adding individual logical files. It contains the metadata of the respective file   as well as some data statistics that can be used for optimizations that we\u2019ll   talk about further down the article.</p> <p>The log contains 8 <code>add</code> entries, from <code>part-00000</code> to <code>part-00007</code>, that were   truncated for simplicity.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-keeping-it-fast-and-clean/#setup","title":"Setup","text":"<pre><code>%fs\ncp -r dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta dbfs:/delta/people-10m.delta\n</code></pre>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-keeping-it-fast-and-clean/#keeping-it-clean","title":"Keeping it clean","text":"<ul> <li>Vacuum:</li> </ul> <p>The first obvious answer is the <code>VACUUM</code> command. What this does are delete the   files that no longer affect our Delta table, given a configurable   <code>delta.deletedFileRetentionDuration</code> that defaults on 7 days.</p> <p>After analyzing the dataset and the delta log, we found that we had 16 files,   all of them older than the default retention interval, but only 8 of them were   referenced in the log. This means that in theory if we run the command the other   8 files would be cleaned.</p> <pre><code>VACUUM delta.`/delta/people-10m.delta`\n</code></pre> <p>Let\u2019s check the result in the underlying filesystem.</p> <p></p> <p>Surprisingly, the files were not cleaned up. What happened?</p> <p>Something that I found surprising is that the timestamps that are internally   used by <code>VACUUM</code> are not the ones referenced in the transaction log files in the   <code>add</code> action but rather the <code>modificationTime</code> of the files. The reason for that is   to avoid reading a huge number of JSON files to find what files should be selected   for deletion. With that said, make sure to keep <code>modificationTime</code> intact when   copying/migrating Delta tables.</p> <p>Given that we just copied the entire dataset, the <code>modificationTime</code> is as of now,   and it won\u2019t be selected for removal, at least until 7 days have passed. If we   try to do so we\u2019ll get the following warning:</p> <p></p> <p>For testing purposes, we are setting <code>delta.retentionDurationCheck.enable=false</code>   so that we can demonstrate the command in action, but it\u2019s something that should   be used with caution as it risks corrupting the table if any other active reader   or writer depends on the data that is being deleted.</p> <pre><code>set spark.databricks.delta.retentionDurationCheck.enable=false;\nVACUUM delta.`/delta/people-10m.delta` RETAIN 0 HOURS\n</code></pre> <p>Everything looks cleaner now. What about the transaction log? There are 4 new   JSON files, each representing a new transaction.</p> <p>Every time a <code>VACUUM</code> is requested, two new commits are generated in the transaction   log, containing <code>VACUUM START</code> and <code>VACUUM END</code> operations, respectively.</p> <pre><code>{\n  \"commitInfo\": {\n    \"timestamp\": 1676202617353,\n    \"userId\": \"8019820830300763\",\n    \"userName\": \"vitor\",\n    \"operation\": \"VACUUM START\",\n    \"operationParameters\": {\n      \"retentionCheckEnabled\": true,\n      \"defaultRetentionMillis\": 604800000\n    },\n    \"notebook\": {\n      \"notebookId\": \"1087108890280137\"\n    },\n    \"clusterId\": \"0102-173902-b3a5lq4t\",\n    \"readVersion\": 0,\n    \"isolationLevel\": \"SnapshotIsolation\",\n    \"isBlindAppend\": true,\n    \"operationMetrics\": {\n      \"numFilesToDelete\": \"0\"\n    },\n    \"engineInfo\": \"Databricks-Runtime/11.3.x-scala2.12\",\n    \"txnId\": \"6b875d5e-4c0e-4724-a87b-a0a6bbfd8419\"\n  }\n}\n</code></pre> <p>The first one did not affect any files, hence the <code>numFilesToDelete</code> is 0.</p> <pre><code>{\n  \"commitInfo\": {\n    \"timestamp\": 1676206833338,\n    \"userId\": \"8019820830300763\",\n    \"userName\": \"vitor\",\n    \"operation\": \"VACUUM START\",\n    \"operationParameters\": {\n      \"retentionCheckEnabled\": false,\n      \"specifiedRetentionMillis\": 0,\n      \"defaultRetentionMillis\": 604800000\n    },\n    \"notebook\": {\n      \"notebookId\": \"1087108890280137\"\n    },\n    \"clusterId\": \"0102-173902-b3a5lq4t\",\n    \"readVersion\": 2,\n    \"isolationLevel\": \"SnapshotIsolation\",\n    \"isBlindAppend\": true,\n    \"operationMetrics\": {\n      \"numFilesToDelete\": \"8\"\n    },\n    \"engineInfo\": \"Databricks-Runtime/11.3.x-scala2.12\",\n    \"txnId\": \"42f93d56-8739-46d5-a8f9-c2c1daffe0ec\"\n  }\n}\n</code></pre> <p>The second one marked 8 files for deletion hence <code>numFilesToDelete</code> is 8.</p> <p>In sum, <code>VACUUM</code> jobs are a must for storage cost reduction. However, we need   to make sure to schedule them regularly (they don\u2019t affect any running jobs),   as they are not scheduled by default. In addition to this, we need to make sure   to tweak the retention value for as long as we\u2019d want to time travel and have   the <code>modificationTime</code> in mind when migrating Delta tables.</p> <ul> <li>Optimize:</li> </ul> <p>The next command we need to be aware of is <code>OPTIMIZE</code>. What this does are compact   small files into larger ones, while keeping all the data intact and delta statistics   re-calculated. It can greatly improve query performance, especially if the data   is written using a Streaming job, where, depending on the trigger interval, a   lot of small files can be generated.</p> <p>The target file size can be changed by tweaking <code>delta.targetFileSize</code>. Have in   mind that setting this value does not guarantee that all the files will end up   with the specified size. The operation will make a best-effort attempt to be   true to the target size, but it heavily depends on the amount of data we\u2019re   processing as well as the parallelism.</p> <p>In this example, we\u2019ll set it to 80MB, since the dataset is much smaller than   the default size which is 1GB.</p> <pre><code>set spark.databricks.delta.targetFileSize=80m;\nOPTIMIZE delta.`/delta/people-10m.delta`\n</code></pre> <p>Let\u2019s analyze the transaction log commit after what happened after we run the   command:</p> <pre><code>{\n  \"commitInfo\": {\n    \"timestamp\": 1676215176645,\n    \"userId\": \"8019820830300763\",\n    \"userName\": \"vitor\",\n    \"operation\": \"OPTIMIZE\",\n    \"operationParameters\": {\n      \"predicate\": \"[]\",\n      \"zOrderBy\": \"[]\",\n      \"batchId\": \"0\",\n      \"auto\": false\n    },\n    \"notebook\": {\n      \"notebookId\": \"1087108890280137\"\n    },\n    \"clusterId\": \"0102-173902-b3a5lq4t\",\n    \"readVersion\": 2,\n    \"isolationLevel\": \"SnapshotIsolation\",\n    \"isBlindAppend\": false,\n    \"operationMetrics\": {\n      \"numRemovedFiles\": \"8\",\n      \"numRemovedBytes\": \"221245652\",\n      \"p25FileSize\": \"59403028\",\n      \"minFileSize\": \"59403028\",\n      \"numAddedFiles\": \"3\",\n      \"maxFileSize\": \"88873012\",\n      \"p75FileSize\": \"88873012\",\n      \"p50FileSize\": \"87441438\",\n      \"numAddedBytes\": \"235717478\"\n    },\n    \"engineInfo\": \"Databricks-Runtime/11.3.x-scala2.12\",\n    \"txnId\": \"55389d3e-4dd5-43a9-b5e1-de67cde8bb72\"\n  }\n}\n</code></pre> <p>A total of 8 files were removed, and 3 were added. Our new target file size is   80MB so all of our files were compacted into three new ones. As the commit info   shows, the log also contains 8 <code>remove</code> actions and three <code>add</code> actions that were   omitted for simplicity.</p> <p></p> <p>You might be wondering if the <code>OPTIMIZE</code> command really did something useful in   this specific dataset so let\u2019s try and run a simple query.</p> <pre><code>SELECT * FROM delta.`/delta/people-10m.delta` WHERE salary &gt; 80000\n</code></pre> <p></p> <p>With <code>OPTIMIZE</code> we have improved the scan time since we read fewer files. Nevertheless,   we are still reading the whole dataset while trying to find salaries that are   greater than 80000. We will tackle this issue in the next section of the article.</p> <p>In sum, one should schedule <code>OPTIMIZE</code> jobs regularly since query reads can heavily   benefit from having fewer files to read. Databricks recommends running it daily,   but it really depends on the frequency of the updates. Have in mind that OPTIMIZE   can take some time and will increase processing costs.</p> <ul> <li>Z-Order Optimize:</li> </ul> <p>Z-Ordering is a technique that is used to collocate related information in the   same set of files.</p> <p>When files are written to a Delta table, min, max, and count statistics are   automatically added in a <code>stats</code> field on the add action as we\u2019ve seen before.   These statistics are used for data-skipping when querying the table.   Data-skipping is an optimization that aims to optimize queries containing <code>WHERE</code>   clauses. By default, the first 32 columns of the dataset have their statistics   collected. It can be changed by tweaking <code>delta.dataSkippingNumIndexedCols</code> to   the desired number. Have in mind that this can affect write performance, especially   for long strings for which it is advised to move them to the end of the schema   and set the property to a number lower than its index.</p> <p>In the <code>OPTIMIZE</code> example, we\u2019ve seen that even though we have these statistics   collected we can\u2019t really make use of them and still end up reading all the files.   That\u2019s because we don\u2019t have any explicit ordering and the salaries are basically   randomized between all files.</p> <p>By adding a <code>ZORDER-BY</code> column with OPTIMIZE we can easily solve this issue:</p> <pre><code>set spark.databricks.delta.targetFileSize=80m;\nOPTIMIZE delta.`/delta/people-10m.delta`\nZORDER BY (salary)\n</code></pre> <p>Let\u2019s analyze the transaction log:</p> <pre><code>{\n  \"commitInfo\": {\n    \"timestamp\": 1676217320722,\n    \"userId\": \"8019820830300763\",\n    \"userName\": \"vitor\",\n    \"operation\": \"OPTIMIZE\",\n    \"operationParameters\": {\n      \"predicate\": \"[]\",\n      \"zOrderBy\": \"[\\\"salary\\\"]\",\n      \"batchId\": \"0\",\n      \"auto\": false\n    },\n    \"notebook\": {\n      \"notebookId\": \"1087108890280137\"\n    },\n    \"clusterId\": \"0102-173902-b3a5lq4t\",\n    \"readVersion\": 2,\n    \"isolationLevel\": \"SnapshotIsolation\",\n    \"isBlindAppend\": false,\n    \"operationMetrics\": {\n      \"numRemovedFiles\": \"8\",\n      \"numRemovedBytes\": \"221245652\",\n      \"p25FileSize\": \"113573613\",\n      \"minFileSize\": \"113573613\",\n      \"numAddedFiles\": \"2\",\n      \"maxFileSize\": \"123467314\",\n      \"p75FileSize\": \"123467314\",\n      \"p50FileSize\": \"123467314\",\n      \"numAddedBytes\": \"237040927\"\n    },\n    \"engineInfo\": \"Databricks-Runtime/11.3.x-scala2.12\",\n    \"txnId\": \"0e9b6467-9385-42fa-bc1a-df5486fc997f\"\n  }\n}\n</code></pre> <p>There are some differences between both <code>OPTIMIZE</code> commands. The first we can notice   is that, as expected, we now have a <code>zOrderBy</code> column in <code>operationParameters</code>.   Moreover, even though we have specified the same target file size, the <code>OPTIMIZE</code>   resulted in 2 files instead of 3, due to the statistics of our column.</p> <p>Below is the <code>add</code> action for the first file. The stats show that this file contains   all the records that have salaries between -26884 and 73676. With that said, our   query should skip this file entirely since the salary value falls out of the range   of our <code>WHERE</code> clause.</p> <pre><code>{\n  \"add\": {\n    \"path\": \"part-00000-edb01f4d-18f1-4c82-ac18-66444343df9b-c000.snappy.parquet\",\n    \"partitionValues\": {},\n    \"size\": 123467314,\n    \"modificationTime\": 1676217320000,\n    \"dataChange\": false,\n    \"stats\": \"{\\\"numRecords\\\":5206176,\\\"minValues\\\":{\\\"id\\\":1,\\\"firstName\\\":\\\"Aaron\\\",\\\"middleName\\\":\\\"Aaron\\\",\\\"lastName\\\":\\\"A'Barrow\\\",\\\"gender\\\":\\\"F\\\",\\\"birthDate\\\":\\\"1951-12-31T05:00:00.000Z\\\",\\\"ssn\\\":\\\"666-10-1010\\\",\\\"salary\\\":-26884},\\\"maxValues\\\":{\\\"id\\\":9999999,\\\"firstName\\\":\\\"Zulma\\\",\\\"middleName\\\":\\\"Zulma\\\",\\\"lastName\\\":\\\"Zywicki\\\",\\\"gender\\\":\\\"M\\\",\\\"birthDate\\\":\\\"2000-01-30T05:00:00.000Z\\\",\\\"ssn\\\":\\\"999-98-9989\\\",\\\"salary\\\":73676},\\\"nullCount\\\":{\\\"id\\\":0,\\\"firstName\\\":0,\\\"middleName\\\":0,\\\"lastName\\\":0,\\\"gender\\\":0,\\\"birthDate\\\":0,\\\"ssn\\\":0,\\\"salary\\\":0}}\",\n    \"tags\": {\n      \"INSERTION_TIME\": \"1602173334000000\",\n      \"ZCUBE_ZORDER_CURVE\": \"hilbert\",\n      \"ZCUBE_ZORDER_BY\": \"[\\\"salary\\\"]\",\n      \"ZCUBE_ID\": \"493cfedf-fdaf-4d34-a911-b4663adefec7\",\n      \"OPTIMIZE_TARGET_SIZE\": \"83886080\"\n    }\n  }\n}\n</code></pre> <p>By running the query again after Z-Ordering the files, we can see that only one   file was read and the other one was pruned.</p> <p></p> <p>Even though Z-Ordering for data-skipping looks to be a game changer, it must be   used correctly in order to be efficient. Below we\u2019ll list some key considerations   that we must have when using Z-Ordering:</p> <ol> <li> <p>Z-Ordering is only suited for columns with high cardinality, if it has a low      cardinality we cannot benefit from data-skipping.</p> </li> <li> <p>We can specify multiple columns on Z-Order but the effectiveness of its      data-skipping decreases with each extra column.</p> </li> <li> <p>Make sure to Z-Order only on columns for which statistics are available.      Have in mind the index of the columns and that only the first 32 columns are      analyzed.</p> </li> <li> <p>Partitioning:</p> </li> </ol> <p>Another technique that can be used is physical partitioning. While Z-ordering   groups data with similar values under the same file, partitioning groups data   files under the same folder.</p> <p>Contrary to Z-Ordering, partitioning works best with low-cardinality columns.   If we choose otherwise, we may end up with possibly infinite partitions and end   up with a lot of small files which in the end results in performance issues.</p> <p>We\u2019ll be using gender as our partition column since it is the only one with low   cardinality present in this dataset.</p> <pre><code>REPLACE TABLE delta.`/delta/people-10m.delta`\n  USING DELTA\n  PARTITIONED BY (gender)\nAS\n  SELECT * FROM delta.`/delta/people-10m.delta`\n</code></pre> <p>By doing this we end up with two folders, one for each gender. This type of   segregation is rather useful for columns that have low cardinality, and are   very often used in <code>WHERE</code> clauses in large tables.</p> <p>Let\u2019s suppose we now want to be able to extract insights based on gender and   salary.</p> <pre><code>SELECT * FROM delta.`/delta/people-10m.delta`\nWHERE gender = 'F' AND salary &gt; 80000\n</code></pre> <p><code>OPTIMIZE</code> can be paired with a partition column if we only want to optimize a   subset of the data. Below we\u2019ll analyze data-skipping with and without partitioning   in Z-Ordered tables to show how can we take advantage of both approaches. We\u2019ve   decreased the target file size to showcase the differences now that our data is   split by gender under different files.</p> <p></p> <p>As shown above, without partitioning we have to read two files to get our results.   We were able to skip 3 files by having it Z-Ordered by salary but had to fully   read them to extract the request gender. With partitioning, we were able to skip   a full partition, which filtered the gender basically for free, and 3 files due   to the Z-Ordering.</p> <p>As we can see, there are benefits to using both approaches simultaneously, but   it needs to be thoroughly thought through as it might only make a significant   difference for very large tables.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-keeping-it-fast-and-clean/#conclusion","title":"Conclusion","text":"<p>In conclusion, keeping Delta tables clean is crucial for maintaining the performance and efficiency of data pipelines. Vacuuming and optimizing Delta tables can help reclaim storage space and improve query execution times. Understanding the small details of each operation is very important to proper fine-tuning which could otherwise lead to unnecessary storage and processing costs.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-keeping-it-fast-and-clean/#references","title":"References","text":"<ul> <li>https://towardsdatascience.com/delta-lake-keeping-it-fast-and-clean-3c9d4f9e2f5e</li> </ul>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-scd2/","title":"Pyspark Delta lake: SCD2","text":"<p>Update: <code>2023-04-25</code> | Tag: <code>Python</code> <code>Spark</code> <code>Delta Lake</code> <code>SCD</code> <code>DWH</code></p> <p>Slowly Changing Dimensions (SCDs) are a concept used in data warehousing to maintain historical data over time, while tracking changes in data. SCDs are important for analysis and reporting purposes, and there are different types of SCDs to choose from depending on the specific requirements of the data warehouse.</p> <p>Table of Contents:</p> <ul> <li>Advantages of Using Delta Lake for SCD2 Implementation</li> <li>Setup Data table for SCD2 Implementation</li> <li>Method 01: Insert, Update, and Overwrite</li> <li>Method 02: Union after Merge</li> <li>Method 03: Merge and Append</li> </ul>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-scd2/#advantages-of-using-delta-lake-for-scd2-implementation","title":"Advantages of Using Delta Lake for SCD2 Implementation","text":"<ul> <li> <p>ACID Transactions: \\   Delta Lake on Databricks provides full support for ACID transactions,   which means that data changes are atomic, consistent, isolated, and durable.   This guarantees that SCD2 updates are executed in a way that preserves the integrity   and consistency of the data.</p> </li> <li> <p>Optimized Performance: \\   Delta Lake on Databricks is optimized for performance, allowing for fast read   and write operations. This means that even as the size of your data grows over   time, your SCD2 updates can be executed quickly and efficiently.</p> </li> <li> <p>Schema Enforcement: \\   Delta Lake on Databricks provides built-in schema enforcement,   which ensures that SCD2 updates adhere to the schema defined for the data. This   helps to prevent errors and inconsistencies that can arise from data type mismatches   or missing fields.</p> </li> <li> <p>Time Travel: \\   Delta Lake on Databricks provides time travel capabilities, allowing   you to access the full history of changes to your SCD2 data. This means that you   can easily view and audit changes over time, or even revert to a previous version   of the data if necessary. Time travel also enables the creation of data-driven   reports and dashboards that can provide insights into trends and patterns over   time.</p> </li> </ul>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-scd2/#setup-data-table-for-scd2-implementation","title":"Setup Data table for SCD2 Implementation","text":"<p>We define customer table that has initialized data name <code>initial_df</code> and incoming data name <code>source_df</code>.</p> <pre><code>from pyspark.sql import Row\nfrom datetime import date\n\ninitial_df = spark.createDataFrame(\n  [\n    Row(cust_id=1001, first_name=\"John\", city=\"A\", address=\"A001\", update_date=date(2023, 4, 1)),\n    Row(cust_id=1002, first_name=\"Sara\", city=\"B\", address=\"B001 IT\", update_date=date(2023, 4, 1)),\n    Row(cust_id=1003, first_name=\"Tommy\", city=\"C\", address=\"C001 BKK\", update_date=date(2023, 4, 1)),\n  ]\n)\n\nsource_df = spark.createDataFrame(\n  [\n    Row(cust_id=1001, first_name=\"John\", city=\"A\", address=\"A001 NEW\", update_date=date(2023, 4, 2)),\n    Row(cust_id=1004, first_name=\"Smile\", city=\"D\", address=\"D012\", update_date=date(2023, 4, 2)),\n  ]\n)\n</code></pre> <p>Let create delta table in the Databricks <code>hiveme_tastore</code> schema.</p> <pre><code>from delta.tables import DeltaTable\n\nspark.sql(\"\"\"\n    CREATE OR REPLACE TABLE customer_history (\n      cust_id INTEGER,\n      first_name STRING,\n      city STRING,\n      address STRING,\n      eff_start_date TIMESTAMP,\n      eff_end_date TIMESTAMP,\n      is_active STRING\n    )\n    USING DELTA\n    LOCATION '/mnt/delta_silver/customer_history'\n\"\"\")\n\n# Read the target data (Delta Lake table)\ntarget_delta_table = DeltaTable.forPath(\n  spark, \"/mnt/delta_silver/customer_history\"\n)\n</code></pre> <pre><code>high_date: str = \"9999-12-31\"\nis_active: str = \"Y\"\ncurrent_date = lambda: f\"{date.today():%Y-%m-%d}\"\n</code></pre> <p>In the given code, <code>high_date</code> and <code>is_active</code> are variables used to define the end date and active status for records in the customer table.</p> <p><code>high_date</code> is used to represent the maximum possible date in the future and is used to indicate the end date for records that are currently active. For records that are closed, the <code>eff_end_date</code> column in the Delta Lake table will be set to the date when the record was closed.</p> <p><code>is_active</code> is used to indicate whether a record is currently active or not.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-scd2/#method-01-insert-update-and-overwrite","title":"Method 01: Insert, Update, and Overwrite","text":"<pre><code>import pyspark.sql.functions as f\n\njoined_data = (\n  source_df.alias(\"src\")\n    .join(\n      other=target_delta_table.toDF().alias(\"tgt\"),\n      on=f.col(\"src.cust_id\") == f.col(\"tgt.cust_id\"),\n      how=\"full_outer\"\n    )\n    .select(\n      \"src.*\",\n      *[\n        f.col(\"tgt.\" + c).alias(\"tgt_\" + c)\n        for c in target_delta_table.toDF().columns\n      ]\n    )\n)\n</code></pre> <p>A full outer join is here to ensure that all rows from both the source and target data are included in the resulting joined data set. This is important because we need to identify rows in the target table that need to be updated or deleted, as well as rows in the source table that need to be inserted. A full outer join ensures that we do not miss any rows that may exist in one table but not the other.</p> <p>Additionally, since the SCD2 implementation requires comparing values from both the source and target data, a full outer join helps us compare all the rows and determine which ones have changed.</p> <pre><code>insert_data = (\n  joined_data.filter(f.col(\"tgt_cust_id\").isNull())\n    .withColumn(\"eff_start_date\", f.lit(current_date()))\n    .withColumn(\"eff_end_date\", f.lit(high_date))\n    .withColumn(\"is_active\", f.lit(\"Y\"))\n)\nif insert_data.count() &gt; 0:\n    (\n      target_delta_table.alias(\"tgt\")\n        .merge(\n          source=insert_data.alias(\"src\"),\n          condition=\"tgt.cust_id = src.cust_id\"\n        )\n        .whenNotMatchedInsertAll()\n        .execute()\n    )\n</code></pre> <p><code>insert_data</code> is a DataFrame variable created by filtering the <code>joined_data</code> DataFrame to select only the rows that have a <code>null</code> value for the <code>tgt_cust_id</code> column. These are the rows that are in the source data but not in the target data. The <code>insert_data</code> DataFrame is then used to insert new records into the target Delta Lake table.</p> <p>Additionally, <code>eff_start_date</code>, <code>eff_end_date</code>, and <code>is_active</code> columns are added to <code>insert_data</code> using the withColumn method before merging it with the target Delta Lake table.</p> <pre><code>update_data = (\n  joined_data.filter(\n    f.col(\"src.cust_id\").isNotNull() &amp;\n    f.col(\"tgt_cust_id\").isNotNull() &amp;\n    (\n        (f.col(\"src.first_name\") != f.col(\"tgt_first_name\")) |\n        (f.col(\"src.city\") != f.col(\"tgt_city\")) |\n        (f.col(\"src.address\") != f.col(\"tgt_address\"))\n    )\n  )\n)\n\nif update_data.count() &gt; 0:\n    (\n      target_delta_table.alias(\"tgt\")\n        .merge(\n          update_data.alias(\"src\"),\n          \"tgt.cust_id = src.cust_id\"\n        )\n        .whenMatchedUpdate(\n            condition=\"tgt.is_active = 'Y'\",\n            set={\n                \"eff_end_date\": f.lit(current_date()),\n                \"tgt.is_active\": f.lit(\"N\")\n            }\n        )\n        .execute()\n    )\n</code></pre> <p>The <code>update_data</code> variable in the given code represents the records from the joined data where the <code>cust_id</code> is present in both the source and target tables, and where there is at least one attribute that has changed. This is determined by checking if any of the following attributes in the source data are different from their corresponding attributes in the target data: <code>first_name</code>, <code>city</code>, and <code>address</code>.</p> <p>The purpose of this variable is to identify the records that need to be updated in the Delta Lake table. The subsequent operations performed on this data, such as closing old records and inserting new records with updated values, ensure that the table is updated with the latest information.</p> <pre><code>new_records = update_data.select(\n    f.col(\"src.cust_id\"),\n    f.col(\"src.first_name\"),\n    f.col(\"src.city\"),\n    f.col(\"src.address\"),\n    f.lit(current_date()).alias(\"eff_start_date\"),\n    f.lit(high_date).alias(\"eff_end_date\"),\n    f.lit(is_active).alias(\"is_active\")\n)\n\n(\n  target_delta_table.toDF()\n    .union(new_records)\n    .write.format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"overwriteSchema\", \"true\")\n    .save(\"/mnt/delta_silver/customer_history\")\n)\n</code></pre> <p>The <code>new_records</code> DataFrame is union with the existing Delta Lake table data and written to the Delta Lake table location using the <code>write()</code> method of the DataFrameWriter object. The <code>mode()</code> method is set to \"overwrite\" to replace the existing data, and the <code>option()</code> method is set to \"overwriteSchema\" to overwrite the schema of the table if it already exists.</p> <p>Note: \\ This method will use for large data source because we check the record of <code>insert_data</code> and <code>update_data</code> before merge to target delta table.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-scd2/#method-02-union-after-merge","title":"Method 02: Union after Merge","text":"<pre><code>new_records = (\n  source_df.alias(\"src\")\n    .join(\n        other=target_delta_table.toDF().alias(\"tgt\"),\n        on=\"cust_id\"\n    )\n    .where(\n      (f.col(\"tgt.is_active\") == 'Y') &amp;\n      (\n        (f.col(\"src.first_name\") != f.col(\"tgt.first_name\")) |\n        (f.col(\"src.city\") != f.col(\"tgt.city\")) |\n        (f.col(\"src.address\") != f.col(\"tgt.address\"))\n      )\n    )\n    .select(\"src.*\")\n)\n</code></pre> <p>An inner join is here to ensure that new rows from the source data are included in the resulting joined data set. This is important because we need to identify rows in the source data that need to be inserted, as well as rows in the target table that need to be updated.</p> <pre><code>staged_updates = (\n  new_records\n    .selectExpr(\"NULL as mergeKey\", \"src.*\")\n    .union(source_df.selectExpr(\"cust_id as mergeKey\", \"*\"))\n)\n</code></pre> <p><code>staged_updates</code> is the update DataFrame by unioning two sets of rows; Rows that will be inserted in the <code>whenNotMatched</code> clause, and rows that will either update the current addresses of existing customers or insert the new addresses of new customers.</p> <pre><code>(\n  target_delta_table.alias(\"tgt\")\n    .merge(\n      source=staged_updates.alias(\"src\"),\n      condition=\"tgt.cust_id = src.mergeKey\"\n    )\n    .whenMatchedUpdate(\n      condition=(\n        \"(tgt.is_active = 'Y') AND (\"\n          \"src.first_name &lt;&gt; tgt.first_name OR \"\n          \"src.city &lt;&gt; tgt.city OR \"\n          \"src.address &lt;&gt; tgt.address\"\n        \")\"\n      ),\n      set = {\n        \"is_active\": f.lit(\"N\"),\n        \"eff_end_date\": f.lit(current_date())\n      }\n    )\n    .whenNotMatchedInsert(\n      values = {\n        \"cust_id\": \"src.cust_id\",\n        \"first_name\": \"src.first_name\",\n        \"city\": \"src.city\",\n        \"address\": \"src.address\",\n        \"is_active\": f.lit(\"Y\"),\n        \"eff_start_date\": f.lit(current_date()),\n        \"eff_end_date\": f.lit(high_date)\n      }\n    )\n    .execute()\n)\n</code></pre> <p>Note: \\ This method will use for small data source because we union the record of <code>new_records</code> to <code>source_df</code> before merge to target delta table.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-scd2/#method-03-merge-and-append","title":"Method 03: Merge and Append","text":"<p>Warning: \\ This method need to use <code>whenNotMatchedBySource</code> method that support for DBR 12.1 or higher. (More Detail about DBR)</p> <pre><code>new_records = (\n  source_df.alias(\"src\")\n    .join(target_delta_table.toDF().alias(\"tgt\"), \"cust_id\")\n    .where(\n      (f.col(\"tgt.is_active\") == 'Y') &amp;\n      (\n        (f.col(\"src.first_name\") != f.col(\"tgt.first_name\")) |\n        (f.col(\"src.city\") != f.col(\"tgt.city\")) |\n        (f.col(\"src.address\") != f.col(\"tgt.address\"))\n      )\n    )\n    .select(\"src.*\")\n    .withColumn(\"eff_start_date\", f.lit(current_date()).cast('timestamp'))\n    .withColumn(\"eff_end_date\", f.lit(high_date).cast('timestamp'))\n    .withColumn(\"is_active\", f.lit(\"Y\"))\n    .withColumn('cust_id', f.col('cust_id').cast('integer'))\n    .drop('update_date')\n)\n</code></pre> <pre><code>(\n  target_delta_table\n    .alias(\"tgt\")\n    .merge(\n      source=source_df.alias(\"src\"),\n      condition=\"tgt.cust_id = src.cust_id\"\n    )\n    .whenMatchedUpdate(\n      condition=\"tgt.is_active = 'Y'\",\n      set={\n        \"eff_end_date\": f.lit(current_date()),\n        \"is_active\": f.lit(\"N\")\n      }\n    )\n    .whenNotMatchedInsert(\n      values={\n        \"cust_id\": \"src.cust_id\",\n        \"first_name\": \"src.first_name\",\n        \"city\": \"src.city\",\n        \"address\": \"src.address\",\n        \"eff_start_date\": f.lit(current_date()),\n        \"eff_end_date\": f.lit(high_date),\n        \"is_active\": f.lit(\"Y\"),\n      }\n    )\n    .whenNotMatchedBySourceUpdate(\n      condition=\"tgt.is_active = 'Y'\",\n      set={\n        \"eff_end_date\": f.lit(current_date()),\n        \"is_active\": f.lit(\"D\"),\n      }\n    )\n    .execute()\n)\n</code></pre> <p>The method <code>whenNotMatchedBySource</code> help you to handle delete records when it does not match by source, but you can still insert record that not match by target data.</p> <pre><code>(\n  target_delta_table.toDF()\n    .write.format(\"delta\")\n    .mode(\"append\")\n    .option(\"overwriteSchema\", \"true\")\n    .save(\"/mnt/delta_silver/customer_history\")\n)\n</code></pre>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-scd2/#references","title":"References","text":"<ul> <li>https://medium.com/@manishshrivastava26/mastering-dimensional-data-with-delta-lake-implementing-scd2-on-databricks-4d56a6f636b5</li> <li>https://docs.delta.io/latest/delta-update.html#slowly-changing-data-scd-type-2-operation-into-delta-tables</li> </ul>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-star-schema/","title":"Pyspark Delta lake: Star Schema","text":"<p>Update: <code>2023-04-27</code> | Tag: <code>Python</code> <code>Spark</code> <code>Delta Lake</code> <code>Dimension Model</code> <code>Star Schema</code></p> <p>Most data warehouse developers are very familiar with the ever-present star schema. Introduced by Ralph Kimball in the 1990s, a star schema is used to denormalize business data into dimensions (like time and product) and facts (like transactions in amounts and quantities). A star schema efficiently stores data, maintains history and updates data by reducing the duplication of repetitive business definitions, making it fast to aggregate and filter.</p> <p>Just like in a traditional data warehouse, there are some simple rules of thumb to follow on Delta Lake that will significantly improve your Delta star schema joins.</p> <p>Table of Contents:</p> <ul> <li>Use Delta Tables to create your fact and dimension tables</li> <li>Optimize your file size for fast file pruning</li> <li></li> </ul>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-star-schema/#use-delta-tables-to-create-your-fact-and-dimension-tables","title":"Use Delta Tables to create your fact and dimension tables","text":"<p>Delta Lake is an open storage format layer that provides the ease of inserts, updates, deletes, and adds ACID transactions on your data lake tables, simplifying maintenance and revisions. Delta Lake also provides the ability to perform dynamic file pruning to optimize for faster SQL queries.</p> <p>The syntax is simple on Databricks Runtimes 8.x and newer where Delta Lake is the default table format. You can create a Delta table using SQL with the following:</p> <pre><code>CREATE TABLE MY_TABLE (COLUMN_NAME STRING)\n</code></pre> <p>Before the 8.x runtime, Databricks required creating the table with the <code>USING DELTA</code> syntax.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-star-schema/#optimize-your-file-size-for-fast-file-pruning","title":"Optimize your file size for fast file pruning","text":"<p>Two of the biggest time sinks in an Apache Spark query are the time spent reading data from cloud storage and the need to read all underlying files. With data skipping on Delta Lake, queries can selectively read only the Delta files containing relevant data, saving significant time. Data skipping can help with static file pruning, dynamic file pruning, static partition pruning and dynamic partition pruning.</p> <p>One of the first things to consider when setting up data skipping is the ideal data file size - too small, and you will have too many files (the well-known \"small-file problem\"); too large and you won\u2019t be able to skip enough data.</p> <p>A good file size range is 32-128MB (1024 _ 1024 _ 32 = 33554432 for 32MB of course). Again, the idea is that if the file size is too big, the dynamic file pruning will skip to the right file or files, but they will be so large it will still have a lot of work to do. By creating smaller files, you can benefit from file pruning and minimize the I/O retrieving the data you need to join.</p> <p>You can set the file size value for the entire notebook in Python:</p> <pre><code>spark.conf.set(\"spark.databricks.delta.targetFileSize\", 33554432)\n</code></pre> <p>Or you can set it only for a specific table using:</p> <pre><code>ALTER TABLE (database).(table)\nSET TBLPROPERTIES (delta.targetFileSize=33554432)\n</code></pre> <p>If you happen to be reading this article after you have already created tables, you can still set the table property for the file size and, when optimizing and creating the <code>ZORDER</code>, the files will be proportioned to the new file size. If you have already added a <code>ZORDER</code>, you can add and/or remove a column to force a re-write before arriving at the final <code>ZORDER</code> configuration. Read more about <code>ZORDER</code> in step 3.</p> <p>As Databricks continues to add features and capabilities, we can also Auto Tune the file size based on the table size. For smaller databases, the above setting will likely provide better performance but for larger tables and/or just to make it simpler, you can follow the guidance here and implement the <code>delta.tuneFileSizesForRewrites</code> table property.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-star-schema/#create-a-z-order-on-your-fact-tables","title":"Create a Z-Order on your fact tables","text":"<p>To improve query speed, Delta Lake supports the ability to optimize the layout of data stored in cloud storage with Z-Ordering, also known as multidimensional clustering. Z-Orders are used in similar situations as clustered indexes in the database world, though they are not actually an auxiliary structure. A Z-Order will cluster the data in the Z-Order definition, so that rows like column values from the Z-order definition are collocated in as few files as possible.</p> <p>Most database systems introduced indexing as a way to improve query performance. Indexes are files, and thus as the data grows in size, they can become another big data problem to solve. Instead, Delta Lake orders the data in the Parquet files to make range selection on object storage more efficient. Combined with the stats collection process and data skipping, Z-Order is similar to seek vs. scan operations in databases, which indexes solved, without creating another compute bottleneck to find the data a query is looking for.</p> <p>For Z-Ordering, the best practice is to limit the number of columns in the Z-Order to the best 1-4. We chose the foreign keys (foreign keys by use, not actually enforced foreign keys) of the 3 largest dimensions which were too large to broadcast to the workers.</p> <pre><code>OPTIMIZE MY_FACT_TABLE\n  ZORDER BY (LARGEST_DIM_FK, NEXT_LARGEST_DIM_FK, ...)\n</code></pre> <p>Additionally, if you have tremendous scale and 100's of billions of rows or Petabytes of data in your fact table, you should consider partitioning to further improve file skipping. Partitions are effective when you are actively filtering on a partitioned field.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-star-schema/#create-z-orders-on-your-dimension-key-fields-and-most-likely-predicates","title":"Create Z-Orders on your dimension key fields and most likely predicates","text":"<p>Although Databricks does not enforce primary keys on a Delta table, since you are reading this blog, you likely have dimensions and a surrogate key exists - one that is an integer or big integer and is validated and expected to be unique.</p> <p>One of the dimensions we were working with had over 1 billion rows and benefited from the file skipping and dynamic file pruning after adding our predicates into the Z-Order. Our smaller dimensions also had Z-Orders on the dimension key field and were broadcast in the join to the facts. Similar to the advice on fact tables, limit the number of columns in the Z-Order to the 1-4 fields in the dimension that are most likely to be included in a filter in addition to the key.</p> <pre><code>OPTIMIZE MY_BIG_DIM\n  ZORDER BY (MY_BIG_DIM_PK, LIKELY_FIELD_1, LIKELY_FIELD_2)\n</code></pre>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-star-schema/#analyze-table-to-gather-statistics-for-adaptive-query-execution-optimizer","title":"Analyze Table to gather statistics for Adaptive Query Execution Optimizer","text":"<p>One of the major advancements in Apache Spark 3.0 was the Adaptive Query Execution, or AQE for short. As of Spark 3.0, there are three major features in AQE, including coalescing post-shuffle partitions, converting sort-merge join to broadcast join, and skew join optimization. Together, these features enable the accelerated performance of dimensional models in Spark.</p> <p>In order for AQE to know which plan to choose for you, we need to collect statistics about the tables. You do this by issuing the ANALYZE TABLE command. Customers have reported that collecting table statistics has significantly reduced query execution for dimensional models, including complex joins.</p> <pre><code>ANALYZE TABLE MY_BIG_DIM COMPUTE STATISTICS FOR ALL COLUMNS\n</code></pre>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-star-schema/#conclusion","title":"Conclusion","text":"<p>By following the above guidelines, organizations can reduce query times - in our example, from 90 seconds to 10 seconds on the same cluster. The optimizations greatly reduced the I/O and ensured that we only processed the correct content. We also benefited from the flexible structure of Delta Lake in that it would both scale and handle the types of queries that will be sent ad hoc from the Business Intelligence tools.</p> <p>In addition to the file skipping optimizations mentioned in this blog, Databricks is investing heavily in improving the performance of Spark SQL queries with Databricks Photon. Learn more about <code>Photon</code> and the performance boost it will provide to all of your Spark SQL queries with Databricks.</p> <p>Customers can expect their ETL/ELT and SQL query performance to improve by enabling Photon in the Databricks Runtime. Combining the best practices outlined here, with the Photon-enabled Databricks Runtime, you can expect to achieve low latency query performance that can outperform the best cloud data warehouses.</p>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-star-schema/#references","title":"References","text":"<ul> <li>https://www.databricks.com/blog/2022/05/20/five-simple-steps-for-implementing-a-star-schema-in-databricks-with-delta-lake.html</li> </ul>"},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-stream-data-with-spark-stream/","title":"Pyspark Delta Lake: Streaming data using Spark Structured Streaming","text":""},{"location":"tools/open_table/deltalake/pyspark/pyspark-delta-stream-data-with-spark-stream/#references","title":"References","text":"<ul> <li>https://medium.com/globant/streaming-data-in-a-delta-table-using-spark-structured-streaming-19bd6f3ea37b</li> </ul>"},{"location":"tools/open_table/hudi/","title":"Apache Hudi","text":"<p>Hudi stands for \u2014 Hadoop Upsert Deletes and Incrementals</p> <p>Apache Hudi (Hadoop Upserts Deletes and Incrementals) is an open-source data management framework that is designed to simplify incremental data processing and data pipeline management for large-scale, high-performance data lakes. It helps us in managing large volumes of data with high velocity.</p>"},{"location":"tools/open_table/hudi/#references","title":"References","text":"<ul> <li>https://asrathore08.medium.com/apache-hudi-d259c1f202db</li> </ul>"},{"location":"tools/open_table/iceberg/","title":"Iceberg","text":"<ul> <li>What is Apache Iceberg</li> <li>https://tabular.medium.com/iceberg-in-modern-data-architecture-c647a1f29cb3</li> </ul> <p>Iceberg is a high-performance format for huge analytic tables. Companies like Netflix, Pinterest, Bloomberg, etc., have their entire analytical platform on Iceberg and require a Pythonic way of accessing the data lake. Enabling Iceberg tables to interact with the large Python ecosystem would be enriching for the business and data science teams.</p> <p>Note</p> <p>PyIceberg is a Python implementation for accessing Iceberg tables, without the need of a JVM.</p>"},{"location":"tools/open_table/iceberg/#examples","title":"Examples","text":"<ul> <li>https://medium.com/@MarinAgli1/learning-apache-iceberg-storing-the-data-to-minio-s3-56670cef199d</li> </ul>"},{"location":"tools/open_table/iceberg/icb-reduce-full-scan/","title":"Iceberg: Reduce Full Table Scan","text":"<p>https://medium.com/@twcardenas/how-to-reduce-full-table-scans-during-merges-in-apache-iceberg-and-save-money-abaed9af4e95</p>"},{"location":"tools/open_table/iceberg/icb-small-files/","title":"Iceberg: Solving the Small File Problem","text":"<p>https://medium.com/ancestry-product-and-technology/solving-the-small-file-problem-in-iceberg-tables-6c31a295f724</p>"},{"location":"tools/open_table/iceberg/icb-with-pyspark/","title":"IceBerg: With Pyspark","text":"<p>https://prukalpa.medium.com/special-edition-rethinking-data-governance-with-modern-data-team-43f2f05fe5f9</p>"},{"location":"tools/orchestration/airflow/","title":"Apache Airflow","text":"<p>Airflow was developed by Airbnb in 2014 and later open-sourced. It joined Apache Software Foundation\u2019s incubation in 2016.</p> <p>You can think of Airflow as a cook/chef following a recipe where the recipe is the data pipeline. The chef will follow this recipe by putting the right ingredients in the right quantity in the right order.</p> <p>Airflow can be used for a wide variety of workflows:</p> <ul> <li>ETL pipelines: extract data from a variety of sources, transform data in various ways</li> <li>Machine Learning workflows: automate the process of building, training, and deploying machine learning models. This might involve tasks such as data preparation, model training, model evaluation, and model deployment.</li> <li>Data Engineering: automate data engineering tasks such as data ingestion, data transformation, and data integration.</li> <li>Schedule tasks: schedule tasks to run at regular intervals, such as daily or weekly. This might include tasks such as running backups, sending emails, or updating data.</li> </ul> <p>Airflow is a workflow management tool, an orchestrator, that allows executing your tasks in the right way, right order, right time.</p> <p>Note</p> <p>Airflow doesn't expect that you process your data in Airflow, in your tasks. Instead, you should use Airflow as a way to trigger the tool that will process your data.</p> <p>Use Cases of Airflow:</p> <ul> <li>AirBNB</li> <li>Twitter</li> <li>Adobe</li> <li>Google - Cloud Composer</li> <li>AWS - Amazon Manged Workflows for Apache Airflow (NWAA)</li> </ul>"},{"location":"tools/orchestration/airflow/#components","title":"Components","text":"<ul> <li>Airflow Scheduler \u2014 the \"heart\" of Airflow, that parses the DAGs,   checks the scheduled intervals, and passes the tasks over to the workers.</li> <li>Airflow Worker \u2014 picks up the tasks and actually performs the work.</li> <li>Airflow Webserver \u2014 provides the main user interface to visualize and   monitor the DAGs and their results.</li> </ul>"},{"location":"tools/orchestration/airflow/#architecture","title":"Architecture","text":""},{"location":"tools/orchestration/airflow/#operators","title":"Operators","text":"<p>https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.html</p>"},{"location":"tools/orchestration/airflow/#how-airflow-solves-the-problems","title":"How Airflow solves the problems","text":"<p>Airflow is a workflow management tool that essentially helps you manage and schedule workflows, particularly in situations where many tasks are running concurrently.</p> <p>Here is how Airflow can help you:</p> <ul> <li>Complex workflows as code: Many workflows are complex and involve multiple steps that need to be executed in a specific order. Airflow makes it easier to define these workflows as code, which can help you to better understand and maintain your workflows over time.</li> <li>Dependencies between tasks: Many workflows have dependencies between tasks, which means that one task must be completed before another can be started. Airflow provides tools to help you manage these dependencies, ensuring that tasks are run in the correct order.</li> <li>Scaling: As the number of tasks in a workflow increases, it can become more difficult to manage and monitor the workflow. Airflow is designed to scale to a large number of tasks, and provides features to help you ensure that your workflows are running smoothly, even when you have many tasks running concurrently.</li> <li>Visibility: It can be difficult to get a clear picture of what is happening in a workflow, especially when many tasks are running at once. Airflow provides a web-based UI that allows you to view the status of your workflows, as well as the logs for each task in your workflow. This can help you to troubleshoot issues when they arise.</li> </ul>"},{"location":"tools/orchestration/airflow/#bast-practice","title":"Bast Practice","text":"<ul> <li>https://medium.com/@datasmiles/over-4-years-with-airflow-our-top-10-make-life-easier-tricks-dc65be6baf66</li> <li>https://medium.com/@datasmiles/mastering-apache-airflow-myessential-best-practices-for-robust-data-orchestration-095460505843</li> <li> <p>https://towardsdatascience.com/getting-started-with-astronomer-airflow-the-data-engineering-workhorse-d5a5cf543c2c</p> </li> <li> <p>Apache Airflow Cookbook \u2014 Part.1</p> </li> </ul>"},{"location":"tools/orchestration/airflow/#references","title":"References","text":"<ul> <li>Getting Started with Airflow - Why Backend Engineers Should Use It</li> <li>https://github.com/zkan/data-pipelines-with-airflow</li> </ul>"},{"location":"tools/orchestration/airflow/airflow-cicd/","title":"Airflow: CICD","text":"<p>https://garystafford.medium.com/devops-for-dataops-building-a-ci-cd-pipeline-for-apache-airflow-dags-975e4a622f83</p>"},{"location":"tools/orchestration/airflow/airflow-cost-optimization/","title":"Airflow: Cost Optimize","text":"<p>https://medium.com/@mariusz_kujawski/flats-price-analysis-airflow-duckdb-power-bi-and-gcp-df58c4c04378</p>"},{"location":"tools/orchestration/airflow/airflow-on-k8s/","title":"On Kubernetes","text":""},{"location":"tools/orchestration/airflow/airflow-on-k8s/#references","title":"References","text":"<ul> <li>What we learned after running Airflow on Kubernetes for 2 years</li> <li>Airflow on Kubernetes with Helm</li> </ul>"},{"location":"tools/orchestration/airflow/airflow-pool/","title":"Airflow: Pool","text":"<p>https://blog.devgenius.io/exploring-airflow-for-data-pipelines-a-dive-into-the-airflow-pool-775b7df44fb8</p>"},{"location":"tools/orchestration/airflow/airflow-pydantic-validator/","title":"Airflow: Pydantic Validator","text":"<p>https://medium.com/towards-data-engineering/pydantic-airflow-data-validation-in-python-and-the-cloud-9fcdf4551a4d</p>"},{"location":"tools/orchestration/airflow/airflow-repeatable-dags/","title":"Airflow: Designing Repeatable DAGS","text":"<p>https://medium.com/cts-technologies/designing-repeatable-dags-in-airflow-part-1-db3a72a2307d</p>"},{"location":"tools/orchestration/airflow/airflow-sensor/","title":"Sensor","text":"<p>SensorOperator is an Operator that will block our DAG by keep checking a certain condition until that condition was met.</p>"},{"location":"tools/orchestration/airflow/airflow-sensor/#getting-started","title":"Getting Started","text":""},{"location":"tools/orchestration/airflow/airflow-sensor/#filesensor","title":"FileSensor","text":"<p>Note</p> <p>FileSensor is a sensor that will keep checking if the target file exists or not.</p> <p>This is an example to use the FileSensor to check <code>/home/hello.txt</code>. The task <code>waiting_for_file</code> will keep running until the target file exists.</p> <pre><code>from airflow.models import DAG\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.operators.empty import EmptyOperator\nfrom pendulum import datetime, now\n\nwith DAG(\n    dag_id='medium_file_sensor',\n    schedule='@daily',\n    catchup=False,\n    start_date=datetime(2024,3,1),\n    max_active_runs=1\n):\n    start = EmptyOperator(task_id='start')\n    waiting_for_file = FileSensor(\n        task_id='waiting_for_file',\n        filepath='/home/hello.txt'\n    )\n    end = EmptyOperator(task_id='end')\n\n    start &gt;&gt; waiting_for_file &gt;&gt; end\n</code></pre>"},{"location":"tools/orchestration/airflow/airflow-sensor/#datetimesensor","title":"DateTimeSensor","text":"<p>Note</p> <p>DateTimeSensor is a sensor that will keep checking if current time pass the target datetime or not.</p> <p>This is an example to use the DateTimeSensor to check if current time pass <code>2024-03-10 4:35 PM (UTC+7)</code>. The task waiting_for_datetime will keep running until pass the target time.</p> <pre><code>from airflow.models import DAG\nfrom airflow.sensors.date_time import DateTimeSensor\nfrom airflow.operators.empty import EmptyOperator\nfrom pendulum import datetime, now\n\n\nwith DAG(\n    dag_id='medium_datetime_sensor',\n    schedule='@daily',\n    catchup=False,\n    start_date=datetime(2024,3,1),\n    max_active_runs=1\n):\n    start = EmptyOperator(task_id='start')\n    waiting_for_datetime = DateTimeSensor(\n        task_id='waiting_for_datetime',\n        target_time=datetime(2024,3,10,16,36,tz= 'Asia/Bangkok')\n    )\n    end = EmptyOperator(task_id='end')\n\n    start &gt;&gt; waiting_for_datetime &gt;&gt; end\n</code></pre>"},{"location":"tools/orchestration/airflow/airflow-sensor/#pythonsensor","title":"PythonSensor","text":"<p>Note</p> <p>PythonSensor is a sensor that will execute Python to do something to return Boolean value, if it\u2019s <code>True</code> then process to the next step.</p> <p>Note</p> <p>Additionally, PythonSensor also able to pass a value to Airflow\u2019s XCom.</p> <p>This is an example of how to PythonSensor to check if current time pass <code>2024-03-10 4:35 PM (UTC+7)</code> just like DateTimeSensor and it will also send the string Hello word to Airflow's XCom for the next task.</p> <pre><code>from airflow.models import DAG\nfrom airflow.decorators import task\nfrom airflow.sensors.base import PokeReturnValue\nfrom airflow.operators.empty import EmptyOperator\nfrom pendulum import datetime, now\n\n\nwith DAG(\n    dag_id='medium_python_sensor',\n    schedule='@daily',\n    catchup=False,\n    start_date=datetime(2024,3,1),\n    max_active_runs=1\n):\n    start= EmptyOperator(task_id='start')\n\n    @task.sensor(task_id='check_datetime_python')\n    def check_datetime_python_task() -&gt; PokeReturnValue:\n        # Check current &gt; target\n        condition_met = now() &gt;= datetime(2024,3,10,16,36,tz= 'Asia/Bangkok')\n        if condition_met :\n            # Return Something\n            operator_return_value = 'hello world'\n        else:\n            # Return Value as None if condition doesn't met\n            operator_return_value = None\n        # Return Poke Value\n        return PokeReturnValue(\n            is_done=condition_met,\n            xcom_value=operator_return_value,\n        )\n\n    @task(task_id= 'print_value')\n    def print_value_task(content) :\n        print(content)\n\n    check_datetime_python = check_datetime_python_task()\n    print_value = print_value_task(check_datetime_python)\n\n    # End\n    end = EmptyOperator(task_id='end')\n    # Set Dependencies Flow\n    start &gt;&gt; check_datetime_python &gt;&gt; print_value &gt;&gt; end\n</code></pre>"},{"location":"tools/orchestration/airflow/airflow-sensor/#externaltasksensor","title":"ExternalTaskSensor","text":"<p>Note</p> <p>ExternalTaskSensor is a sensor that will keep checking one of these:</p> <ul> <li>Check if a certain task in the upstream DAG is finish or not.</li> <li>Check if the upstream DAG is finish or not.</li> </ul> <p>Note</p> <p>*DAG Run Date of both upstream DAG and Sensor must be the same.</p> <p>This is an example to use the ExternalTaskSensor if the upstream DAG named <code>medium_datetime_sensor</code> from the previous example finish or not. One good thing about this sensor is that we can re-direct into the upstream DAG using the External DAG button in the UI.</p> <pre><code>from airflow.models import DAG\nfrom airflow.sensors.external_task import ExternalTaskSensor\nfrom airflow.operators.empty import EmptyOperator\nfrom pendulum import datetime, now\n\n\nwith DAG(\n    dag_id='medium_external_sensor',\n    schedule='@daily',\n    catchup=False,\n    start_date=datetime(2024, 3, 1),\n    max_active_runs=1\n):\n    start = EmptyOperator(task_id='start')\n    waiting_for_upstream = ExternalTaskSensor(\n        task_id='waiting_for_upstream',\n        external_dag_id='medium_datetime_sensor',\n        # None for DAG finish, Task_id for specific task\n        external_task_id=None\n    )\n    end = EmptyOperator(task_id= 'end')\n\n    start &gt;&gt; waiting_for_upstream &gt;&gt; end\n</code></pre> <p>Warning</p> <p>Something to be aware of is that the default ExternalTaskSensor will only check the upstream DAG\u2019s status only when the current DAG and the upstream DAG have exactly the same DAG execution date.</p> <p>But we can make some adjustments with the <code>execution_date_fn</code> parameter.</p> <p>This is an example if we want the current DAG to check the upstream DAG from previous date.</p> <pre><code>waiting_for_upstream = ExternalTaskSensor(\n    task_id='waiting_for_upstream',\n    external_dag_id='medium_datetime_sensor',\n    # None for DAG finish, Task_id for specific task\n    external_task_id=None,\n    # Input of function is DAG execution date (pendulum datetime)\n    execution_date_fn=(lambda dt : dt.add(days= -1))\n)\n</code></pre>"},{"location":"tools/orchestration/airflow/airflow-sensor/#idempotent-sensoroperator","title":"Idempotent SensorOperator","text":"<p>Note</p> <p>From my previous article about Idempotent DAG HERE.</p> <p>We also want our SensorOperator to has an Idempotent behavior too. That could be done with the same template method as the previous article.</p> <p>This is an example of a simple DAG with Idempotent FileSensor and Idempotent DateTimeSensor. It will create a DAG which apply the Idempotent concept into sensors.</p> <pre><code>from airflow.models import DAG\nfrom airflow.sensors.date_time import DateTimeSensor\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.operators.empty import EmptyOperator\nfrom pendulum import datetime, now\n\n\nwith DAG(\n    dag_id='medium_idempotent_sensor',\n    schedule='@daily',\n    catchup=False,\n    start_date=datetime(2024,3,1),\n    max_active_runs=1\n):\n    start = EmptyOperator(task_id='start')\n    waiting_for_datetime= DateTimeSensor(\n        task_id='waiting_for_datetime',\n        target_time='{{ data_interval_end.in_tz(\"Asia/Bangkok\").replace(hour= 23) }}'\n    )\n    waiting_for_file= FileSensor(\n        task_id='waiting_for_file',\n        # File name is hello_YYYYMMDD.txt\n        filepath='/home/hello_{{ data_interval_end.in_tz(\"Asia/Bangkok\").strftime(\"%Y%m%d\") }}.txt',\n    )\n    end = EmptyOperator(task_id= 'end')\n\n    start &gt;&gt; [waiting_for_datetime, waiting_for_file] &gt;&gt; end\n</code></pre>"},{"location":"tools/orchestration/airflow/airflow-sensor/#options","title":"Options","text":""},{"location":"tools/orchestration/airflow/airflow-sensor/#pokeinterval-timeout","title":"PokeInterval &amp; Timeout","text":"<p>Every SensorOperators are built-in with these parameters.</p> <p>1) <code>poke_interval</code>: After check, how long should the Sensor wait before check again. 2) <code>timeout</code>: How long can this Sensor wait before raise an error.</p> <p>Here is the sample of these parameters.</p> <pre><code>FileSensor(\n    task_id='waiting_for_file',\n    filepath='/home/hello.txt',\n    poke_interval=30, # Check every 30 seconds\n    timeout=3600 # After 1st poke, will wait for 1 hour before raise an error\n)\n</code></pre>"},{"location":"tools/orchestration/airflow/airflow-sensor/#modes","title":"Modes","text":"<p>Mode is the behavior of the Sensor during the poke_interval, there are 3 different modes.</p> <p>1) <code>poke</code> : Sensor will be active, it\u2019s fast but it will consume resources.</p> <p>2) <code>reschedule</code> : Sensor will be inactive, slower but consume less resources.</p> <p>3) <code>deferrable</code> : Consume even less resource and even slower than reschedule.    (Don\u2019t forget to <code>airflow triggerer</code> before use deferrable.)</p> <p>Note</p> <p>Deferrable is more complicate than poke and reschedule. If you want to understand how it works, I suggest taking this free course from Astronomer: Airflow: Deferrable Operators (astronomer.io)</p> <p>Here is the example of how to use each mode with DateTimeSensor.</p> <pre><code>from airflow.models import DAG\nfrom airflow.sensors.date_time import DateTimeSensor, DateTimeSensorAsync\nfrom pendulum import datetime, now\n\n\nwith DAG(\n    dag_id='medium_poke',\n    schedule='@daily',\n    catchup=True,\n    start_date=datetime(2024,1,1),\n):\n    poke = DateTimeSensor(\n        task_id='waiting_for_datetime',\n        target_time=\"{{ data_interval_end.add(years= 1) }}\",\n        mode='poke'\n    )\n\n\nwith DAG(\n    dag_id='medium_reschedule',\n    schedule='@daily',\n    catchup=True,\n    start_date=datetime(2024,1,1),\n):\n    reschedule = DateTimeSensor(\n        task_id='waiting_for_datetime',\n        target_time=\"{{ data_interval_end.add(years= 1) }}\",\n        mode='reschedule'\n    )\n\n\nwith DAG(\n    dag_id='medium_deferrable',\n    schedule='@daily',\n    catchup=True,\n    start_date=datetime(2024,1,1),\n):\n    deferrable = DateTimeSensorAsync(\n        task_id='waiting_for_datetime',\n        target_time=\"{{ data_interval_end.add(years= 1) }}\"\n    )\n</code></pre>"},{"location":"tools/orchestration/airflow/airflow-sensor/#read-mores","title":"Read Mores","text":"<ul> <li>Airflow - Sensor</li> <li>Astronomer - Airflow Sensor</li> <li>Apache Airflow Useful Practices: Sensor Operator</li> </ul>"},{"location":"tools/orchestration/airflow/airflow-unittest/","title":"Airflow: Unittest","text":"<p>https://medium.com/cj-express-tech-tildi/airflow-unit-testing-%E0%B9%80%E0%B8%9E%E0%B8%B4%E0%B9%88%E0%B8%A1%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%AA%E0%B8%B4%E0%B8%97%E0%B8%98%E0%B8%B4%E0%B8%A0%E0%B8%B2%E0%B8%9E%E0%B8%82%E0%B8%AD%E0%B8%87-data-pipeline-%E0%B9%81%E0%B8%A5%E0%B8%B0%E0%B9%80%E0%B8%AA%E0%B8%A3%E0%B8%B4%E0%B8%A1%E0%B9%80%E0%B8%81%E0%B8%A3%E0%B8%B2%E0%B8%B0-data-quality-ec000cf8b2cf</p>"},{"location":"tools/orchestration/airflow/airflow-with-dbt/","title":"With DBT","text":"<p>https://rasiksuhail.medium.com/orchestrating-dbt-with-airflow-a-step-by-step-guide-to-automating-data-pipelines-part-i-7a6db8ebc974</p>"},{"location":"tools/orchestration/airflow/airflow-with-kafka/","title":"Airflow: With Kafka","text":"<p>https://adzic-tanja.medium.com/etl-and-data-pipelines-using-airflow-and-kafka-0f82c186c97e</p>"},{"location":"tools/orchestration/dagster/","title":"Dagster","text":"<p>https://medium.com/@tgautam7/dagster-deep-dive-into-data-orchestrator-f67ec3ab38f9</p>"},{"location":"tools/orchestration/dagster/dagster-dlt/","title":"With DLT","text":""},{"location":"tools/orchestration/dagster/dagster-dlt/#references","title":"References","text":"<ul> <li>Orchestrate Unstructured Data Pipelines with Dagster and dlt | Dagster Blog</li> </ul>"},{"location":"tools/orchestration/dagster/dagster-dynamic-partition/","title":"Dagster: Dynamic Partitioning","text":""},{"location":"tools/orchestration/dagster/dagster-dynamic-partition/#references","title":"References","text":"<ul> <li>https://dagster.io/blog/dynamic-partitioning</li> </ul>"},{"location":"tools/quality/great_expectations/","title":"Great Expectations","text":""},{"location":"tools/quality/great_expectations/#getting-started","title":"Getting Started","text":"<pre><code>pip install great_expectations\n</code></pre> <p>https://medium.com/@pallavisinha12/create-data-quality-framework-with-great-expectations-911b42a5312f</p>"},{"location":"tools/quality/great_expectations/#references","title":"References","text":"<ul> <li>IT Next: Getting Started with Great Expectations Part I</li> <li>Medium Toward Data Science: Getting Started with Great Expectations</li> </ul>"},{"location":"tools/quality/great_expectations/gexp-with-databricks/","title":"Great Expectations: Implement on Databricks","text":"<p>Great Expectations (GE) is a great python library for data quality. It comes with integrations for Apache Spark and dozens of preconfigured data expectations. Databricks is a top-tier data platform built on Spark. So you\u2019d expect them to integrate seamlessly, but that is not quite the case.</p>"},{"location":"tools/quality/great_expectations/gexp-with-databricks/#references","title":"References","text":"<ul> <li>How to Integrate Great Expectations with Databricks</li> </ul>"},{"location":"tools/quality/great_expectations/gexp-with-spark/","title":"Great Expectations: Implement on Spark","text":""},{"location":"tools/quality/great_expectations/gexp-with-spark/#references","title":"References","text":"<ul> <li>https://medium.com/towards-data-engineering/great-expectations-with-pyspark-bb08538212c2</li> <li>https://towardsdatascience.com/data-quality-unit-tests-in-pyspark-using-great-expectations-e2e2c0a2c102</li> </ul>"},{"location":"tools/query/duckdb/","title":"DuckDB","text":"<p>https://mihaibojin.medium.com/duckdb-the-big-data-rising-star-71916f953f18</p>"},{"location":"tools/query/duckdb/#practices","title":"Practices","text":"<ul> <li>https://levelup.gitconnected.com/six-duckdb-sql-enhancements-you-should-learn-6a229b3c2d3e</li> </ul>"},{"location":"tools/query/duckdb/duckdb-db/","title":"TO Database","text":""},{"location":"tools/query/duckdb/duckdb-db/#attaching-to-multiple-databases-at-once","title":"Attaching to multiple databases at once","text":"<pre><code>import duckdb as db\n\ndb.sql(\"ATTACH 'dbname=your_db user=your_user password=your_password host=127.0.0.1' AS db_postgres (TYPE postgres)\")\ndb.sql(\"ATTACH 'my_sqlite.db' AS sql_lite (TYPE sqlite)\")\ndb.sql(\"create table sql_lite.small_table as SELECT * FROM db_postgres.large_table LIMIT 10\")\n\nsql_out = db.sql(\"SELECT * FROM sql_lite.small_table\")\n\nprint(sql_out)\n</code></pre>"},{"location":"tools/query/duckdb/duckdb-db/#copying-data-between-databases","title":"Copying Data Between Databases","text":"<pre><code>import duckdb as db\n\ndb.sql(\"ATTACH 'dbname=your_db user=your_user password=your_passwrd host=127.0.0.1' AS db_postgres (TYPE postgres)\")\n\n# attach a DuckDB file\ndb.sql(\"ATTACH 'database.db' AS ddb\")\n\n\n# export all tables and views from the Postgres database to the DuckDB file\ndb.sql(\"COPY FROM DATABASE db_postgres TO ddb\")\n</code></pre> <p>https://levelup.gitconnected.com/duckdb-database-connections-e041f8fcde9e</p>"},{"location":"tools/query/duckdb/duckdb-delta/","title":"To Delta","text":"<p>https://duckdb.org/2024/06/10/delta.html</p>"},{"location":"tools/query/trino/","title":"Trino","text":"<p>https://medium.com/@geektuhin/understanding-trino-architecture-unleashing-the-power-of-distributed-sql-queries-e996aa3eb39f</p>"},{"location":"tools/storage/hadoop/","title":"Hadoop","text":"<p>Hadoop is an open-source framework that allows for the distributed processing of large data sets across clusters of computers. It is a fundamental tool in big data analytics.</p> <p> One of the key features of Hadoop is its ability to handle massive amounts of data. Traditional databases often struggle with processing and analyzing large datasets, but Hadoop's distributed architecture allows it to scale horizontally by adding more machines to the cluster. This enables organizations to store and process petabytes of data efficiently.</p> <p>Key Concept</p> <p>The key concept is that we split the data up and store it across the collection of machines known as a cluster. Then when we want to process the data, we process it where it\u2019s actually stored. Rather than retrieving the data from a central server, the data\u2019s already on the cluster, so we can process it in place.</p>"},{"location":"tools/storage/hadoop/#getting-started","title":"Getting Started","text":"<p> Thus, Hadoop consists of two main components: the Hadoop Distributed File System (HDFS) and the MapReduce programming model.</p> <ul> <li> <p>HDFS is a distributed file system that provides high-throughput access to   data across multiple machines. It breaks down large files into smaller blocks   and distributes them across the cluster, ensuring data redundancy   and fault tolerance.</p> </li> <li> <p>The MapReduce programming model is the heart of Hadoop.   It allows users to write parallelizable algorithms that can process large   datasets in a distributed manner.</p> </li> </ul>"},{"location":"tools/storage/hadoop/#hdfs","title":"HDFS","text":"<p>Imagine we\u2019re going to store a file called mydata.txt. In HDFS. This file is 150 megabytes. When a file is loaded into HDFS, it\u2019s split into chunks which we call blocks. Each block is pretty big. The default is 64 megabytes. Each block is given a unique name, which is BLK, an underscore, and a large number.</p>"},{"location":"tools/storage/hadoop/#mapreduce","title":"MapReduce","text":"<p>...</p>"},{"location":"tools/storage/hadoop/#ecosystems","title":"Ecosystems","text":"<p>Hadoop also provides a range of tools and libraries that enhance its functionality. For example,</p> <ul> <li> <p>Apache Hive allows users to query and analyze data using a SQL-like language called HiveQL.</p> </li> <li> <p>Apache Pig provides a high-level scripting language called Pig Latin, which simplifies the development of data processing tasks.</p> </li> </ul> <p>Additionally, Hadoop supports machine learning libraries like Apache Mahout, enabling the implementation of advanced analytics and predictive modeling.</p>"},{"location":"tools/storage/hadoop/#references","title":"References","text":"<ul> <li>Big Data: Hadoop</li> </ul>"},{"location":"tools/storage/hadoop/hadoop-on-mac/","title":"Hadoop: On Mac","text":"<p>https://pub.towardsai.net/how-to-install-hadoop-on-macbook-m1-or-m2-without-homebrew-or-virtual-machine-ac7c3c5a6ac9</p>"},{"location":"tools/storage/lakefs/","title":"LakeFS","text":"<p>LakeFS</p>"},{"location":"tools/storage/lakefs/#practices","title":"Practices","text":"<p>Enhance Lakehouse Data Quality &amp; Workflows Productivity</p>"},{"location":"tools/storage/minio/","title":"MinIO","text":"<p>MinIO</p>"}]}